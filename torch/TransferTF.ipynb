{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c44d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 14:26:24.103032: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PatchTSTForPrediction\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96672c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "\n",
    "output_dir = \"saved_models\"\n",
    "log_dir = os.path.join('logstf', data)\n",
    "\n",
    "loss_name = \"mse\"\n",
    "\n",
    "num_train_epochs = 2000\n",
    "model_num = 1\n",
    "model_path = \"./saved_models\"\n",
    "learning_rate = 1e-6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3474e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "target_y = pd.read_csv(f\"../data/{data}/train_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "target_X_val = target_X[-round(target_X.shape[0] * 0.2):, :].astype(np.float32)\n",
    "target_y_val = target_y[-round(target_y.shape[0] * 0.2):].astype(np.float32)\n",
    "target_X = target_X[:-round(target_X.shape[0] * 0.2), :].astype(np.float32)\n",
    "target_y = target_y[:-round(target_y.shape[0] * 0.2)].astype(np.float32)\n",
    "\n",
    "test_X  = pd.read_csv(f\"../data/{data}/val_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "test_y  = pd.read_csv(f\"../data/{data}/val_output_7.csv\").iloc[:, 1:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10be638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_dataset(X, y):\n",
    "    X, y = torch.tensor(X), torch.tensor(y)\n",
    "    X = X.reshape(-1, X.shape[1], 1)\n",
    "    y = y.reshape(-1, y.shape[1], 1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_dataset = array_to_dataset(target_X, target_y)\n",
    "val_dataset = array_to_dataset(target_X_val, target_y_val)\n",
    "test_dataset = array_to_dataset(test_X, test_y)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 64)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1123fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.05, max_len = 5000, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p = dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)) * (-np.log(10000.0) / d_model)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(torch.tensor(position * div_term))\n",
    "        pe[:, 1::2] = torch.cos(torch.tensor(position * div_term))\n",
    "        pe = pe.unsqueeze(0)    ## (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe)  ## 불변값. 학습되지 않음. tf.constant\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8b3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerHead(torch.nn.Module):\n",
    "    def __init__(self, d_model, nlayers, nhead, dropout, iw, ow, input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_adapter = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, d_model // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_model // 2, d_model),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)    ## patch만 position encoding\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            torch.nn.ModuleDict({\n",
    "                \"attn\": torch.nn.MultiheadAttention(embed_dim = d_model, num_heads = nhead, dropout = dropout, batch_first = True),\n",
    "                \"norm1\": torch.nn.LayerNorm(d_model, eps = 1e-6),\n",
    "                \"ffn1\": torch.nn.Linear(d_model, d_model),\n",
    "                \"relu\": torch.nn.ReLU(),\n",
    "                \"ffn2\": torch.nn.Linear(d_model, d_model),\n",
    "                \"norm2\": torch.nn.LayerNorm(d_model, eps = 1e-6)\n",
    "            }) for _ in range(nlayers)\n",
    "        ])\n",
    "\n",
    "        self.outlayer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, d_model // 2),         ## (B, 7, 42)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),                             ## (B, 7*42 = 294). squeeze 역할\n",
    "            torch.nn.Linear(iw * (d_model // 2), 128),      ## (B, 128)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, ow) ## (B, 24)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## x : (B, 1, 7, 256)\n",
    "        x = x.squeeze(1)            ## (B, 7, 256)\n",
    "        x = self.input_adapter(x)   ## (B, 7, 84)\n",
    "        x = self.pos_encoding(x)    ## (B, 7, 84)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            attn_output, _ = layer[\"attn\"](x, x, x)\n",
    "            x = layer[\"norm1\"](x + attn_output)\n",
    "\n",
    "            ffn_output = layer[\"relu\"](layer[\"ffn1\"](x))\n",
    "            ffn_output = layer[\"ffn2\"](ffn_output)\n",
    "            x = layer[\"norm2\"](x + ffn_output)\n",
    "\n",
    "        outputs = self.outlayer(x)\n",
    "        outputs = outputs.unsqueeze(2)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7ea475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1635193/4015960496.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pe[:, 0::2] = torch.sin(torch.tensor(position * div_term))\n",
      "/tmp/ipykernel_1635193/4015960496.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pe[:, 1::2] = torch.cos(torch.tensor(position * div_term))\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, model_num+1):\n",
    "    current_path = os.path.join(model_path, f\"model_{loss_name}_{k}.pth\")\n",
    "\n",
    "    model_instance = PatchTSTForPrediction.from_pretrained(os.path.join(model_path, \"PatchTSTBackbone\")).to(device)\n",
    "    model_instance.load_state_dict(torch.load(current_path))\n",
    "    \n",
    "    model_instance.head.flatten = torch.nn.Identity()\n",
    "    model_instance.head.projection = TransformerHead(\n",
    "        d_model = 84, nlayers = 2, nhead = 2, dropout = 0.2,\n",
    "        iw = 7, ow = target_y.shape[1], input_dim = 256\n",
    "    )\n",
    "    model_instance.dropout = torch.nn.Identity()\n",
    "    model_instance.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33881c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom loss function\n",
    "def SMAPE(yhat, y):\n",
    "    numerator = 100*torch.abs(y - yhat)\n",
    "    denominator = (torch.abs(y) + torch.abs(yhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "def MAPE(y_pred, y_true, epsilon=1e-7):\n",
    "    denominator = torch.clamp(torch.abs(y_true), min=epsilon)       ## 분모에 0이 들어오는 것을 방지\n",
    "    abs_percent_error = torch.abs((y_true - y_pred) / denominator)\n",
    "\n",
    "    return torch.mean(100. * abs_percent_error)\n",
    "\n",
    "\n",
    "class MASE(torch.nn.Module):\n",
    "    def __init__(self, training_data, period = 1):\n",
    "        super().__init__()\n",
    "        ## 원본 코드 구현, 사실상 MAE와 동일, 잘못 짜여진 코드, 일단은 하던대로 할 것.\n",
    "        self.scale = torch.mean(torch.abs(torch.tensor(training_data[period:] - training_data[:-period])))\n",
    "    \n",
    "    def forward(self, yhat, y):\n",
    "        error = torch.abs(y - yhat)\n",
    "        return torch.mean(error) / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2179b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([8, 24, 1])) that is different to the input size (torch.Size([8, 1, 24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/root/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([2, 24, 1])) that is different to the input size (torch.Size([2, 1, 24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/root/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([145, 24, 1])) that is different to the input size (torch.Size([145, 1, 24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000 | Train Loss: 374.501749\t\t Val Loss: 30.208729\n",
      "Epoch 2/2000 | Train Loss: 374.483415\t\t Val Loss: 30.126707\n",
      "Epoch 3/2000 | Train Loss: 373.494290\t\t Val Loss: 30.029268\n",
      "Epoch 4/2000 | Train Loss: 372.504199\t\t Val Loss: 29.929281\n",
      "Epoch 5/2000 | Train Loss: 372.048122\t\t Val Loss: 29.766218\n",
      "Epoch 6/2000 | Train Loss: 371.594040\t\t Val Loss: 29.706589\n",
      "Epoch 7/2000 | Train Loss: 370.759946\t\t Val Loss: 29.638998\n",
      "Epoch 8/2000 | Train Loss: 370.734294\t\t Val Loss: 29.568966\n",
      "Epoch 9/2000 | Train Loss: 369.709900\t\t Val Loss: 29.425726\n",
      "Epoch 10/2000 | Train Loss: 369.651657\t\t Val Loss: 29.298334\n",
      "Epoch 11/2000 | Train Loss: 368.614187\t\t Val Loss: 29.213438\n",
      "Epoch 12/2000 | Train Loss: 367.322813\t\t Val Loss: 29.144989\n",
      "Epoch 13/2000 | Train Loss: 366.868405\t\t Val Loss: 29.017574\n",
      "Epoch 14/2000 | Train Loss: 366.688773\t\t Val Loss: 28.968971\n",
      "Epoch 15/2000 | Train Loss: 365.014013\t\t Val Loss: 28.886753\n",
      "Epoch 16/2000 | Train Loss: 364.273270\t\t Val Loss: 28.787378\n",
      "Epoch 17/2000 | Train Loss: 363.174918\t\t Val Loss: 28.643696\n",
      "Epoch 18/2000 | Train Loss: 361.973302\t\t Val Loss: 28.584223\n",
      "Epoch 19/2000 | Train Loss: 361.390330\t\t Val Loss: 28.459896\n",
      "Epoch 20/2000 | Train Loss: 360.283037\t\t Val Loss: 28.413965\n",
      "Epoch 21/2000 | Train Loss: 359.856052\t\t Val Loss: 28.288363\n",
      "Epoch 22/2000 | Train Loss: 358.419934\t\t Val Loss: 28.249674\n",
      "Epoch 23/2000 | Train Loss: 357.450604\t\t Val Loss: 28.133879\n",
      "Epoch 24/2000 | Train Loss: 356.660725\t\t Val Loss: 28.054478\n",
      "Epoch 25/2000 | Train Loss: 355.721527\t\t Val Loss: 27.966856\n",
      "Epoch 26/2000 | Train Loss: 354.765359\t\t Val Loss: 27.892906\n",
      "Epoch 27/2000 | Train Loss: 353.583354\t\t Val Loss: 27.832987\n",
      "Epoch 28/2000 | Train Loss: 351.683043\t\t Val Loss: 27.705259\n",
      "Epoch 29/2000 | Train Loss: 352.208183\t\t Val Loss: 27.617207\n",
      "Epoch 30/2000 | Train Loss: 349.998058\t\t Val Loss: 27.560980\n",
      "Epoch 31/2000 | Train Loss: 348.298455\t\t Val Loss: 27.446241\n",
      "Epoch 32/2000 | Train Loss: 347.868253\t\t Val Loss: 27.307484\n",
      "Epoch 33/2000 | Train Loss: 345.422816\t\t Val Loss: 27.259459\n",
      "Epoch 34/2000 | Train Loss: 344.699964\t\t Val Loss: 27.167574\n",
      "Epoch 35/2000 | Train Loss: 343.141194\t\t Val Loss: 27.056616\n",
      "Epoch 36/2000 | Train Loss: 342.166377\t\t Val Loss: 26.952114\n",
      "Epoch 37/2000 | Train Loss: 342.186275\t\t Val Loss: 26.873344\n",
      "Epoch 38/2000 | Train Loss: 339.828293\t\t Val Loss: 26.770523\n",
      "Epoch 39/2000 | Train Loss: 338.467148\t\t Val Loss: 26.703735\n",
      "Epoch 40/2000 | Train Loss: 337.314349\t\t Val Loss: 26.581799\n",
      "Epoch 41/2000 | Train Loss: 335.962408\t\t Val Loss: 26.469780\n",
      "Epoch 42/2000 | Train Loss: 335.102060\t\t Val Loss: 26.364300\n",
      "Epoch 43/2000 | Train Loss: 333.208048\t\t Val Loss: 26.225595\n",
      "Epoch 44/2000 | Train Loss: 331.940204\t\t Val Loss: 26.144547\n",
      "Epoch 45/2000 | Train Loss: 329.784489\t\t Val Loss: 26.040472\n",
      "Epoch 46/2000 | Train Loss: 329.758702\t\t Val Loss: 25.965473\n",
      "Epoch 47/2000 | Train Loss: 326.803697\t\t Val Loss: 25.803455\n",
      "Epoch 48/2000 | Train Loss: 325.813760\t\t Val Loss: 25.733477\n",
      "Epoch 49/2000 | Train Loss: 324.678078\t\t Val Loss: 25.572220\n",
      "Epoch 50/2000 | Train Loss: 323.351191\t\t Val Loss: 25.523605\n",
      "Epoch 51/2000 | Train Loss: 322.655399\t\t Val Loss: 25.390860\n",
      "Epoch 52/2000 | Train Loss: 318.994854\t\t Val Loss: 25.264366\n",
      "Epoch 53/2000 | Train Loss: 318.093233\t\t Val Loss: 25.160971\n",
      "Epoch 54/2000 | Train Loss: 315.931798\t\t Val Loss: 25.014336\n",
      "Epoch 55/2000 | Train Loss: 314.018670\t\t Val Loss: 24.886961\n",
      "Epoch 56/2000 | Train Loss: 313.408804\t\t Val Loss: 24.749237\n",
      "Epoch 57/2000 | Train Loss: 310.690364\t\t Val Loss: 24.705986\n",
      "Epoch 58/2000 | Train Loss: 310.333713\t\t Val Loss: 24.545908\n",
      "Epoch 59/2000 | Train Loss: 309.467646\t\t Val Loss: 24.384909\n",
      "Epoch 60/2000 | Train Loss: 306.966039\t\t Val Loss: 24.294821\n",
      "Epoch 61/2000 | Train Loss: 305.101050\t\t Val Loss: 24.220425\n",
      "Epoch 62/2000 | Train Loss: 304.310221\t\t Val Loss: 24.048046\n",
      "Epoch 63/2000 | Train Loss: 300.897347\t\t Val Loss: 23.942545\n",
      "Epoch 64/2000 | Train Loss: 299.743207\t\t Val Loss: 23.801565\n",
      "Epoch 65/2000 | Train Loss: 298.678439\t\t Val Loss: 23.715212\n",
      "Epoch 66/2000 | Train Loss: 297.103212\t\t Val Loss: 23.684357\n",
      "Epoch 67/2000 | Train Loss: 295.957888\t\t Val Loss: 23.502935\n",
      "Epoch 68/2000 | Train Loss: 296.261768\t\t Val Loss: 23.317072\n",
      "Epoch 69/2000 | Train Loss: 291.930304\t\t Val Loss: 23.205196\n",
      "Epoch 70/2000 | Train Loss: 291.653910\t\t Val Loss: 23.076309\n",
      "Epoch 71/2000 | Train Loss: 289.544158\t\t Val Loss: 22.960478\n",
      "Epoch 72/2000 | Train Loss: 288.718855\t\t Val Loss: 22.811741\n",
      "Epoch 73/2000 | Train Loss: 285.871407\t\t Val Loss: 22.760818\n",
      "Epoch 74/2000 | Train Loss: 285.059292\t\t Val Loss: 22.629520\n",
      "Epoch 75/2000 | Train Loss: 283.030963\t\t Val Loss: 22.431993\n",
      "Epoch 76/2000 | Train Loss: 282.399067\t\t Val Loss: 22.270288\n",
      "Epoch 77/2000 | Train Loss: 279.354470\t\t Val Loss: 22.299473\n",
      "Epoch 78/2000 | Train Loss: 277.655858\t\t Val Loss: 22.081579\n",
      "Epoch 79/2000 | Train Loss: 277.243052\t\t Val Loss: 22.016150\n",
      "Epoch 80/2000 | Train Loss: 273.406644\t\t Val Loss: 21.793344\n",
      "Epoch 81/2000 | Train Loss: 274.395514\t\t Val Loss: 21.713579\n",
      "Epoch 82/2000 | Train Loss: 272.683803\t\t Val Loss: 21.616562\n",
      "Epoch 83/2000 | Train Loss: 269.914301\t\t Val Loss: 21.494543\n",
      "Epoch 84/2000 | Train Loss: 269.105227\t\t Val Loss: 21.324083\n",
      "Epoch 85/2000 | Train Loss: 267.056276\t\t Val Loss: 21.319368\n",
      "Epoch 86/2000 | Train Loss: 267.063257\t\t Val Loss: 21.107180\n",
      "Epoch 87/2000 | Train Loss: 262.872712\t\t Val Loss: 21.018202\n",
      "Epoch 88/2000 | Train Loss: 262.402956\t\t Val Loss: 20.873268\n",
      "Epoch 89/2000 | Train Loss: 261.287846\t\t Val Loss: 20.762829\n",
      "Epoch 90/2000 | Train Loss: 260.300507\t\t Val Loss: 20.633011\n",
      "Epoch 91/2000 | Train Loss: 257.645460\t\t Val Loss: 20.525076\n",
      "Epoch 92/2000 | Train Loss: 257.094809\t\t Val Loss: 20.404936\n",
      "Epoch 93/2000 | Train Loss: 254.947448\t\t Val Loss: 20.234941\n",
      "Epoch 94/2000 | Train Loss: 253.559013\t\t Val Loss: 20.114042\n",
      "Epoch 95/2000 | Train Loss: 251.588898\t\t Val Loss: 20.035912\n",
      "Epoch 96/2000 | Train Loss: 251.540416\t\t Val Loss: 19.928759\n",
      "Epoch 97/2000 | Train Loss: 250.345991\t\t Val Loss: 19.779219\n",
      "Epoch 98/2000 | Train Loss: 247.935807\t\t Val Loss: 19.634521\n",
      "Epoch 99/2000 | Train Loss: 247.901343\t\t Val Loss: 19.524488\n",
      "Epoch 100/2000 | Train Loss: 243.570634\t\t Val Loss: 19.393774\n",
      "Epoch 101/2000 | Train Loss: 242.955246\t\t Val Loss: 19.343658\n",
      "Epoch 102/2000 | Train Loss: 244.716395\t\t Val Loss: 19.180964\n",
      "Epoch 103/2000 | Train Loss: 241.171345\t\t Val Loss: 19.060539\n",
      "Epoch 104/2000 | Train Loss: 239.072310\t\t Val Loss: 18.891645\n",
      "Epoch 105/2000 | Train Loss: 237.798208\t\t Val Loss: 18.788219\n",
      "Epoch 106/2000 | Train Loss: 234.695069\t\t Val Loss: 18.791182\n",
      "Epoch 107/2000 | Train Loss: 235.087943\t\t Val Loss: 18.669107\n",
      "Epoch 108/2000 | Train Loss: 232.133775\t\t Val Loss: 18.631660\n",
      "Epoch 109/2000 | Train Loss: 230.549729\t\t Val Loss: 18.338577\n",
      "Epoch 110/2000 | Train Loss: 230.543137\t\t Val Loss: 18.228008\n",
      "Epoch 111/2000 | Train Loss: 230.023452\t\t Val Loss: 18.114479\n",
      "Epoch 112/2000 | Train Loss: 227.635494\t\t Val Loss: 18.040140\n",
      "Epoch 113/2000 | Train Loss: 227.805952\t\t Val Loss: 17.836998\n",
      "Epoch 114/2000 | Train Loss: 224.270468\t\t Val Loss: 17.848742\n",
      "Epoch 115/2000 | Train Loss: 225.684891\t\t Val Loss: 17.626234\n",
      "Epoch 116/2000 | Train Loss: 222.451466\t\t Val Loss: 17.492971\n",
      "Epoch 117/2000 | Train Loss: 221.012578\t\t Val Loss: 17.421894\n",
      "Epoch 118/2000 | Train Loss: 220.851152\t\t Val Loss: 17.326473\n",
      "Epoch 119/2000 | Train Loss: 219.942988\t\t Val Loss: 17.186237\n",
      "Epoch 120/2000 | Train Loss: 217.718032\t\t Val Loss: 17.373070\n",
      "Epoch 121/2000 | Train Loss: 215.927810\t\t Val Loss: 17.023567\n",
      "Epoch 122/2000 | Train Loss: 213.246071\t\t Val Loss: 16.896608\n",
      "Epoch 123/2000 | Train Loss: 211.904732\t\t Val Loss: 16.818420\n",
      "Epoch 124/2000 | Train Loss: 216.110492\t\t Val Loss: 16.600870\n",
      "Epoch 125/2000 | Train Loss: 208.995482\t\t Val Loss: 16.861313\n",
      "Epoch 126/2000 | Train Loss: 210.484864\t\t Val Loss: 16.456005\n",
      "Epoch 127/2000 | Train Loss: 211.578326\t\t Val Loss: 16.326342\n",
      "Epoch 128/2000 | Train Loss: 209.566492\t\t Val Loss: 16.195965\n",
      "Epoch 129/2000 | Train Loss: 207.618510\t\t Val Loss: 16.114412\n",
      "Epoch 130/2000 | Train Loss: 202.997649\t\t Val Loss: 16.037649\n",
      "Epoch 131/2000 | Train Loss: 204.104701\t\t Val Loss: 15.909456\n",
      "Epoch 132/2000 | Train Loss: 202.091638\t\t Val Loss: 15.939189\n",
      "Epoch 133/2000 | Train Loss: 201.594332\t\t Val Loss: 15.770280\n",
      "Epoch 134/2000 | Train Loss: 200.156212\t\t Val Loss: 15.578179\n",
      "Epoch 135/2000 | Train Loss: 199.210164\t\t Val Loss: 15.543618\n",
      "Epoch 136/2000 | Train Loss: 199.687472\t\t Val Loss: 15.602466\n",
      "Epoch 137/2000 | Train Loss: 198.109601\t\t Val Loss: 15.356254\n",
      "Epoch 138/2000 | Train Loss: 197.120387\t\t Val Loss: 15.301976\n",
      "Epoch 139/2000 | Train Loss: 194.680853\t\t Val Loss: 15.211402\n",
      "Epoch 140/2000 | Train Loss: 195.054956\t\t Val Loss: 15.079439\n",
      "Epoch 141/2000 | Train Loss: 193.271936\t\t Val Loss: 15.203606\n",
      "Epoch 142/2000 | Train Loss: 192.540693\t\t Val Loss: 14.899234\n",
      "Epoch 143/2000 | Train Loss: 190.278223\t\t Val Loss: 14.858391\n",
      "Epoch 144/2000 | Train Loss: 188.950289\t\t Val Loss: 14.711899\n",
      "Epoch 145/2000 | Train Loss: 190.156547\t\t Val Loss: 14.594990\n",
      "Epoch 146/2000 | Train Loss: 187.504466\t\t Val Loss: 14.531186\n",
      "Epoch 147/2000 | Train Loss: 189.047723\t\t Val Loss: 14.527826\n",
      "Epoch 148/2000 | Train Loss: 186.399104\t\t Val Loss: 14.560706\n",
      "Epoch 149/2000 | Train Loss: 182.925988\t\t Val Loss: 14.322827\n",
      "Epoch 150/2000 | Train Loss: 183.460826\t\t Val Loss: 14.133304\n",
      "Epoch 151/2000 | Train Loss: 182.624434\t\t Val Loss: 14.021318\n",
      "Epoch 152/2000 | Train Loss: 179.334586\t\t Val Loss: 13.965240\n",
      "Epoch 153/2000 | Train Loss: 179.147590\t\t Val Loss: 13.918481\n",
      "Epoch 154/2000 | Train Loss: 178.000664\t\t Val Loss: 13.841494\n",
      "Epoch 155/2000 | Train Loss: 177.149329\t\t Val Loss: 13.732536\n",
      "Epoch 156/2000 | Train Loss: 181.487729\t\t Val Loss: 13.970220\n",
      "Epoch 157/2000 | Train Loss: 176.221683\t\t Val Loss: 13.763530\n",
      "Epoch 158/2000 | Train Loss: 176.127147\t\t Val Loss: 13.684287\n",
      "Epoch 159/2000 | Train Loss: 174.957723\t\t Val Loss: 13.484618\n",
      "Epoch 160/2000 | Train Loss: 173.108576\t\t Val Loss: 13.482066\n",
      "Epoch 161/2000 | Train Loss: 173.511364\t\t Val Loss: 13.215347\n",
      "Epoch 162/2000 | Train Loss: 173.210490\t\t Val Loss: 13.311996\n",
      "Epoch 163/2000 | Train Loss: 170.692006\t\t Val Loss: 13.168858\n",
      "Epoch 164/2000 | Train Loss: 168.853929\t\t Val Loss: 13.046951\n",
      "Epoch 165/2000 | Train Loss: 168.033781\t\t Val Loss: 13.279538\n",
      "Epoch 166/2000 | Train Loss: 167.525090\t\t Val Loss: 13.030852\n",
      "Epoch 167/2000 | Train Loss: 167.887973\t\t Val Loss: 12.835725\n",
      "Epoch 168/2000 | Train Loss: 166.063313\t\t Val Loss: 12.740562\n",
      "Epoch 169/2000 | Train Loss: 164.793152\t\t Val Loss: 12.742646\n",
      "Epoch 170/2000 | Train Loss: 167.345359\t\t Val Loss: 12.722448\n",
      "Epoch 171/2000 | Train Loss: 163.568831\t\t Val Loss: 12.700994\n",
      "Epoch 172/2000 | Train Loss: 163.875035\t\t Val Loss: 12.458098\n",
      "Epoch 173/2000 | Train Loss: 166.212733\t\t Val Loss: 12.666757\n",
      "Epoch 174/2000 | Train Loss: 162.350604\t\t Val Loss: 12.855757\n",
      "Epoch 175/2000 | Train Loss: 160.257520\t\t Val Loss: 12.647690\n",
      "Epoch 176/2000 | Train Loss: 161.429258\t\t Val Loss: 12.325862\n",
      "Epoch 177/2000 | Train Loss: 159.144668\t\t Val Loss: 12.222179\n",
      "Epoch 178/2000 | Train Loss: 161.410999\t\t Val Loss: 12.135039\n",
      "Epoch 179/2000 | Train Loss: 158.657669\t\t Val Loss: 12.100844\n",
      "Epoch 180/2000 | Train Loss: 155.661503\t\t Val Loss: 12.038557\n",
      "Epoch 181/2000 | Train Loss: 159.137723\t\t Val Loss: 11.914078\n",
      "Epoch 182/2000 | Train Loss: 155.721954\t\t Val Loss: 11.919305\n",
      "Epoch 183/2000 | Train Loss: 156.417335\t\t Val Loss: 11.943153\n",
      "Epoch 184/2000 | Train Loss: 157.036828\t\t Val Loss: 11.742127\n",
      "Epoch 185/2000 | Train Loss: 154.544348\t\t Val Loss: 11.604339\n",
      "Epoch 186/2000 | Train Loss: 152.555688\t\t Val Loss: 11.696528\n",
      "Epoch 187/2000 | Train Loss: 154.433630\t\t Val Loss: 11.553611\n",
      "Epoch 188/2000 | Train Loss: 152.650222\t\t Val Loss: 11.466814\n",
      "Epoch 189/2000 | Train Loss: 151.086699\t\t Val Loss: 11.423650\n",
      "Epoch 190/2000 | Train Loss: 147.789401\t\t Val Loss: 11.481499\n",
      "Epoch 191/2000 | Train Loss: 151.471357\t\t Val Loss: 11.425906\n",
      "Epoch 192/2000 | Train Loss: 148.879925\t\t Val Loss: 11.352855\n",
      "Epoch 193/2000 | Train Loss: 152.718629\t\t Val Loss: 11.427218\n",
      "Epoch 194/2000 | Train Loss: 146.408357\t\t Val Loss: 11.240265\n",
      "Epoch 195/2000 | Train Loss: 146.866742\t\t Val Loss: 11.141402\n",
      "Epoch 196/2000 | Train Loss: 146.298129\t\t Val Loss: 10.995721\n",
      "Epoch 197/2000 | Train Loss: 147.029405\t\t Val Loss: 11.532727\n",
      "Epoch 198/2000 | Train Loss: 145.105679\t\t Val Loss: 10.999162\n",
      "Epoch 199/2000 | Train Loss: 144.712809\t\t Val Loss: 11.095369\n",
      "Epoch 200/2000 | Train Loss: 143.293317\t\t Val Loss: 11.081307\n",
      "Epoch 201/2000 | Train Loss: 144.152912\t\t Val Loss: 10.960095\n",
      "Epoch 202/2000 | Train Loss: 142.250770\t\t Val Loss: 10.920909\n",
      "Epoch 203/2000 | Train Loss: 140.207658\t\t Val Loss: 10.730587\n",
      "Epoch 204/2000 | Train Loss: 144.746290\t\t Val Loss: 10.866277\n",
      "Epoch 205/2000 | Train Loss: 142.991894\t\t Val Loss: 10.659704\n",
      "Epoch 206/2000 | Train Loss: 142.502070\t\t Val Loss: 10.648135\n",
      "Epoch 207/2000 | Train Loss: 140.197188\t\t Val Loss: 10.576484\n",
      "Epoch 208/2000 | Train Loss: 138.339539\t\t Val Loss: 10.367871\n",
      "Epoch 209/2000 | Train Loss: 136.877362\t\t Val Loss: 10.458958\n",
      "Epoch 210/2000 | Train Loss: 136.806634\t\t Val Loss: 10.554751\n",
      "Epoch 211/2000 | Train Loss: 137.127489\t\t Val Loss: 10.480156\n",
      "Epoch 212/2000 | Train Loss: 135.666208\t\t Val Loss: 10.728804\n",
      "Epoch 213/2000 | Train Loss: 137.777211\t\t Val Loss: 10.339073\n",
      "Epoch 214/2000 | Train Loss: 135.375761\t\t Val Loss: 10.448318\n",
      "Epoch 215/2000 | Train Loss: 137.187950\t\t Val Loss: 10.163936\n",
      "Epoch 216/2000 | Train Loss: 135.929065\t\t Val Loss: 10.143207\n",
      "Epoch 217/2000 | Train Loss: 135.001215\t\t Val Loss: 10.311722\n",
      "Epoch 218/2000 | Train Loss: 132.968701\t\t Val Loss: 10.121755\n",
      "Epoch 219/2000 | Train Loss: 132.156625\t\t Val Loss: 9.926138\n",
      "Epoch 220/2000 | Train Loss: 134.678794\t\t Val Loss: 10.214301\n",
      "Epoch 221/2000 | Train Loss: 133.111198\t\t Val Loss: 9.808485\n",
      "Epoch 222/2000 | Train Loss: 130.730625\t\t Val Loss: 9.892184\n",
      "Epoch 223/2000 | Train Loss: 130.555587\t\t Val Loss: 9.924443\n",
      "Epoch 224/2000 | Train Loss: 129.501554\t\t Val Loss: 10.032214\n",
      "Epoch 225/2000 | Train Loss: 130.403406\t\t Val Loss: 10.047040\n",
      "Epoch 226/2000 | Train Loss: 131.696665\t\t Val Loss: 9.900155\n",
      "Epoch 227/2000 | Train Loss: 128.453941\t\t Val Loss: 10.020377\n",
      "Epoch 228/2000 | Train Loss: 130.619289\t\t Val Loss: 9.644029\n",
      "Epoch 229/2000 | Train Loss: 126.911794\t\t Val Loss: 9.740336\n",
      "Epoch 230/2000 | Train Loss: 128.477295\t\t Val Loss: 9.495289\n",
      "Epoch 231/2000 | Train Loss: 126.662504\t\t Val Loss: 9.747361\n",
      "Epoch 232/2000 | Train Loss: 127.405951\t\t Val Loss: 9.670609\n",
      "Epoch 233/2000 | Train Loss: 127.596697\t\t Val Loss: 9.494467\n",
      "Epoch 234/2000 | Train Loss: 132.065467\t\t Val Loss: 9.462529\n",
      "Epoch 235/2000 | Train Loss: 128.381667\t\t Val Loss: 9.768048\n",
      "Epoch 236/2000 | Train Loss: 128.372828\t\t Val Loss: 9.295671\n",
      "Epoch 237/2000 | Train Loss: 125.006242\t\t Val Loss: 9.640798\n",
      "Epoch 238/2000 | Train Loss: 125.037324\t\t Val Loss: 9.537798\n",
      "Epoch 239/2000 | Train Loss: 122.405023\t\t Val Loss: 9.379131\n",
      "Epoch 240/2000 | Train Loss: 124.306555\t\t Val Loss: 9.539618\n",
      "Epoch 241/2000 | Train Loss: 124.421630\t\t Val Loss: 9.535233\n",
      "Epoch 242/2000 | Train Loss: 119.433719\t\t Val Loss: 9.205436\n",
      "Epoch 243/2000 | Train Loss: 123.653220\t\t Val Loss: 9.301052\n",
      "Epoch 244/2000 | Train Loss: 121.166931\t\t Val Loss: 9.607757\n",
      "Epoch 245/2000 | Train Loss: 121.597210\t\t Val Loss: 9.422158\n",
      "Epoch 246/2000 | Train Loss: 119.565991\t\t Val Loss: 9.389300\n",
      "Epoch 247/2000 | Train Loss: 118.660310\t\t Val Loss: 9.089363\n",
      "Epoch 248/2000 | Train Loss: 117.800770\t\t Val Loss: 9.460667\n",
      "Epoch 249/2000 | Train Loss: 123.641122\t\t Val Loss: 9.236026\n",
      "Epoch 250/2000 | Train Loss: 118.270841\t\t Val Loss: 8.901268\n",
      "Epoch 251/2000 | Train Loss: 118.943134\t\t Val Loss: 9.215941\n",
      "Epoch 252/2000 | Train Loss: 117.798801\t\t Val Loss: 9.273432\n",
      "Epoch 253/2000 | Train Loss: 117.190369\t\t Val Loss: 9.284143\n",
      "Epoch 254/2000 | Train Loss: 118.191969\t\t Val Loss: 9.125115\n",
      "Epoch 255/2000 | Train Loss: 121.235297\t\t Val Loss: 9.218047\n",
      "Epoch 256/2000 | Train Loss: 119.169424\t\t Val Loss: 9.155016\n",
      "Epoch 257/2000 | Train Loss: 120.004221\t\t Val Loss: 8.791423\n",
      "Epoch 258/2000 | Train Loss: 117.148645\t\t Val Loss: 9.036829\n",
      "Epoch 259/2000 | Train Loss: 115.345506\t\t Val Loss: 8.690672\n",
      "Epoch 260/2000 | Train Loss: 116.052484\t\t Val Loss: 9.010290\n",
      "Epoch 261/2000 | Train Loss: 116.156492\t\t Val Loss: 8.834657\n",
      "Epoch 262/2000 | Train Loss: 115.149878\t\t Val Loss: 8.749426\n",
      "Epoch 263/2000 | Train Loss: 115.662387\t\t Val Loss: 8.911177\n",
      "Epoch 264/2000 | Train Loss: 115.552090\t\t Val Loss: 8.838268\n",
      "Epoch 265/2000 | Train Loss: 113.908948\t\t Val Loss: 8.610093\n",
      "Epoch 266/2000 | Train Loss: 116.254769\t\t Val Loss: 8.736667\n",
      "Epoch 267/2000 | Train Loss: 117.320615\t\t Val Loss: 8.747372\n",
      "Epoch 268/2000 | Train Loss: 112.769653\t\t Val Loss: 8.400295\n",
      "Epoch 269/2000 | Train Loss: 115.864991\t\t Val Loss: 9.067675\n",
      "Epoch 270/2000 | Train Loss: 111.656415\t\t Val Loss: 8.376025\n",
      "Epoch 271/2000 | Train Loss: 111.471959\t\t Val Loss: 8.520222\n",
      "Epoch 272/2000 | Train Loss: 112.263377\t\t Val Loss: 8.420964\n",
      "Epoch 273/2000 | Train Loss: 111.413085\t\t Val Loss: 8.977644\n",
      "Epoch 274/2000 | Train Loss: 109.228786\t\t Val Loss: 8.665789\n",
      "Epoch 275/2000 | Train Loss: 111.312102\t\t Val Loss: 8.799182\n",
      "Epoch 276/2000 | Train Loss: 110.390604\t\t Val Loss: 8.382020\n",
      "Epoch 277/2000 | Train Loss: 111.336679\t\t Val Loss: 8.463790\n",
      "Epoch 278/2000 | Train Loss: 107.502640\t\t Val Loss: 8.517391\n",
      "Epoch 279/2000 | Train Loss: 111.686992\t\t Val Loss: 8.541820\n",
      "Epoch 280/2000 | Train Loss: 110.705738\t\t Val Loss: 8.501787\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr = learning_rate)\n",
    "log_data = []\n",
    "\n",
    "if loss_name == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "elif loss_name == \"mae\":\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "elif loss_name == \"SMAPE\":\n",
    "    loss_fn = SMAPE\n",
    "elif loss_name == \"mape\":\n",
    "    loss_fn = MAPE\n",
    "elif loss_name == \"MASE\":\n",
    "    loss_fn = MASE(target_y, target_y.shape[1])\n",
    "else:\n",
    "    raise Exception(\"Your loss name is not valid.\")\n",
    "\n",
    "## early stopping\n",
    "PATIENCE = 10\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "for epoc in range(num_train_epochs):\n",
    "    model_instance.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model_instance(X).prediction_outputs\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()*X.shape[0]\n",
    "\n",
    "    avg_train_loss = total_train_loss/len(train_dataloader.dataset)\n",
    "\n",
    "    model_instance.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yys = []\n",
    "        yyhats = []\n",
    "\n",
    "        for XX, yy in val_dataloader:\n",
    "            XX = XX.to(device)\n",
    "            yys.append(yy.to(device))\n",
    "            yyhats.append(model_instance(XX).prediction_outputs)\n",
    "\n",
    "        yyhat = torch.concat(yyhats)\n",
    "        yy = torch.concat(yys)\n",
    "\n",
    "        val_loss = loss_fn(yyhat, yy).item()\n",
    "\n",
    "    print(f\"Epoch {epoc+1}/{num_train_epochs} | Train Loss: {avg_train_loss:.6f}\\t\\t Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    log_data.append({\"epoch\": epoc, \"loss\": avg_train_loss, \"eval_loss\": val_loss})\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state_dict = model_instance.state_dict()   ## 저장 없이 결과물만 산출...\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f6c1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    yys = []\n",
    "    yyhats = []\n",
    "\n",
    "    for XX, yy in test_dataloader:\n",
    "        XX = XX.to(device)\n",
    "        yys.append(yy.to(device))\n",
    "        yyhats.append(model_instance(XX).prediction_outputs)\n",
    "\n",
    "    yyhat = torch.concat(yyhats)\n",
    "    yy = torch.concat(yys)\n",
    "\n",
    "    test_loss = loss_fn(yyhat, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab49e291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE: 3.929018497467041\n",
      "test MAE: 1.772931694984436\n",
      "test SMAPE: 2.4976139068603516\n"
     ]
    }
   ],
   "source": [
    "mseLoss = torch.nn.MSELoss()\n",
    "maeLoss = torch.nn.L1Loss()\n",
    "\n",
    "def smape(yy, yyhat):\n",
    "    numerator = 100*abs(yy - yyhat)\n",
    "    denominator = (abs(yy) + abs(yyhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "print(f\"test RMSE: {torch.sqrt(mseLoss(yyhat, yy))}\")\n",
    "print(f\"test MAE: {maeLoss(yyhat, yy)}\")\n",
    "print(f\"test SMAPE: {smape(yy, yyhat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddab20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
