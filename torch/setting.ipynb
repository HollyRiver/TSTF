{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4991e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 11:55:59.521546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    PatchTSTConfig, PatchTSTForPrediction,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fbee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "output_dir = \"./pretrained/MAE\"\n",
    "logging_dir = \"./logs/MAE\"\n",
    "loss = \"mae\"\n",
    "learning_rate = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7961096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "target_y = pd.read_csv(f\"../data/{data}/train_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "X_train = target_X[:-round(target_X.shape[0] * 0.2), :].astype(np.float32)  ## ??? 잘못된 것 아님?\n",
    "y_train = target_y[:-round(target_y.shape[0] * 0.2)].astype(np.float32)\n",
    "target_X_val = target_X[-round(target_X.shape[0] * 0.2):, :].astype(np.float32)\n",
    "target_y_val = target_y[-round(target_y.shape[0] * 0.2):].astype(np.float32)\n",
    "\n",
    "test_X  = pd.read_csv(f\"../data/{data}/val_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "test_y  = pd.read_csv(f\"../data/{data}/val_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "np.random.seed(2)\n",
    "random_indices1 = np.random.choice(pd.read_csv(\"../data/M4_train.csv\").iloc[:, (1):].index,\n",
    "                                   size=target_X.shape[0] * 20, replace=True)\n",
    "\n",
    "X_data = pd.read_csv(\"../data/M4_train.csv\").iloc[:, 1 + (24 * 0):].loc[random_indices1].values.astype(np.float32)\n",
    "y_data = pd.read_csv(\"../data/M4_test.csv\").iloc[:, 1:].loc[random_indices1].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b2b9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14460, 168)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2836e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSTconfig = PatchTSTConfig(\n",
    "    num_input_channels = 1,\n",
    "    context_length = X_data.shape[1],\n",
    "    prediction_length = y_data.shape[1],\n",
    "\n",
    "    patch_length = 16,\n",
    "    patch_stride = 16,\n",
    "    d_model = 128,\n",
    "    num_attention_heads = 16,\n",
    "    num_hidden_layers = 3,\n",
    "    ffn_dim = 256,\n",
    "    dropout = 0.2,\n",
    "    head_dropout = 0.2,\n",
    "    pooling_type = None,\n",
    "    channel_attention = False,\n",
    "    scaling = \"std\",\n",
    "    loss = loss,\n",
    "    pre_norm = True,\n",
    "    do_mask_input = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa63645",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PatchTSTForPrediction(TSTconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad5d9ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchTSTModel(\n",
       "  (scaler): PatchTSTScaler(\n",
       "    (scaler): PatchTSTStdScaler()\n",
       "  )\n",
       "  (patchifier): PatchTSTPatchify()\n",
       "  (masking): Identity()\n",
       "  (encoder): PatchTSTEncoder(\n",
       "    (embedder): PatchTSTEmbedding(\n",
       "      (input_embedding): Linear(in_features=16, out_features=128, bias=True)\n",
       "    )\n",
       "    (positional_encoder): PatchTSTPositionalEncoding(\n",
       "      (positional_dropout): Identity()\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x PatchTSTEncoderLayer(\n",
       "        (self_attn): PatchTSTAttention(\n",
       "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (dropout_path1): Identity()\n",
       "        (norm_sublayer1): PatchTSTBatchNorm(\n",
       "          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Identity()\n",
       "          (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (dropout_path3): Identity()\n",
       "        (norm_sublayer3): PatchTSTBatchNorm(\n",
       "          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91eb7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "select = np.random.choice(len(X_data), size=len(X_data), replace=True)\n",
    "X_bootstrap = X_data[select]\n",
    "y_bootstrap = y_data[select]\n",
    "\n",
    "val_split_index = int(len(X_bootstrap) * 0.8)\n",
    "X_train, X_valid = X_bootstrap[:val_split_index], X_bootstrap[val_split_index:]\n",
    "y_train, y_valid = y_bootstrap[:val_split_index], y_bootstrap[val_split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4812b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_dataset(x, y):\n",
    "    x_list = [s[..., np.newaxis] for s in x]    ## (N, 168) -> (N, 168, 1)\n",
    "    y_list = [s[..., np.newaxis] for s in y]    ## (N, 24) -> (N, 24, 1)\n",
    "\n",
    "    data_dict = {\n",
    "        \"past_values\": x_list,\n",
    "        \"future_values\": y_list\n",
    "    }\n",
    "\n",
    "    return Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd3e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_hf_dataset(X_train, y_train)\n",
    "test_dataset = create_hf_dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "226c7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    overwrite_output_dir = True,\n",
    "    learning_rate = learning_rate,\n",
    "    num_train_epochs = 100,\n",
    "    do_eval = True,\n",
    "    eval_strategy = \"epoch\",\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    dataloader_num_workers = 16,\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_total_limit = 3,\n",
    "    logging_dir = logging_dir,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    greater_is_better = False,\n",
    "    label_names = [\"future_values\"]\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience = 10,\n",
    "    early_stopping_threshold = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "957e3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    callbacks = [early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a252b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18100' max='18100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18100/18100 06:57, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.105100</td>\n",
       "      <td>7.908236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.825900</td>\n",
       "      <td>7.580915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.522100</td>\n",
       "      <td>7.357135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.391600</td>\n",
       "      <td>7.273916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.337200</td>\n",
       "      <td>7.225021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.304300</td>\n",
       "      <td>7.193517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.276000</td>\n",
       "      <td>7.163679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.249500</td>\n",
       "      <td>7.132695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.229400</td>\n",
       "      <td>7.111053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.210600</td>\n",
       "      <td>7.091843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.195500</td>\n",
       "      <td>7.077294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.182000</td>\n",
       "      <td>7.063362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7.169400</td>\n",
       "      <td>7.051247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>7.163200</td>\n",
       "      <td>7.035276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.151400</td>\n",
       "      <td>7.027205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.143000</td>\n",
       "      <td>7.017769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.130700</td>\n",
       "      <td>7.007879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.126800</td>\n",
       "      <td>6.998003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>7.116900</td>\n",
       "      <td>6.990531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.111200</td>\n",
       "      <td>6.982357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>7.106000</td>\n",
       "      <td>6.977538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>7.097600</td>\n",
       "      <td>6.980011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>7.096700</td>\n",
       "      <td>6.968505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>7.090800</td>\n",
       "      <td>6.962478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.086900</td>\n",
       "      <td>6.960082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>7.078100</td>\n",
       "      <td>6.949965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>7.076000</td>\n",
       "      <td>6.944128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>7.068200</td>\n",
       "      <td>6.944336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>7.067900</td>\n",
       "      <td>6.944358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.062700</td>\n",
       "      <td>6.938585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>7.056900</td>\n",
       "      <td>6.936368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>7.054100</td>\n",
       "      <td>6.933892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>7.053400</td>\n",
       "      <td>6.924750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>7.054000</td>\n",
       "      <td>6.925574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>7.044600</td>\n",
       "      <td>6.924174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>7.037600</td>\n",
       "      <td>6.915505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>7.039400</td>\n",
       "      <td>6.911456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>7.034600</td>\n",
       "      <td>6.911380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>7.033100</td>\n",
       "      <td>6.909254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.030500</td>\n",
       "      <td>6.909958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>7.028000</td>\n",
       "      <td>6.906850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>7.027300</td>\n",
       "      <td>6.903348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>7.022200</td>\n",
       "      <td>6.897987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>7.020700</td>\n",
       "      <td>6.898264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>7.016500</td>\n",
       "      <td>6.899726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>7.014000</td>\n",
       "      <td>6.892166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>7.011600</td>\n",
       "      <td>6.896930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>7.013600</td>\n",
       "      <td>6.891553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>7.011500</td>\n",
       "      <td>6.889760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.005700</td>\n",
       "      <td>6.900253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>7.001400</td>\n",
       "      <td>6.884136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>7.003000</td>\n",
       "      <td>6.886276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>6.999800</td>\n",
       "      <td>6.891626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.878725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>6.996700</td>\n",
       "      <td>6.883390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>6.995800</td>\n",
       "      <td>6.879423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>6.992700</td>\n",
       "      <td>6.878166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>6.991700</td>\n",
       "      <td>6.878151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>6.995400</td>\n",
       "      <td>6.877538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.991100</td>\n",
       "      <td>6.878442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>6.984200</td>\n",
       "      <td>6.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>6.986300</td>\n",
       "      <td>6.874624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>6.983800</td>\n",
       "      <td>6.869507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>6.983500</td>\n",
       "      <td>6.871613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>6.981900</td>\n",
       "      <td>6.868190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>6.984100</td>\n",
       "      <td>6.867797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>6.983700</td>\n",
       "      <td>6.875766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>6.977500</td>\n",
       "      <td>6.868877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>6.978100</td>\n",
       "      <td>6.866464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>6.979700</td>\n",
       "      <td>6.866239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>6.974200</td>\n",
       "      <td>6.864843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>6.973400</td>\n",
       "      <td>6.860814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>6.972300</td>\n",
       "      <td>6.867763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>6.973400</td>\n",
       "      <td>6.860997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>6.973000</td>\n",
       "      <td>6.862776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>6.970900</td>\n",
       "      <td>6.863934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>6.971900</td>\n",
       "      <td>6.859746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>6.968200</td>\n",
       "      <td>6.859987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>6.968800</td>\n",
       "      <td>6.861456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.969100</td>\n",
       "      <td>6.856580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>6.966300</td>\n",
       "      <td>6.860504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>6.964900</td>\n",
       "      <td>6.856911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>6.966800</td>\n",
       "      <td>6.857633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>6.963200</td>\n",
       "      <td>6.857907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>6.964500</td>\n",
       "      <td>6.860950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>6.962300</td>\n",
       "      <td>6.856442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>6.962600</td>\n",
       "      <td>6.858846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>6.962000</td>\n",
       "      <td>6.857146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>6.960800</td>\n",
       "      <td>6.858621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.966100</td>\n",
       "      <td>6.855208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>6.961800</td>\n",
       "      <td>6.855335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>6.959800</td>\n",
       "      <td>6.855149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>6.959900</td>\n",
       "      <td>6.853944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>6.960500</td>\n",
       "      <td>6.853755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>6.959000</td>\n",
       "      <td>6.855073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>6.961500</td>\n",
       "      <td>6.852376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>6.961900</td>\n",
       "      <td>6.852392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>6.959800</td>\n",
       "      <td>6.852762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>6.962000</td>\n",
       "      <td>6.854506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.960700</td>\n",
       "      <td>6.852770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18100, training_loss=7.0613434613475485, metrics={'train_runtime': 418.3653, 'train_samples_per_second': 2765.047, 'train_steps_per_second': 43.264, 'total_flos': 575014074163200.0, 'train_loss': 7.0613434613475485, 'epoch': 100.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21441498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- '실제 MAE' (Unscaled) 계산 시작 ---\n",
      "--- 훈련된 모델의 '실제 MAE' (Unscaled) ---\n",
      "Final Real MAE: 4368.487526203752\n"
     ]
    }
   ],
   "source": [
    "# 1. 훈련된 베스트 모델 로드\n",
    "best_model_path = \"./pretrained/MAE/checkpoint-17376\" # 베스트 모델 경로\n",
    "best_model = PatchTSTForPrediction.from_pretrained(best_model_path)\n",
    "best_model.eval()\n",
    "\n",
    "# 2. test_dataset으로 DataLoader 생성\n",
    "# (test_dataset은 'past_values'와 'future_values'를 포함하는 Hf Dataset)\n",
    "test_dataset.set_format(type='torch', columns=['past_values', 'future_values'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "total_real_mae = 0\n",
    "total_samples = 0\n",
    "\n",
    "print(\"--- '실제 MAE' (Unscaled) 계산 시작 ---\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 모델의 forward pass 실행\n",
    "        outputs = best_model(\n",
    "            past_values=batch['past_values'],\n",
    "            # [참고] 'future_values'를 전달하지 않아도 예측은 가능합니다.\n",
    "            # (전달하면 outputs.loss도 계산해줌)\n",
    "        )\n",
    "\n",
    "        if isinstance(outputs.prediction_outputs, tuple):\n",
    "            unscaled_preds = outputs.prediction_outputs[0]\n",
    "        else:\n",
    "            unscaled_preds = outputs.prediction_outputs # (튜플이 아닌 경우 대비)\n",
    "\n",
    "        unscaled_labels = batch['future_values']\n",
    "        \n",
    "        #    (배치 전체의 평균 MAE)\n",
    "        real_mae = torch.abs(unscaled_preds - unscaled_labels).mean()\n",
    "        \n",
    "        # (정확한 계산을 위해 배치 크기 가중 평균)\n",
    "        total_real_mae += real_mae.item() * len(batch['future_values'])\n",
    "        total_samples += len(batch['future_values'])\n",
    "\n",
    "final_real_mae = total_real_mae / total_samples\n",
    "print(f\"--- 훈련된 모델의 '실제 MAE' (Unscaled) ---\")\n",
    "print(f\"Final Real MAE: {final_real_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a32c038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 22:57:34.252156: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tbparse import SummaryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16d51789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 파일이 있는 폴더 경로\n",
    "log_dir = \"./logs/events.out.tfevents.1761999330.cd0dd4fe3564.737145.0\"\n",
    "\n",
    "# 1. 로그 폴더를 읽습니다.\n",
    "reader = SummaryReader(log_dir)\n",
    "\n",
    "# 2. 스칼라 값(loss 등)을 DataFrame으로 변환합니다.\n",
    "df_scalars = reader.scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39663eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 훈련된 모델의 '실제 MAE' (Unscaled) ---\n",
      "Final Real MAE: 360.81136456037126\n"
     ]
    }
   ],
   "source": [
    "# 훈련된 베스트 모델 로드\n",
    "best_model_path = \"./pretrained/checkpoint-12851\" # 베스트 모델 경로\n",
    "best_model = PatchTSTForPrediction.from_pretrained(best_model_path)\n",
    "best_model.eval()\n",
    "\n",
    "# 2. test_dataset으로 DataLoader 생성\n",
    "# (test_dataset은 'past_values'와 'future_values'를 포함하는 Hf Dataset)\n",
    "test_dataset.set_format(type='torch', columns=['past_values', 'future_values'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "total_real_mae = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 모델의 forward pass 실행\n",
    "        outputs = best_model(\n",
    "            past_values=batch['past_values']   ## attribute\n",
    "        )\n",
    "\n",
    "        if isinstance(outputs.prediction_outputs, tuple):\n",
    "            unscaled_preds = outputs.prediction_outputs[0]\n",
    "        else:\n",
    "            unscaled_preds = outputs.prediction_outputs # (튜플이 아닌 경우 대비)\n",
    "\n",
    "        unscaled_labels = batch['future_values']    ## label\n",
    "        \n",
    "        #    (배치 전체의 평균 MAE)\n",
    "        real_mae = torch.abs(unscaled_preds - unscaled_labels).mean()\n",
    "        \n",
    "        # (정확한 계산을 위해 배치 크기 가중 평균)\n",
    "        total_real_mae += real_mae.item() * len(batch['future_values'])\n",
    "        total_samples += len(batch['future_values'])\n",
    "\n",
    "final_real_mae = total_real_mae / total_samples\n",
    "print(f\"--- 훈련된 모델의 '실제 MAE' (Unscaled) ---\")\n",
    "print(f\"Final Real MAE: {final_real_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d671b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 훈련된 모델의 '실제 MSE' (Unscaled) ---\n",
      "Final Real MSE: 697501.4553393184\n"
     ]
    }
   ],
   "source": [
    "total_real_mse = 0\n",
    "total_samples = 0\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 모델의 forward pass 실행\n",
    "        outputs = best_model(\n",
    "            past_values=batch['past_values']   ## attribute\n",
    "        )\n",
    "\n",
    "        if isinstance(outputs.prediction_outputs, tuple):\n",
    "            unscaled_preds = outputs.prediction_outputs[0]\n",
    "        else:\n",
    "            unscaled_preds = outputs.prediction_outputs # (튜플이 아닌 경우 대비)\n",
    "\n",
    "        unscaled_labels = batch['future_values']    ## label\n",
    "        \n",
    "        real_mse = loss_fn(unscaled_preds, unscaled_labels)\n",
    "        \n",
    "        # (정확한 계산을 위해 배치 크기 가중 평균)\n",
    "        total_real_mae += real_mse.item() * len(batch['future_values'])\n",
    "        total_samples += len(batch['future_values'])\n",
    "\n",
    "final_real_mae = total_real_mae / total_samples\n",
    "print(f\"--- 훈련된 모델의 '실제 MSE' (Unscaled) ---\")\n",
    "print(f\"Final Real MSE: {final_real_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115446e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835.1655257129082"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_real_mae**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a37e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_preds = outputs.prediction_outputs\n",
    "loss = torch.nn.MSELoss()(unscaled_preds, unscaled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8129688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(222041.7812, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd10f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'past_values': tensor([[[3942.0000],\n",
       "          [3878.0000],\n",
       "          [3961.3301],\n",
       "          ...,\n",
       "          [2803.2300],\n",
       "          [2882.3799],\n",
       "          [2873.1399]],\n",
       " \n",
       "         [[1000.0000],\n",
       "          [ 996.0000],\n",
       "          [ 990.0000],\n",
       "          ...,\n",
       "          [1483.0000],\n",
       "          [1470.0000],\n",
       "          [1478.0000]],\n",
       " \n",
       "         [[1090.0000],\n",
       "          [1810.0000],\n",
       "          [1710.0000],\n",
       "          ...,\n",
       "          [4280.0000],\n",
       "          [4160.0000],\n",
       "          [3980.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[6195.3130],\n",
       "          [5702.5293],\n",
       "          [5317.4561],\n",
       "          ...,\n",
       "          [6554.5898],\n",
       "          [6141.6689],\n",
       "          [5777.8862]],\n",
       " \n",
       "         [[8210.8125],\n",
       "          [8264.2178],\n",
       "          [8222.5205],\n",
       "          ...,\n",
       "          [6681.6230],\n",
       "          [6582.5327],\n",
       "          [6502.2646]],\n",
       " \n",
       "         [[4167.0000],\n",
       "          [4561.0000],\n",
       "          [4574.0000],\n",
       "          ...,\n",
       "          [7551.0000],\n",
       "          [7693.0000],\n",
       "          [8011.0000]]]),\n",
       " 'future_values': tensor([[[2969.6399],\n",
       "          [3179.4399],\n",
       "          [3250.2300],\n",
       "          [3411.7000],\n",
       "          [3294.8401],\n",
       "          [3252.3601],\n",
       "          [3301.5500],\n",
       "          [3346.5500],\n",
       "          [3455.2700],\n",
       "          [3438.7400],\n",
       "          [3473.0500],\n",
       "          [3464.6201],\n",
       "          [3312.1399],\n",
       "          [3260.2600],\n",
       "          [3233.7300],\n",
       "          [3230.0000],\n",
       "          [3256.4199],\n",
       "          [3316.5400],\n",
       "          [3296.5000],\n",
       "          [3279.0500],\n",
       "          [3252.5000],\n",
       "          [3233.7600],\n",
       "          [3207.6399],\n",
       "          [3184.8101]],\n",
       " \n",
       "         [[1493.0000],\n",
       "          [1480.0000],\n",
       "          [1493.0000],\n",
       "          [1495.0000],\n",
       "          [1499.0000],\n",
       "          [1498.0000],\n",
       "          [1515.0000],\n",
       "          [1546.0000],\n",
       "          [1542.0000],\n",
       "          [1551.0000],\n",
       "          [1554.0000],\n",
       "          [1565.0000],\n",
       "          [1567.0000],\n",
       "          [1559.0000],\n",
       "          [1555.0000],\n",
       "          [1564.0000],\n",
       "          [1564.0000],\n",
       "          [1560.0000],\n",
       "          [1568.0000],\n",
       "          [1566.0000],\n",
       "          [1563.0000],\n",
       "          [1562.0000],\n",
       "          [1572.0000],\n",
       "          [1577.0000]],\n",
       " \n",
       "         [[3180.0000],\n",
       "          [3690.0000],\n",
       "          [4010.0000],\n",
       "          [3800.0000],\n",
       "          [4370.0000],\n",
       "          [3350.0000],\n",
       "          [4540.0000],\n",
       "          [4340.0000],\n",
       "          [4400.0000],\n",
       "          [4360.0000],\n",
       "          [4310.0000],\n",
       "          [4380.0000],\n",
       "          [3460.0000],\n",
       "          [3980.0000],\n",
       "          [4340.0000],\n",
       "          [4140.0000],\n",
       "          [4140.0000],\n",
       "          [3790.0000],\n",
       "          [4350.0000],\n",
       "          [4310.0000],\n",
       "          [4480.0000],\n",
       "          [4580.0000],\n",
       "          [4050.0000],\n",
       "          [4440.0000]],\n",
       " \n",
       "         [[3022.2800],\n",
       "          [3095.3999],\n",
       "          [3110.3101],\n",
       "          [3206.8201],\n",
       "          [3256.9600],\n",
       "          [3508.6101],\n",
       "          [3439.4199],\n",
       "          [3382.3799],\n",
       "          [3393.5000],\n",
       "          [3550.4399],\n",
       "          [3606.9399],\n",
       "          [3203.3301],\n",
       "          [3538.5901],\n",
       "          [3785.2000],\n",
       "          [3562.7500],\n",
       "          [3714.6101],\n",
       "          [3956.0100],\n",
       "          [4005.5801],\n",
       "          [4314.3999],\n",
       "          [4187.4902],\n",
       "          [4529.0400],\n",
       "          [5044.8999],\n",
       "          [5241.7598],\n",
       "          [5141.7798]],\n",
       " \n",
       "         [[4768.5498],\n",
       "          [4821.4556],\n",
       "          [4880.4121],\n",
       "          [4864.5542],\n",
       "          [4855.7734],\n",
       "          [4870.7798],\n",
       "          [4915.4614],\n",
       "          [4949.8770],\n",
       "          [5040.3711],\n",
       "          [5121.3066],\n",
       "          [5141.2769],\n",
       "          [5168.5898],\n",
       "          [5198.2422],\n",
       "          [5201.8193],\n",
       "          [5198.8193],\n",
       "          [5238.2305],\n",
       "          [5310.9731],\n",
       "          [5455.0547],\n",
       "          [5605.9014],\n",
       "          [5782.8062],\n",
       "          [5942.1611],\n",
       "          [6074.5132],\n",
       "          [6190.5703],\n",
       "          [6331.1001]],\n",
       " \n",
       "         [[3409.8000],\n",
       "          [3407.5000],\n",
       "          [3417.7000],\n",
       "          [3403.5000],\n",
       "          [3421.5000],\n",
       "          [3455.3000],\n",
       "          [3432.6001],\n",
       "          [3441.3000],\n",
       "          [3447.1001],\n",
       "          [3453.2000],\n",
       "          [3454.1001],\n",
       "          [3441.8999],\n",
       "          [3492.3999],\n",
       "          [3498.5000],\n",
       "          [3523.8000],\n",
       "          [3515.0000],\n",
       "          [3512.8000],\n",
       "          [3551.3000],\n",
       "          [3551.3000],\n",
       "          [3541.8000],\n",
       "          [3507.8999],\n",
       "          [3533.2000],\n",
       "          [3515.8000],\n",
       "          [3540.6001]],\n",
       " \n",
       "         [[ 545.0000],\n",
       "          [ 547.0000],\n",
       "          [ 559.0000],\n",
       "          [ 586.0000],\n",
       "          [ 590.0000],\n",
       "          [ 560.0000],\n",
       "          [ 551.0000],\n",
       "          [ 560.0000],\n",
       "          [ 583.0000],\n",
       "          [ 611.0000],\n",
       "          [ 642.0000],\n",
       "          [ 640.0000],\n",
       "          [ 751.0000],\n",
       "          [ 703.0000],\n",
       "          [ 623.0000],\n",
       "          [ 613.0000],\n",
       "          [ 597.0000],\n",
       "          [ 576.0000],\n",
       "          [ 572.0000],\n",
       "          [ 645.0000],\n",
       "          [ 888.0000],\n",
       "          [1098.0000],\n",
       "          [1091.0000],\n",
       "          [ 957.0000]],\n",
       " \n",
       "         [[8830.0000],\n",
       "          [8840.0000],\n",
       "          [8900.0000],\n",
       "          [8850.0000],\n",
       "          [8850.0000],\n",
       "          [8810.0000],\n",
       "          [8690.0000],\n",
       "          [8710.0000],\n",
       "          [8810.0000],\n",
       "          [8810.0000],\n",
       "          [8800.0000],\n",
       "          [8780.0000],\n",
       "          [8710.0000],\n",
       "          [8790.0000],\n",
       "          [8830.0000],\n",
       "          [8770.0000],\n",
       "          [8780.0000],\n",
       "          [8740.0000],\n",
       "          [8660.0000],\n",
       "          [8710.0000],\n",
       "          [8770.0000],\n",
       "          [8790.0000],\n",
       "          [8770.0000],\n",
       "          [8760.0000]],\n",
       " \n",
       "         [[ 958.6300],\n",
       "          [ 985.1500],\n",
       "          [1037.2000],\n",
       "          [1081.9700],\n",
       "          [1088.2400],\n",
       "          [1048.6200],\n",
       "          [1077.1000],\n",
       "          [1073.6600],\n",
       "          [1079.0900],\n",
       "          [1105.9000],\n",
       "          [1087.2500],\n",
       "          [1063.6400],\n",
       "          [1064.4900],\n",
       "          [1038.2700],\n",
       "          [1037.3199],\n",
       "          [1050.9600],\n",
       "          [1063.9800],\n",
       "          [1045.6600],\n",
       "          [1059.2900],\n",
       "          [1062.2900],\n",
       "          [1067.5000],\n",
       "          [1073.0200],\n",
       "          [1100.6500],\n",
       "          [1076.6100]],\n",
       " \n",
       "         [[5577.2446],\n",
       "          [5546.3374],\n",
       "          [4695.0566],\n",
       "          [4224.8525],\n",
       "          [4982.1284],\n",
       "          [5299.3579],\n",
       "          [4838.5264],\n",
       "          [4506.6274],\n",
       "          [6024.7402],\n",
       "          [5739.6431],\n",
       "          [6268.9517],\n",
       "          [6390.6465],\n",
       "          [6173.0752],\n",
       "          [5789.3398],\n",
       "          [5377.3975],\n",
       "          [5638.1836],\n",
       "          [5685.8530],\n",
       "          [6007.7017],\n",
       "          [6099.0332],\n",
       "          [5305.2793],\n",
       "          [6898.7134],\n",
       "          [7126.8193],\n",
       "          [6573.6255],\n",
       "          [5972.1704]],\n",
       " \n",
       "         [[6588.4062],\n",
       "          [6492.4985],\n",
       "          [6485.6943],\n",
       "          [6573.5093],\n",
       "          [6669.8291],\n",
       "          [6698.4160],\n",
       "          [6632.6675],\n",
       "          [6713.0576],\n",
       "          [6679.1147],\n",
       "          [6442.0518],\n",
       "          [6459.3037],\n",
       "          [6185.2715],\n",
       "          [6149.2124],\n",
       "          [6352.2446],\n",
       "          [6345.1382],\n",
       "          [6329.3267],\n",
       "          [6263.5693],\n",
       "          [6198.1025],\n",
       "          [6159.3345],\n",
       "          [6339.0527],\n",
       "          [6415.6514],\n",
       "          [6446.3896],\n",
       "          [6393.4438],\n",
       "          [6425.0742]],\n",
       " \n",
       "         [[7986.0000],\n",
       "          [7690.0000],\n",
       "          [7874.0000],\n",
       "          [7886.0000],\n",
       "          [8024.0000],\n",
       "          [7949.0000],\n",
       "          [8303.0000],\n",
       "          [8777.0000],\n",
       "          [8626.0000],\n",
       "          [9026.0000],\n",
       "          [8291.0000],\n",
       "          [8141.0000],\n",
       "          [7544.0000],\n",
       "          [8331.0000],\n",
       "          [8538.0000],\n",
       "          [8596.0000],\n",
       "          [8388.0000],\n",
       "          [8200.0000],\n",
       "          [8538.0000],\n",
       "          [8218.0000],\n",
       "          [8846.0000],\n",
       "          [8872.0000],\n",
       "          [9322.0000],\n",
       "          [8662.0000]]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    outputs = best_model(past_values = batch[\"past_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f87c60",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones_like(): argument 'input' (position 1) must be Tensor, not Column",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TF/lib/python3.12/site-packages/transformers/models/patchtst/modeling_patchtst.py:1652\u001b[39m, in \u001b[36mPatchTSTForPrediction.forward\u001b[39m\u001b[34m(self, past_values, past_observed_mask, future_values, output_hidden_states, output_attentions, return_dict)\u001b[39m\n\u001b[32m   1649\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1651\u001b[39m \u001b[38;5;66;03m# get model output\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1652\u001b[39m model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[38;5;66;03m# get output head\u001b[39;00m\n\u001b[32m   1660\u001b[39m y_hat = \u001b[38;5;28mself\u001b[39m.head(model_output.last_hidden_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/TF/lib/python3.12/site-packages/transformers/models/patchtst/modeling_patchtst.py:1147\u001b[39m, in \u001b[36mPatchTSTModel.forward\u001b[39m\u001b[34m(self, past_values, past_observed_mask, future_values, output_hidden_states, output_attentions, return_dict)\u001b[39m\n\u001b[32m   1142\u001b[39m output_hidden_states = (\n\u001b[32m   1143\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1144\u001b[39m )\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_observed_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     past_observed_mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[38;5;66;03m# x: tensor [bs x sequence_length x num_input_channels]\u001b[39;00m\n\u001b[32m   1150\u001b[39m scaled_past_values, loc, scale = \u001b[38;5;28mself\u001b[39m.scaler(past_values, past_observed_mask)\n",
      "\u001b[31mTypeError\u001b[39m: ones_like(): argument 'input' (position 1) must be Tensor, not Column"
     ]
    }
   ],
   "source": [
    "best_model(past_values = test_dataset[\"past_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f33344af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(4472.5435)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b942667",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_layers = base_loaded.layers[:-1]\n",
    "pretrained_model = Model(inputs=base_loaded.input, outputs=pretrained_layers[-1].output)\n",
    "\n",
    "inputs = Input(shape=(target_X.shape[1], 1))\n",
    "flat_inp = layers.Reshape((target_X.shape[1],), name='transfer_flatten')(inputs) \n",
    "pretrained_output = pretrained_model(flat_inp)                                   \n",
    "pretrained_output_reshaped = Reshape((target_y.shape[1], -1))(pretrained_output) \n",
    "\n",
    "x = Dense(128, activation='linear')(pretrained_output_reshaped)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='linear')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "out = Dense(1, activation='linear')(x)  # (B, T_out, 1)\n",
    "\n",
    "model_instance = Model(inputs=inputs, outputs=out)\n",
    "model_instance.compile(optimizer=Adam(learning_rate=lr_), loss=lossf)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=pt, verbose=0, restore_best_weights=True)\n",
    "csv_log   = CSVLogger(os.path.join(LOG_DIR, f'transfer_{str(lossf)}_lr{lr_}_run{i}.csv'))\n",
    "\n",
    "history = model_instance.fit(\n",
    "    target_X.reshape(-1, target_X.shape[1], 1), target_y.reshape(-1, target_y.shape[1], 1),\n",
    "    batch_size=batch_size_, epochs=epochs_, verbose=0,\n",
    "    callbacks=[early_stop, csv_log, PrintValLossEveryN(1)],\n",
    "    validation_data=(target_X_val.reshape(-1, target_X_val.shape[1], 1),\n",
    "                        target_y_val.reshape(-1, target_y_val.shape[1], 1))\n",
    ")\n",
    "\n",
    "pred_val = model_instance.predict(target_X_val.reshape(-1, target_X_val.shape[1], 1), verbose=0).reshape(-1, target_y.shape[1])\n",
    "pred_test = model_instance.predict(test_X.reshape(-1, test_X.shape[1], 1), verbose=0).reshape(-1, target_y.shape[1])\n",
    "\n",
    "model_pred_val.append(pred_val)\n",
    "model_pred_test.append(pred_test)\n",
    "history_mapes_G.append(history)\n",
    "print(f\"######################################################## Loaded & fine-tuned (FC) model {i}\")\n",
    "del model_instance\n",
    "del base_loaded\n",
    "del pretrained_model\n",
    "tf.keras.backend.clear_session()\n",
    "import gc; gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
