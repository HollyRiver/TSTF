{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb5b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 16:05:50.161959: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-01 16:05:50.254372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-01 16:05:51.714147: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    PatchTSTConfig,\n",
    "    PatchTSTForPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce569a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62e98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "target_y = pd.read_csv(f\"../data/{data}/train_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "X_train = target_X[:-round(target_X.shape[0] * 0.2), :].astype(np.float32)  ## ??? 왜 있는 거임?\n",
    "y_train = target_y[:-round(target_y.shape[0] * 0.2)].astype(np.float32)\n",
    "target_X_val = target_X[-round(target_X.shape[0] * 0.2):, :].astype(np.float32)\n",
    "target_y_val = target_y[-round(target_y.shape[0] * 0.2):].astype(np.float32)\n",
    "\n",
    "test_X  = pd.read_csv(f\"../data/{data}/val_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "test_y  = pd.read_csv(f\"../data/{data}/val_output_7.csv\").iloc[:, 1:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0541e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "random_indices1 = np.random.choice(pd.read_csv(\"../data/M4_train.csv\").iloc[:, (1):].index,\n",
    "                                   size=target_X.shape[0] * 20, replace=True)\n",
    "X_train = pd.read_csv(\"../data/M4_train.csv\").iloc[:, 1 + (24 * 0):].loc[random_indices1].values.astype(np.float32)\n",
    "y_train = pd.read_csv(\"../data/M4_test.csv\").iloc[:, 1:].loc[random_indices1].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9708b2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d76b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56af8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_dataset(data: np.array):\n",
    "    data_list = [series for series in data]\n",
    "    data_dict = {\"target_values\": data_list}\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "train_dataset = create_hf_dataset(X_train)\n",
    "test_dataset = create_hf_dataset(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6632a8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['target_values'],\n",
       "    num_rows: 14460\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424ba3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSTconfig = PatchTSTConfig(\n",
    "    num_input_channels = 1,\n",
    "    context_length = X_train.shape[1] + y_train.shape[1],\n",
    "    prediction_length = y_train.shape[1],\n",
    "    patch_length = 16,\n",
    "    patch_stride = 16,\n",
    "\n",
    "    ## masking\n",
    "    mask_type = \"random\",\n",
    "    random_mask_ratio = 0.4,\n",
    "\n",
    "    ## TF setting\n",
    "    d_model = 128,\n",
    "    num_attention_heads = 16,\n",
    "    num_hidden_layers=3,\n",
    "    ffn_dim=256,\n",
    "    dropout=0.2,\n",
    "    head_dropout=0.2,\n",
    "    pooling_type=None,\n",
    "    channel_attention=False,\n",
    "    scaling=\"std\",\n",
    "    loss=\"mse\",\n",
    "    pre_norm=True,\n",
    "    norm_type=\"batchnorm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af8ac22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PatchTSTForPrediction(TSTconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12b38dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14460, 168)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd14f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14460, 24)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1d1810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "\n",
    "X_bootstrap = X_train[select]\n",
    "y_bootstrap = y_train[select]\n",
    "\n",
    "val_split_index = int(len(X_bootstrap) * 0.8)\n",
    "X_train_split, X_val_split = X_bootstrap[:val_split_index], X_bootstrap[val_split_index:]\n",
    "y_train_split, y_val_split = y_bootstrap[:val_split_index], y_bootstrap[val_split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30e66a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2644.  , 2635.  , 2714.  , ..., 3657.  , 3493.  , 3486.  ],\n",
       "       [1000.  , 1004.  , 1000.  , ..., 1304.  , 1309.  , 1342.  ],\n",
       "       [ 780.  , 1230.  ,  790.  , ..., 1480.  , 1380.  ,  930.  ],\n",
       "       ...,\n",
       "       [7986.6 , 7945.7 , 7716.9 , ..., 5505.4 , 5457.3 , 5541.9 ],\n",
       "       [4702.  , 4437.  , 4562.  , ..., 3619.  , 3750.  , 3668.  ],\n",
       "       [ 603.23,  416.09,  589.32, ..., 1576.78, 1253.44, 1070.77]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8c517e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature: (past_values, past_observed_mask, future_values, output_hidden_states, output_attentions, return_dict, label_ids, label, future_values). The following columns have been ignored: [target_values]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     28\u001b[39m trainer = Trainer(\n\u001b[32m     29\u001b[39m     model=model,\n\u001b[32m     30\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pretrain\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF/lib/python3.12/site-packages/transformers/trainer.py:2375\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2373\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2374\u001b[39m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2375\u001b[39m train_dataloader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fsdp_xla_v2_enabled:\n\u001b[32m   2377\u001b[39m     train_dataloader = tpu_spmd_dataloader(train_dataloader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF/lib/python3.12/site-packages/transformers/trainer.py:1140\u001b[39m, in \u001b[36mTrainer.get_train_dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1138\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTrainer: training requires a train_dataset.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_train_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF/lib/python3.12/site-packages/transformers/trainer.py:1095\u001b[39m, in \u001b[36mTrainer._get_dataloader\u001b[39m\u001b[34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[39m\n\u001b[32m   1093\u001b[39m data_collator = \u001b[38;5;28mself\u001b[39m.data_collator\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, datasets.Dataset):\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m     dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1097\u001b[39m     data_collator = \u001b[38;5;28mself\u001b[39m._get_collator_with_removed_columns(\u001b[38;5;28mself\u001b[39m.data_collator, description=description)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF/lib/python3.12/site-packages/transformers/trainer.py:1021\u001b[39m, in \u001b[36mTrainer._remove_unused_columns\u001b[39m\u001b[34m(self, dataset, description)\u001b[39m\n\u001b[32m   1019\u001b[39m columns = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature_columns \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dataset.column_names]\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo columns in the dataset match the model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms forward method signature: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following columns have been ignored: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(ignored_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1025\u001b[39m     )\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(datasets.__version__) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m1.4.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1028\u001b[39m     dataset.set_format(\n\u001b[32m   1029\u001b[39m         \u001b[38;5;28mtype\u001b[39m=dataset.format[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m], columns=columns, format_kwargs=dataset.format[\u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1030\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No columns in the dataset match the model's forward method signature: (past_values, past_observed_mask, future_values, output_hidden_states, output_attentions, return_dict, label_ids, label, future_values). The following columns have been ignored: [target_values]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrained/patchtst/output/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=0.0001,\n",
    "    num_train_epochs=100,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    dataloader_num_workers=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    logging_dir=\"./pretrained/patchtst/logs/\",  # Make sure to specify a logging directory\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # For loss\n",
    "    label_names=[\"future_values\"],\n",
    ")\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.0001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# pretrain\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512942f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchTSTModel(\n",
       "  (scaler): PatchTSTScaler(\n",
       "    (scaler): PatchTSTStdScaler()\n",
       "  )\n",
       "  (patchifier): PatchTSTPatchify()\n",
       "  (masking): Identity()\n",
       "  (encoder): PatchTSTEncoder(\n",
       "    (embedder): PatchTSTEmbedding(\n",
       "      (input_embedding): Linear(in_features=16, out_features=128, bias=True)\n",
       "    )\n",
       "    (positional_encoder): PatchTSTPositionalEncoding(\n",
       "      (positional_dropout): Identity()\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x PatchTSTEncoderLayer(\n",
       "        (self_attn): PatchTSTAttention(\n",
       "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (dropout_path1): Identity()\n",
       "        (norm_sublayer1): PatchTSTBatchNorm(\n",
       "          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (1): GELUActivation()\n",
       "          (2): Identity()\n",
       "          (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (dropout_path3): Identity()\n",
       "        (norm_sublayer3): PatchTSTBatchNorm(\n",
       "          (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabcbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
