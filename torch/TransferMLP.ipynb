{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c44d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 11:53:05.856430: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    PatchTSTConfig, PatchTSTForPrediction,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "loss_name = \"mse\"\n",
    "model_num = 1\n",
    "model_path = \"./saved_models\"\n",
    "learning_rate = 1e-7\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e10038dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "target_y = pd.read_csv(f\"../data/{data}/train_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "target_X_val = target_X[-round(target_X.shape[0] * 0.2):, :].astype(np.float32)\n",
    "target_y_val = target_y[-round(target_y.shape[0] * 0.2):].astype(np.float32)\n",
    "target_X = target_X[:-round(target_X.shape[0] * 0.2), :].astype(np.float32)\n",
    "target_y = target_y[:-round(target_y.shape[0] * 0.2)].astype(np.float32)\n",
    "\n",
    "test_X  = pd.read_csv(f\"../data/{data}/val_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "test_y  = pd.read_csv(f\"../data/{data}/val_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "np.random.seed(2)\n",
    "random_indices1 = np.random.choice(pd.read_csv(\"../data/M4_train.csv\").iloc[:, (1):].index,\n",
    "                                   size=target_X.shape[0] * 20, replace=True)\n",
    "\n",
    "X_data = pd.read_csv(\"../data/M4_train.csv\").iloc[:, 1 + (24 * 0):].loc[random_indices1].values.astype(np.float32)\n",
    "y_data = pd.read_csv(\"../data/M4_test.csv\").iloc[:, 1:].loc[random_indices1].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e121127d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[59.35, 59.85, 60.13, ..., 63.74, 63.76, 63.28],\n",
       "       [60.58, 60.71, 60.25, ..., 61.8 , 62.09, 62.16],\n",
       "       [61.69, 60.69, 60.58, ..., 61.15, 61.05, 61.38],\n",
       "       ...,\n",
       "       [70.77, 70.95, 70.92, ..., 69.96, 69.96, 69.97],\n",
       "       [71.76, 71.64, 71.06, ..., 70.55, 70.94, 70.8 ],\n",
       "       [73.02, 72.86, 72.2 , ..., 71.47, 71.55, 71.3 ]],\n",
       "      shape=(528, 168), dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b412e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_dataset(x, y):\n",
    "    x_list = [s[..., np.newaxis] for s in x]    ## (N, 168) -> (N, 168, 1)\n",
    "    y_list = [s[..., np.newaxis] for s in y]    ## (N, 24) -> (N, 24, 1)\n",
    "\n",
    "    data_dict = {\n",
    "        \"past_values\": x_list,\n",
    "        \"future_values\": y_list\n",
    "    }\n",
    "\n",
    "    return Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae446d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "select = np.random.choice(len(X_data), size=len(X_data), replace=True)\n",
    "X_bootstrap = X_data[select]\n",
    "y_bootstrap = y_data[select]\n",
    "\n",
    "val_split_index = int(len(X_bootstrap) * 0.8)\n",
    "X_train, X_valid = X_bootstrap[:val_split_index], X_bootstrap[val_split_index:]\n",
    "y_train, y_valid = y_bootstrap[:val_split_index], y_bootstrap[val_split_index:]\n",
    "\n",
    "train_dataset = create_hf_dataset(X_train, y_train)\n",
    "test_dataset = create_hf_dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "887f2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, model_num+1):\n",
    "    current_path = os.path.join(model_path, f\"model_{loss_name}_{k}\")\n",
    "\n",
    "    backbone_model = PatchTSTForPrediction.from_pretrained(current_path).to(device)\n",
    "    backbone = backbone_model.model\n",
    "    config = backbone_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52519830",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PATCHES = config.patch_length\n",
    "D_MODEL = config.d_model\n",
    "T_OUT = target_y.shape[1]\n",
    "C_NEW = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99275f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_hf_dataset(target_X, target_y)\n",
    "val_dataset = create_hf_dataset(target_X_val, target_y_val)\n",
    "test_dataset = create_hf_dataset(test_X, test_y)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['past_values', 'future_values'])\n",
    "val_dataset.set_format(type='torch', columns=['past_values', 'future_values'])\n",
    "test_dataset.set_format(type='torch', columns=['past_values', 'future_values'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 8)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c900586",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be91d914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 168, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"past_values\"].shape  ## (B, t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c4444f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 10, 256])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone(past_values = batch[\"past_values\"].to(\"cuda:0\")).last_hidden_state.shape    ## (B, 1, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8ce8a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferModel(torch.nn.Module):\n",
    "    def __init__(self, body, t_out, c_new):\n",
    "        super().__init__()\n",
    "        self.body = body\n",
    "        self.t_out = t_out\n",
    "        body_out_features = body.encoder.layers[-1].ff[-1].out_features\n",
    "        self.c_new = c_new\n",
    "        \n",
    "        self.flatten = torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.adapter = torch.nn.Linear(body_out_features*10, self.t_out * self.c_new)  ## Dense(128)\n",
    "\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.body(past_values=x).last_hidden_state\n",
    "        flat_feat = self.flatten(features)  ## (B, body_out_features*10)\n",
    "        adapted_feat = self.adapter(flat_feat)\n",
    "        head_input = adapted_feat.view(-1, self.t_out, self.c_new)  ## (B, 24, 128)\n",
    "        output = self.head(head_input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a545526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = TransferModel(backbone, T_OUT, C_NEW).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c3886e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 24, 1])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_instance(batch[\"past_values\"].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "19fe7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aeab2cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['past_values', 'future_values'])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ba74a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 168, 1])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"past_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cfa48b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000 - Train Loss: 26554.466702 - Val Loss: 4728.464389\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4728.464389)\n",
      "Epoch 2/2000 - Train Loss: 26254.817364 - Val Loss: 4611.591836\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4611.591836)\n",
      "Epoch 3/2000 - Train Loss: 25960.967809 - Val Loss: 4496.641962\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4496.641962)\n",
      "Epoch 4/2000 - Train Loss: 25678.530838 - Val Loss: 4383.305945\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4383.305945)\n",
      "Epoch 5/2000 - Train Loss: 25393.768440 - Val Loss: 4271.443834\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4271.443834)\n",
      "Epoch 6/2000 - Train Loss: 25089.297174 - Val Loss: 4160.696126\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4160.696126)\n",
      "Epoch 7/2000 - Train Loss: 24814.659719 - Val Loss: 4050.924530\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4050.924530)\n",
      "Epoch 8/2000 - Train Loss: 24541.910852 - Val Loss: 3942.260525\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3942.260525)\n",
      "Epoch 9/2000 - Train Loss: 24252.273091 - Val Loss: 3834.227241\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3834.227241)\n",
      "Epoch 10/2000 - Train Loss: 23973.606479 - Val Loss: 3727.337052\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3727.337052)\n",
      "Epoch 11/2000 - Train Loss: 23690.665208 - Val Loss: 3621.581523\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3621.581523)\n",
      "Epoch 12/2000 - Train Loss: 23405.591452 - Val Loss: 3516.540772\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3516.540772)\n",
      "Epoch 13/2000 - Train Loss: 23123.861511 - Val Loss: 3412.549501\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3412.549501)\n",
      "Epoch 14/2000 - Train Loss: 22833.827970 - Val Loss: 3309.397812\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3309.397812)\n",
      "Epoch 15/2000 - Train Loss: 22556.620222 - Val Loss: 3207.313443\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3207.313443)\n",
      "Epoch 16/2000 - Train Loss: 22257.171946 - Val Loss: 3106.553300\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3106.553300)\n",
      "Epoch 17/2000 - Train Loss: 22002.848727 - Val Loss: 3006.676932\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3006.676932)\n",
      "Epoch 18/2000 - Train Loss: 21707.188292 - Val Loss: 2907.997111\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2907.997111)\n",
      "Epoch 19/2000 - Train Loss: 21415.829875 - Val Loss: 2810.338246\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2810.338246)\n",
      "Epoch 20/2000 - Train Loss: 21134.926294 - Val Loss: 2713.718509\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2713.718509)\n",
      "Epoch 21/2000 - Train Loss: 20858.785456 - Val Loss: 2618.901078\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2618.901078)\n",
      "Epoch 22/2000 - Train Loss: 20568.055607 - Val Loss: 2525.101876\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2525.101876)\n",
      "Epoch 23/2000 - Train Loss: 20280.208007 - Val Loss: 2432.986247\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2432.986247)\n",
      "Epoch 24/2000 - Train Loss: 20008.396121 - Val Loss: 2341.893409\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2341.893409)\n",
      "Epoch 25/2000 - Train Loss: 19757.339931 - Val Loss: 2252.768764\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2252.768764)\n",
      "Epoch 26/2000 - Train Loss: 19450.655585 - Val Loss: 2164.894040\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2164.894040)\n",
      "Epoch 27/2000 - Train Loss: 19174.965878 - Val Loss: 2079.045193\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2079.045193)\n",
      "Epoch 28/2000 - Train Loss: 18904.883730 - Val Loss: 1994.494049\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1994.494049)\n",
      "Epoch 29/2000 - Train Loss: 18611.616638 - Val Loss: 1911.916025\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1911.916025)\n",
      "Epoch 30/2000 - Train Loss: 18333.031154 - Val Loss: 1830.769365\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1830.769365)\n",
      "Epoch 31/2000 - Train Loss: 18045.401374 - Val Loss: 1751.479745\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1751.479745)\n",
      "Epoch 32/2000 - Train Loss: 17812.750743 - Val Loss: 1674.249103\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1674.249103)\n",
      "Epoch 33/2000 - Train Loss: 17531.874248 - Val Loss: 1599.201724\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1599.201724)\n",
      "Epoch 34/2000 - Train Loss: 17250.325136 - Val Loss: 1526.010654\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1526.010654)\n",
      "Epoch 35/2000 - Train Loss: 16982.941095 - Val Loss: 1454.774224\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1454.774224)\n",
      "Epoch 36/2000 - Train Loss: 16699.296666 - Val Loss: 1385.653999\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1385.653999)\n",
      "Epoch 37/2000 - Train Loss: 16433.256831 - Val Loss: 1318.459220\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1318.459220)\n",
      "Epoch 38/2000 - Train Loss: 16163.091230 - Val Loss: 1253.821132\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1253.821132)\n",
      "Epoch 39/2000 - Train Loss: 15893.460942 - Val Loss: 1191.081113\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1191.081113)\n",
      "Epoch 40/2000 - Train Loss: 15663.337575 - Val Loss: 1130.888992\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1130.888992)\n",
      "Epoch 41/2000 - Train Loss: 15403.591172 - Val Loss: 1072.897728\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1072.897728)\n",
      "Epoch 42/2000 - Train Loss: 15125.153266 - Val Loss: 1017.064914\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1017.064914)\n",
      "Epoch 43/2000 - Train Loss: 14879.691599 - Val Loss: 963.839054\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 963.839054)\n",
      "Epoch 44/2000 - Train Loss: 14602.250527 - Val Loss: 912.718345\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 912.718345)\n",
      "Epoch 45/2000 - Train Loss: 14377.915766 - Val Loss: 864.334159\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 864.334159)\n",
      "Epoch 46/2000 - Train Loss: 14106.301324 - Val Loss: 818.299086\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 818.299086)\n",
      "Epoch 47/2000 - Train Loss: 13879.535611 - Val Loss: 774.642403\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 774.642403)\n",
      "Epoch 48/2000 - Train Loss: 13636.999260 - Val Loss: 733.690151\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 733.690151)\n",
      "Epoch 49/2000 - Train Loss: 13385.185214 - Val Loss: 695.185696\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 695.185696)\n",
      "Epoch 50/2000 - Train Loss: 13157.228556 - Val Loss: 659.234765\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 659.234765)\n",
      "Epoch 51/2000 - Train Loss: 12936.622071 - Val Loss: 625.763594\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 625.763594)\n",
      "Epoch 52/2000 - Train Loss: 12697.134203 - Val Loss: 595.062490\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 595.062490)\n",
      "Epoch 53/2000 - Train Loss: 12459.711818 - Val Loss: 567.025284\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 567.025284)\n",
      "Epoch 54/2000 - Train Loss: 12245.924453 - Val Loss: 541.443969\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 541.443969)\n",
      "Epoch 55/2000 - Train Loss: 12024.381208 - Val Loss: 518.514464\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 518.514464)\n",
      "Epoch 56/2000 - Train Loss: 11791.605750 - Val Loss: 498.099121\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 498.099121)\n",
      "Epoch 57/2000 - Train Loss: 11585.176345 - Val Loss: 480.603436\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 480.603436)\n",
      "Epoch 58/2000 - Train Loss: 11365.828610 - Val Loss: 465.477574\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 465.477574)\n",
      "Epoch 59/2000 - Train Loss: 11170.498863 - Val Loss: 453.033535\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 453.033535)\n",
      "Epoch 60/2000 - Train Loss: 10964.545775 - Val Loss: 443.181346\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 443.181346)\n",
      "Epoch 61/2000 - Train Loss: 10768.641755 - Val Loss: 435.988705\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 435.988705)\n",
      "Epoch 62/2000 - Train Loss: 10544.273936 - Val Loss: 431.399239\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 431.399239)\n",
      "Epoch 63/2000 - Train Loss: 10337.884713 - Val Loss: 429.542511\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 429.542511)\n",
      "Epoch 64/2000 - Train Loss: 10167.970130 - Val Loss: 430.192366\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 65/2000 - Train Loss: 9972.706838 - Val Loss: 433.521781\n",
      "  > Val loss 개선 없음. (Patience: 2/10)\n",
      "Epoch 66/2000 - Train Loss: 9833.087504 - Val Loss: 439.300533\n",
      "  > Val loss 개선 없음. (Patience: 3/10)\n",
      "Epoch 67/2000 - Train Loss: 9598.000500 - Val Loss: 447.516578\n",
      "  > Val loss 개선 없음. (Patience: 4/10)\n",
      "Epoch 68/2000 - Train Loss: 9406.071626 - Val Loss: 458.407691\n",
      "  > Val loss 개선 없음. (Patience: 5/10)\n",
      "Epoch 69/2000 - Train Loss: 9252.530374 - Val Loss: 471.611867\n",
      "  > Val loss 개선 없음. (Patience: 6/10)\n",
      "Epoch 70/2000 - Train Loss: 9067.657037 - Val Loss: 487.378140\n",
      "  > Val loss 개선 없음. (Patience: 7/10)\n",
      "Epoch 71/2000 - Train Loss: 8900.589381 - Val Loss: 505.552741\n",
      "  > Val loss 개선 없음. (Patience: 8/10)\n",
      "Epoch 72/2000 - Train Loss: 8709.701821 - Val Loss: 526.213968\n",
      "  > Val loss 개선 없음. (Patience: 9/10)\n",
      "Epoch 73/2000 - Train Loss: 8579.411989 - Val Loss: 549.208289\n",
      "  > Val loss 개선 없음. (Patience: 10/10)\n",
      "--- 조기 종료 (Epoch 73) ---\n",
      "--- 훈련 종료 ---\n",
      "가장 좋았던 모델(Val Loss: 429.542511)을 로드했습니다.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2000\n",
    "PATIENCE = 10\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "BEST_MODEL_PATH = \"./result/test\"\n",
    "\n",
    "model_instance.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_instance.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch[\"past_values\"].to(device), batch[\"future_values\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_instance(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()*batch[\"past_values\"].shape[0]\n",
    "\n",
    "    train_loss = total_train_loss / train_dataset.num_rows\n",
    "\n",
    "    # evaluation\n",
    "    model_instance.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = val_batch[\"past_values\"].to(device), val_batch[\"future_values\"].to(device)\n",
    "            \n",
    "            val_outputs = model_instance(X_val_batch)\n",
    "            val_loss = loss_fn(val_outputs, y_val_batch)\n",
    "            total_val_loss += val_loss.item()*val_batch[\"past_values\"].shape[0]\n",
    "\n",
    "    val_loss = total_val_loss / val_dataset.num_rows\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Keras의 restore_best_weights=True (모델 저장)\n",
    "        torch.save(model_instance.state_dict(), BEST_MODEL_PATH) \n",
    "        print(f\"  > Val loss 개선. 베스트 모델 저장. (Loss: {best_val_loss:.6f})\")\n",
    "        patience_counter = 0 # 인내심 초기화\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  > Val loss 개선 없음. (Patience: {patience_counter}/{PATIENCE})\")\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"--- 조기 종료 (Epoch {epoch+1}) ---\")\n",
    "        break\n",
    "\n",
    "print(\"--- 훈련 종료 ---\")\n",
    "# Keras의 restore_best_weights (베스트 모델 불러오기)\n",
    "model_instance.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "print(f\"가장 좋았던 모델(Val Loss: {best_val_loss:.6f})을 로드했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0f6c1775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 24, 1])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_instance(batch[\"past_values\"].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ab49e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    preds.append(model_instance(batch[\"past_values\"].to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d0457fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[63.7900, 63.6200, 64.2500,  ..., 61.8000, 62.0900, 62.1600],\n",
       "        [61.2900, 62.5900, 61.7500,  ..., 61.1500, 61.0500, 61.3800],\n",
       "        [60.7600, 60.8100, 61.4700,  ..., 59.1000, 59.0200, 59.1000],\n",
       "        ...,\n",
       "        [69.9900, 69.9200, 70.2000,  ..., 70.5500, 70.9400, 70.8000],\n",
       "        [70.9300, 70.8000, 71.2400,  ..., 71.4700, 71.5500, 71.3000],\n",
       "        [71.0000, 70.8500, 71.0100,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8e440686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.5024)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.mean((torch.tensor(test_y) - torch.concat(preds, axis = 0).squeeze().data.to('cpu'))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "88a70088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.7543)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(abs(torch.tensor(test_y) - torch.concat(preds, axis = 0).squeeze().data.to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "50205e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(75.789345)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94ebff",
   "metadata": {},
   "source": [
    "> `16, 13` 수준으로 나옴. 일단 훨씬 나아지긴 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddab20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
