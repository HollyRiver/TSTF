{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c44d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import PatchTSTForPrediction\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c5a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "\n",
    "output_dir = \"saved_models\"\n",
    "log_dir = os.path.join('logstf', data)\n",
    "\n",
    "loss_name = \"MASE\"\n",
    "\n",
    "num_train_epochs = 300\n",
    "model_num = 1\n",
    "model_path = \"./saved_models\"\n",
    "learning_rate = 1e-6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10038dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "target_y = pd.read_csv(f\"../data/{data}/train_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "target_X_val = target_X[-round(target_X.shape[0] * 0.2):, :].astype(np.float32)\n",
    "target_y_val = target_y[-round(target_y.shape[0] * 0.2):].astype(np.float32)\n",
    "target_X = target_X[:-round(target_X.shape[0] * 0.2), :].astype(np.float32)\n",
    "target_y = target_y[:-round(target_y.shape[0] * 0.2)].astype(np.float32)\n",
    "\n",
    "test_X  = pd.read_csv(f\"../data/{data}/val_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "test_y  = pd.read_csv(f\"../data/{data}/val_output_7.csv\").iloc[:, 1:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87c87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_dataset(X, y):\n",
    "    X, y = torch.tensor(X), torch.tensor(y)\n",
    "    X = X.reshape(-1, X.shape[1], 1)\n",
    "    y = y.reshape(-1, y.shape[1], 1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_dataset = array_to_dataset(target_X, target_y)\n",
    "val_dataset = array_to_dataset(target_X_val, target_y_val)\n",
    "test_dataset = array_to_dataset(test_X, test_y)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 64)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1156405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Backbone 전용 아키텍쳐.\n",
    "    backbone_model.head.projection 자리에 삽입\n",
    "    backbone_model.head.dropout 자리에 torch.nn.Identity() 삽입 (드롭아웃 레이어를 제거하는 방법을 찾는 것이 더 깔끔하지만, 임시로)\n",
    "    \"\"\"\n",
    "    def __init__(self, t_out):\n",
    "        super().__init__()\n",
    "        self.t_out = t_out  ## 24. target_y.shape[1]\n",
    "\n",
    "        self.adapter = torch.nn.Linear(1792, self.t_out * 128)   ## Dense(128)\n",
    "\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.adapted_feat = self.adapter(x)\n",
    "        head_input = self.adapted_feat.view(-1, self.t_out, 128)  ## (B, 24, 128)\n",
    "        output = self.head(head_input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3fdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_OUT = target_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314ebf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, model_num+1):\n",
    "    current_path = os.path.join(model_path, f\"model_{loss_name}_{k}.pth\")\n",
    "\n",
    "    model_instance = PatchTSTForPrediction.from_pretrained(os.path.join(model_path, \"PatchTSTBackbone\"))\n",
    "    model_instance.load_state_dict(torch.load(current_path))\n",
    "\n",
    "    ## MLP 헤드 부착\n",
    "    model_instance.head.projection = TransferModel(T_OUT)\n",
    "    model_instance.head.dropout = torch.nn.Identity()\n",
    "    model_instance.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19fe7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom loss function\n",
    "def SMAPE(yhat, y):\n",
    "    numerator = 100*torch.abs(y - yhat)\n",
    "    denominator = (torch.abs(y) + torch.abs(yhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "def MAPE(y_pred, y_true, epsilon=1e-7):\n",
    "    denominator = torch.clamp(torch.abs(y_true), min=epsilon)       ## 분모에 0이 들어오는 것을 방지\n",
    "    abs_percent_error = torch.abs((y_true - y_pred) / denominator)\n",
    "\n",
    "    return torch.mean(100. * abs_percent_error)\n",
    "\n",
    "\n",
    "class MASE(torch.nn.Module):\n",
    "    def __init__(self, training_data, period = 1):\n",
    "        super().__init__()\n",
    "        ## 원본 코드 구현, 사실상 MAE와 동일, 잘못 짜여진 코드, 일단은 하던대로 할 것.\n",
    "        self.scale = torch.mean(torch.abs(torch.tensor(training_data[period:] - training_data[:-period])))\n",
    "    \n",
    "    def forward(self, yhat, y):\n",
    "        error = torch.abs(y - yhat)\n",
    "        return torch.mean(error) / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0e7de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 | Train Loss: 0.360258\t\t Val Loss: 0.108685\n",
      "Epoch 2/300 | Train Loss: 0.354039\t\t Val Loss: 0.107465\n",
      "Epoch 3/300 | Train Loss: 0.347369\t\t Val Loss: 0.105266\n",
      "Epoch 4/300 | Train Loss: 0.338614\t\t Val Loss: 0.103429\n",
      "Epoch 5/300 | Train Loss: 0.331023\t\t Val Loss: 0.100298\n",
      "Epoch 6/300 | Train Loss: 0.325545\t\t Val Loss: 0.095825\n",
      "Epoch 7/300 | Train Loss: 0.311174\t\t Val Loss: 0.094819\n",
      "Epoch 8/300 | Train Loss: 0.299749\t\t Val Loss: 0.091770\n",
      "Epoch 9/300 | Train Loss: 0.285591\t\t Val Loss: 0.085413\n",
      "Epoch 10/300 | Train Loss: 0.276684\t\t Val Loss: 0.083688\n",
      "Epoch 11/300 | Train Loss: 0.267485\t\t Val Loss: 0.082602\n",
      "Epoch 12/300 | Train Loss: 0.268595\t\t Val Loss: 0.075233\n",
      "Epoch 13/300 | Train Loss: 0.252494\t\t Val Loss: 0.069002\n",
      "Epoch 14/300 | Train Loss: 0.251141\t\t Val Loss: 0.066718\n",
      "Epoch 15/300 | Train Loss: 0.244459\t\t Val Loss: 0.067014\n",
      "Epoch 16/300 | Train Loss: 0.241532\t\t Val Loss: 0.063198\n",
      "Epoch 17/300 | Train Loss: 0.232976\t\t Val Loss: 0.062862\n",
      "Epoch 18/300 | Train Loss: 0.228530\t\t Val Loss: 0.062371\n",
      "Epoch 19/300 | Train Loss: 0.229362\t\t Val Loss: 0.061757\n",
      "Epoch 20/300 | Train Loss: 0.223208\t\t Val Loss: 0.060338\n",
      "Epoch 21/300 | Train Loss: 0.225102\t\t Val Loss: 0.061202\n",
      "Epoch 22/300 | Train Loss: 0.211573\t\t Val Loss: 0.060273\n",
      "Epoch 23/300 | Train Loss: 0.208800\t\t Val Loss: 0.059169\n",
      "Epoch 24/300 | Train Loss: 0.203891\t\t Val Loss: 0.058190\n",
      "Epoch 25/300 | Train Loss: 0.208225\t\t Val Loss: 0.057523\n",
      "Epoch 26/300 | Train Loss: 0.204376\t\t Val Loss: 0.056126\n",
      "Epoch 27/300 | Train Loss: 0.199161\t\t Val Loss: 0.057006\n",
      "Epoch 28/300 | Train Loss: 0.193277\t\t Val Loss: 0.055917\n",
      "Epoch 29/300 | Train Loss: 0.191234\t\t Val Loss: 0.055262\n",
      "Epoch 30/300 | Train Loss: 0.189263\t\t Val Loss: 0.055700\n",
      "Epoch 31/300 | Train Loss: 0.191718\t\t Val Loss: 0.056432\n",
      "Epoch 32/300 | Train Loss: 0.185745\t\t Val Loss: 0.054386\n",
      "Epoch 33/300 | Train Loss: 0.186625\t\t Val Loss: 0.053354\n",
      "Epoch 34/300 | Train Loss: 0.180955\t\t Val Loss: 0.053856\n",
      "Epoch 35/300 | Train Loss: 0.178638\t\t Val Loss: 0.053192\n",
      "Epoch 36/300 | Train Loss: 0.184609\t\t Val Loss: 0.053623\n",
      "Epoch 37/300 | Train Loss: 0.182137\t\t Val Loss: 0.052307\n",
      "Epoch 38/300 | Train Loss: 0.174470\t\t Val Loss: 0.055368\n",
      "Epoch 39/300 | Train Loss: 0.178623\t\t Val Loss: 0.053492\n",
      "Epoch 40/300 | Train Loss: 0.178693\t\t Val Loss: 0.052615\n",
      "Epoch 41/300 | Train Loss: 0.175831\t\t Val Loss: 0.052775\n",
      "Epoch 42/300 | Train Loss: 0.175808\t\t Val Loss: 0.052414\n",
      "Epoch 43/300 | Train Loss: 0.176367\t\t Val Loss: 0.051062\n",
      "Epoch 44/300 | Train Loss: 0.173688\t\t Val Loss: 0.051468\n",
      "Epoch 45/300 | Train Loss: 0.174863\t\t Val Loss: 0.051756\n",
      "Epoch 46/300 | Train Loss: 0.171435\t\t Val Loss: 0.051428\n",
      "Epoch 47/300 | Train Loss: 0.173021\t\t Val Loss: 0.050460\n",
      "Epoch 48/300 | Train Loss: 0.173054\t\t Val Loss: 0.054284\n",
      "Epoch 49/300 | Train Loss: 0.173531\t\t Val Loss: 0.053513\n",
      "Epoch 50/300 | Train Loss: 0.170104\t\t Val Loss: 0.050519\n",
      "Epoch 51/300 | Train Loss: 0.172148\t\t Val Loss: 0.050441\n",
      "Epoch 52/300 | Train Loss: 0.173065\t\t Val Loss: 0.050556\n",
      "Epoch 53/300 | Train Loss: 0.167744\t\t Val Loss: 0.050699\n",
      "Epoch 54/300 | Train Loss: 0.171487\t\t Val Loss: 0.051343\n",
      "Epoch 55/300 | Train Loss: 0.170560\t\t Val Loss: 0.051185\n",
      "Epoch 56/300 | Train Loss: 0.169846\t\t Val Loss: 0.050762\n",
      "Epoch 57/300 | Train Loss: 0.173335\t\t Val Loss: 0.052206\n",
      "Epoch 58/300 | Train Loss: 0.167710\t\t Val Loss: 0.050004\n",
      "Epoch 59/300 | Train Loss: 0.167505\t\t Val Loss: 0.050726\n",
      "Epoch 60/300 | Train Loss: 0.169265\t\t Val Loss: 0.049953\n",
      "Epoch 61/300 | Train Loss: 0.167916\t\t Val Loss: 0.050144\n",
      "Epoch 62/300 | Train Loss: 0.167818\t\t Val Loss: 0.051076\n",
      "Epoch 63/300 | Train Loss: 0.170274\t\t Val Loss: 0.052059\n",
      "Epoch 64/300 | Train Loss: 0.165484\t\t Val Loss: 0.049827\n",
      "Epoch 65/300 | Train Loss: 0.165011\t\t Val Loss: 0.051163\n",
      "Epoch 66/300 | Train Loss: 0.167833\t\t Val Loss: 0.052373\n",
      "Epoch 67/300 | Train Loss: 0.165732\t\t Val Loss: 0.050161\n",
      "Epoch 68/300 | Train Loss: 0.167538\t\t Val Loss: 0.050489\n",
      "Epoch 69/300 | Train Loss: 0.165703\t\t Val Loss: 0.049302\n",
      "Epoch 70/300 | Train Loss: 0.163850\t\t Val Loss: 0.051670\n",
      "Epoch 71/300 | Train Loss: 0.167378\t\t Val Loss: 0.049787\n",
      "Epoch 72/300 | Train Loss: 0.163397\t\t Val Loss: 0.049453\n",
      "Epoch 73/300 | Train Loss: 0.168105\t\t Val Loss: 0.049712\n",
      "Epoch 74/300 | Train Loss: 0.161296\t\t Val Loss: 0.049598\n",
      "Epoch 75/300 | Train Loss: 0.162184\t\t Val Loss: 0.050249\n",
      "Epoch 76/300 | Train Loss: 0.159911\t\t Val Loss: 0.049097\n",
      "Epoch 77/300 | Train Loss: 0.160550\t\t Val Loss: 0.050787\n",
      "Epoch 78/300 | Train Loss: 0.159743\t\t Val Loss: 0.049396\n",
      "Epoch 79/300 | Train Loss: 0.161283\t\t Val Loss: 0.049308\n",
      "Epoch 80/300 | Train Loss: 0.160825\t\t Val Loss: 0.049602\n",
      "Epoch 81/300 | Train Loss: 0.161992\t\t Val Loss: 0.050908\n",
      "Epoch 82/300 | Train Loss: 0.160095\t\t Val Loss: 0.050351\n",
      "Epoch 83/300 | Train Loss: 0.161847\t\t Val Loss: 0.050072\n",
      "Epoch 84/300 | Train Loss: 0.159430\t\t Val Loss: 0.049966\n",
      "Epoch 85/300 | Train Loss: 0.163039\t\t Val Loss: 0.049782\n",
      "Epoch 86/300 | Train Loss: 0.160285\t\t Val Loss: 0.049514\n",
      "Epoch 87/300 | Train Loss: 0.158931\t\t Val Loss: 0.049593\n",
      "Epoch 88/300 | Train Loss: 0.162400\t\t Val Loss: 0.049680\n",
      "Epoch 89/300 | Train Loss: 0.159781\t\t Val Loss: 0.049181\n",
      "Epoch 90/300 | Train Loss: 0.161065\t\t Val Loss: 0.050323\n",
      "Epoch 91/300 | Train Loss: 0.159122\t\t Val Loss: 0.049120\n",
      "Epoch 92/300 | Train Loss: 0.161713\t\t Val Loss: 0.049731\n",
      "Epoch 93/300 | Train Loss: 0.159059\t\t Val Loss: 0.049052\n",
      "Epoch 94/300 | Train Loss: 0.159848\t\t Val Loss: 0.049219\n",
      "Epoch 95/300 | Train Loss: 0.157717\t\t Val Loss: 0.048827\n",
      "Epoch 96/300 | Train Loss: 0.157380\t\t Val Loss: 0.050711\n",
      "Epoch 97/300 | Train Loss: 0.156233\t\t Val Loss: 0.049476\n",
      "Epoch 98/300 | Train Loss: 0.157381\t\t Val Loss: 0.049673\n",
      "Epoch 99/300 | Train Loss: 0.157289\t\t Val Loss: 0.049122\n",
      "Epoch 100/300 | Train Loss: 0.157296\t\t Val Loss: 0.048845\n",
      "Epoch 101/300 | Train Loss: 0.155405\t\t Val Loss: 0.050278\n",
      "Epoch 102/300 | Train Loss: 0.159114\t\t Val Loss: 0.051097\n",
      "Epoch 103/300 | Train Loss: 0.156360\t\t Val Loss: 0.050367\n",
      "Epoch 104/300 | Train Loss: 0.154437\t\t Val Loss: 0.049088\n",
      "Epoch 105/300 | Train Loss: 0.159027\t\t Val Loss: 0.049451\n",
      "Epoch 106/300 | Train Loss: 0.157284\t\t Val Loss: 0.049105\n",
      "Epoch 107/300 | Train Loss: 0.158708\t\t Val Loss: 0.048792\n",
      "Epoch 108/300 | Train Loss: 0.155368\t\t Val Loss: 0.049881\n",
      "Epoch 109/300 | Train Loss: 0.157437\t\t Val Loss: 0.049666\n",
      "Epoch 110/300 | Train Loss: 0.157907\t\t Val Loss: 0.049498\n",
      "Epoch 111/300 | Train Loss: 0.153977\t\t Val Loss: 0.049106\n",
      "Epoch 112/300 | Train Loss: 0.155475\t\t Val Loss: 0.049342\n",
      "Epoch 113/300 | Train Loss: 0.153424\t\t Val Loss: 0.049457\n",
      "Epoch 114/300 | Train Loss: 0.157357\t\t Val Loss: 0.049445\n",
      "Epoch 115/300 | Train Loss: 0.154676\t\t Val Loss: 0.050006\n",
      "Epoch 116/300 | Train Loss: 0.154304\t\t Val Loss: 0.049218\n",
      "Epoch 117/300 | Train Loss: 0.155015\t\t Val Loss: 0.049364\n",
      "Epoch 118/300 | Train Loss: 0.153337\t\t Val Loss: 0.049240\n",
      "Epoch 119/300 | Train Loss: 0.152595\t\t Val Loss: 0.048826\n",
      "Epoch 120/300 | Train Loss: 0.154669\t\t Val Loss: 0.049310\n",
      "Epoch 121/300 | Train Loss: 0.153788\t\t Val Loss: 0.048999\n",
      "Epoch 122/300 | Train Loss: 0.152648\t\t Val Loss: 0.049280\n",
      "Epoch 123/300 | Train Loss: 0.155079\t\t Val Loss: 0.049431\n",
      "Epoch 124/300 | Train Loss: 0.155275\t\t Val Loss: 0.048976\n",
      "Epoch 125/300 | Train Loss: 0.152455\t\t Val Loss: 0.049044\n",
      "Epoch 126/300 | Train Loss: 0.152841\t\t Val Loss: 0.048973\n",
      "Epoch 127/300 | Train Loss: 0.149059\t\t Val Loss: 0.049482\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr = learning_rate)\n",
    "log_data = []\n",
    "\n",
    "if loss_name == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "elif loss_name == \"mae\":\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "elif loss_name == \"SMAPE\":\n",
    "    loss_fn = SMAPE\n",
    "elif loss_name == \"mape\":\n",
    "    loss_fn = MAPE\n",
    "elif loss_name == \"MASE\":\n",
    "    loss_fn = MASE(target_y, target_y.shape[1])\n",
    "else:\n",
    "    raise Exception(\"Your loss name is not valid.\")\n",
    "\n",
    "## early stopping\n",
    "PATIENCE = 20\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "for epoc in range(num_train_epochs):\n",
    "    model_instance.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model_instance(X).prediction_outputs\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()*X.shape[0]\n",
    "\n",
    "    avg_train_loss = total_train_loss/len(train_dataloader.dataset)\n",
    "\n",
    "    model_instance.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yys = []\n",
    "        yyhats = []\n",
    "\n",
    "        for XX, yy in val_dataloader:\n",
    "            XX = XX.to(device)\n",
    "            yys.append(yy.to(device))\n",
    "            yyhats.append(model_instance(XX).prediction_outputs)\n",
    "\n",
    "        yyhat = torch.concat(yyhats)\n",
    "        yy = torch.concat(yys)\n",
    "\n",
    "        val_loss = loss_fn(yyhat, yy).item()\n",
    "\n",
    "    print(f\"Epoch {epoc+1}/{num_train_epochs} | Train Loss: {avg_train_loss:.6f}\\t\\t Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    log_data.append({\"epoch\": epoc, \"loss\": avg_train_loss, \"eval_loss\": val_loss})\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state_dict = model_instance.state_dict()   ## 저장 없이 결과물만 산출...\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894c25f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(log_data).to_csv(os.path.join(log_dir, f\"transfer_{loss_name}_lr{learning_rate}_run{1}.csv\"))\n",
    "\n",
    "model_instance.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ddab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yys = []\n",
    "    yyhats = []\n",
    "\n",
    "    for XX, yy in test_dataloader:\n",
    "        XX = XX.to(device)\n",
    "        yys.append(yy.to(device))\n",
    "        yyhats.append(model_instance(XX).prediction_outputs)\n",
    "\n",
    "    yyhat = torch.concat(yyhats)\n",
    "    yy = torch.concat(yys)\n",
    "\n",
    "    test_loss = loss_fn(yyhat, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c779bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE: 3.721747636795044\n",
      "test MAE: 1.4623256921768188\n",
      "test SMAPE: 2.091921806335449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([528, 24, 1])) that is different to the input size (torch.Size([528, 1, 24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/root/anaconda3/envs/TF/lib/python3.12/site-packages/torch/nn/modules/loss.py:132: UserWarning: Using a target size (torch.Size([528, 24, 1])) that is different to the input size (torch.Size([528, 1, 24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "mseLoss = torch.nn.MSELoss()\n",
    "maeLoss = torch.nn.L1Loss()\n",
    "\n",
    "def smape(yy, yyhat):\n",
    "    numerator = 100*abs(yy - yyhat)\n",
    "    denominator = (abs(yy) + abs(yyhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "print(f\"test RMSE: {torch.sqrt(mseLoss(yyhat, yy))}\")\n",
    "print(f\"test MAE: {maeLoss(yyhat, yy)}\")\n",
    "print(f\"test SMAPE: {smape(yy, yyhat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e98fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
