{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c44d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 23:15:09.537422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import PatchTSTForPrediction\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c5a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "\n",
    "output_dir = \"saved_models\"\n",
    "log_dir = os.path.join('logstf', data)\n",
    "\n",
    "loss_name = \"mse\"\n",
    "\n",
    "num_train_epochs = 300\n",
    "model_num = 1\n",
    "model_path = \"./saved_models\"\n",
    "learning_rate = 1e-6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10038dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "target_y = pd.read_csv(f\"../data/{data}/train_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "target_X_val = target_X[-round(target_X.shape[0] * 0.2):, :].astype(np.float32)\n",
    "target_y_val = target_y[-round(target_y.shape[0] * 0.2):].astype(np.float32)\n",
    "target_X = target_X[:-round(target_X.shape[0] * 0.2), :].astype(np.float32)\n",
    "target_y = target_y[:-round(target_y.shape[0] * 0.2)].astype(np.float32)\n",
    "\n",
    "test_X  = pd.read_csv(f\"../data/{data}/val_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "test_y  = pd.read_csv(f\"../data/{data}/val_output_7.csv\").iloc[:, 1:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87c87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_dataset(X, y):\n",
    "    X, y = torch.tensor(X), torch.tensor(y)\n",
    "    X = X.reshape(-1, X.shape[1], 1)\n",
    "    y = y.reshape(-1, y.shape[1], 1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_dataset = array_to_dataset(target_X, target_y)\n",
    "val_dataset = array_to_dataset(target_X_val, target_y_val)\n",
    "test_dataset = array_to_dataset(test_X, test_y)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 64)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ebf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, model_num+1):\n",
    "    current_path = os.path.join(model_path, f\"model_{loss_name}_{k}.pth\")\n",
    "\n",
    "    backbone_model = PatchTSTForPrediction.from_pretrained(os.path.join(model_path, \"PatchTSTBackbone\")).to(device)\n",
    "    backbone_model.load_state_dict(torch.load(current_path))\n",
    "    backbone = backbone_model.model     ## 헤드 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6340533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferAll(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(24, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 24)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        yhat = self.net(x.prediction_outputs.squeeze())\n",
    "\n",
    "        return yhat\n",
    "    \n",
    "model_instance = torch.nn.Sequential(\n",
    "    backbone_model,\n",
    "    TransferAll()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_OUT = target_y.shape[1]\n",
    "C_NEW = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransferMLP(torch.nn.Module):\n",
    "#     def __init__(self, body, t_out, c_new):\n",
    "#         super().__init__()\n",
    "#         self.body = body\n",
    "#         self.t_out = t_out\n",
    "#         self.c_new = c_new\n",
    "#         body_out_features = body.encoder.layers[-1].ff[-1].out_features\n",
    "\n",
    "#         self.flatten = torch.nn.Flatten(start_dim = 2, end_dim = -1)\n",
    "#         self.adapter = torch.nn.Linear(body_out_features*7, self.t_out * self.c_new)  ## Dense(128)\n",
    "\n",
    "#         self.head = torch.nn.Sequential(\n",
    "#             torch.nn.Dropout(0.2),\n",
    "#             torch.nn.Linear(128, 64),\n",
    "#             torch.nn.Dropout(0.2),\n",
    "#             torch.nn.Linear(64, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features = self.body(past_values=x).last_hidden_state\n",
    "#         flat_feat = self.flatten(features)  ## (B, body_out_features*10)\n",
    "#         adapted_feat = self.adapter(flat_feat)\n",
    "#         head_input = adapted_feat.view(-1, self.t_out, self.c_new)  ## (B, 24, 128)\n",
    "#         output = self.head(head_input)\n",
    "\n",
    "#         return output\n",
    "\n",
    "\n",
    "# model_instance = TransferMLP(backbone, T_OUT, C_NEW).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom loss function\n",
    "def SMAPE(yhat, y):\n",
    "    numerator = 100*torch.abs(y - yhat)\n",
    "    denominator = (torch.abs(y) + torch.abs(yhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "def MAPE(y_pred, y_true, epsilon=1e-7):\n",
    "    denominator = torch.clamp(torch.abs(y_true), min=epsilon)       ## 분모에 0이 들어오는 것을 방지\n",
    "    abs_percent_error = torch.abs((y_true - y_pred) / denominator)\n",
    "\n",
    "    return torch.mean(100. * abs_percent_error)\n",
    "\n",
    "\n",
    "class MASE(torch.nn.Module):\n",
    "    def __init__(self, training_data, period = 1):\n",
    "        super().__init__()\n",
    "        ## 원본 코드 구현, 사실상 MAE와 동일, 잘못 짜여진 코드, 일단은 하던대로 할 것.\n",
    "        self.scale = torch.mean(torch.abs(torch.tensor(training_data[period:] - training_data[:-period])))\n",
    "    \n",
    "    def forward(self, yhat, y):\n",
    "        error = torch.abs(y - yhat)\n",
    "        return torch.mean(error) / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e7de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 | Train Loss: 26763.466331\t\t Val Loss: 4877.003418\n",
      "Epoch 2/300 | Train Loss: 26759.033359\t\t Val Loss: 4875.743164\n",
      "Epoch 3/300 | Train Loss: 26754.698482\t\t Val Loss: 4873.718750\n",
      "Epoch 4/300 | Train Loss: 26749.697272\t\t Val Loss: 4872.794434\n",
      "Epoch 5/300 | Train Loss: 26745.558094\t\t Val Loss: 4870.468262\n",
      "Epoch 6/300 | Train Loss: 26740.279631\t\t Val Loss: 4869.871582\n",
      "Epoch 7/300 | Train Loss: 26735.570907\t\t Val Loss: 4868.115723\n",
      "Epoch 8/300 | Train Loss: 26730.364849\t\t Val Loss: 4866.382812\n",
      "Epoch 9/300 | Train Loss: 26725.104793\t\t Val Loss: 4864.151855\n",
      "Epoch 10/300 | Train Loss: 26719.639544\t\t Val Loss: 4862.168457\n",
      "Epoch 11/300 | Train Loss: 26714.222724\t\t Val Loss: 4861.132324\n",
      "Epoch 12/300 | Train Loss: 26709.405986\t\t Val Loss: 4859.150391\n",
      "Epoch 13/300 | Train Loss: 26702.729401\t\t Val Loss: 4855.653809\n",
      "Epoch 14/300 | Train Loss: 26697.281669\t\t Val Loss: 4853.259766\n",
      "Epoch 15/300 | Train Loss: 26690.540184\t\t Val Loss: 4850.000977\n",
      "Epoch 16/300 | Train Loss: 26683.998588\t\t Val Loss: 4847.559570\n",
      "Epoch 17/300 | Train Loss: 26677.262259\t\t Val Loss: 4847.421875\n",
      "Epoch 18/300 | Train Loss: 26671.076327\t\t Val Loss: 4845.328613\n",
      "Epoch 19/300 | Train Loss: 26664.489268\t\t Val Loss: 4846.327637\n",
      "Epoch 20/300 | Train Loss: 26657.529412\t\t Val Loss: 4843.896484\n",
      "Epoch 21/300 | Train Loss: 26650.437081\t\t Val Loss: 4837.594238\n",
      "Epoch 22/300 | Train Loss: 26643.441906\t\t Val Loss: 4840.455078\n",
      "Epoch 23/300 | Train Loss: 26635.276783\t\t Val Loss: 4835.471680\n",
      "Epoch 24/300 | Train Loss: 26628.077375\t\t Val Loss: 4827.308105\n",
      "Epoch 25/300 | Train Loss: 26621.439554\t\t Val Loss: 4828.319336\n",
      "Epoch 26/300 | Train Loss: 26613.027087\t\t Val Loss: 4828.385742\n",
      "Epoch 27/300 | Train Loss: 26605.017720\t\t Val Loss: 4819.646484\n",
      "Epoch 28/300 | Train Loss: 26596.009083\t\t Val Loss: 4817.652344\n",
      "Epoch 29/300 | Train Loss: 26587.123750\t\t Val Loss: 4815.875488\n",
      "Epoch 30/300 | Train Loss: 26578.413603\t\t Val Loss: 4809.955078\n",
      "Epoch 31/300 | Train Loss: 26571.017477\t\t Val Loss: 4803.471191\n",
      "Epoch 32/300 | Train Loss: 26562.157304\t\t Val Loss: 4814.896484\n",
      "Epoch 33/300 | Train Loss: 26552.372608\t\t Val Loss: 4801.579590\n",
      "Epoch 34/300 | Train Loss: 26541.606611\t\t Val Loss: 4805.163574\n",
      "Epoch 35/300 | Train Loss: 26532.257812\t\t Val Loss: 4798.594727\n",
      "Epoch 36/300 | Train Loss: 26523.937040\t\t Val Loss: 4783.815918\n",
      "Epoch 37/300 | Train Loss: 26510.493593\t\t Val Loss: 4797.292480\n",
      "Epoch 38/300 | Train Loss: 26503.794050\t\t Val Loss: 4787.218262\n",
      "Epoch 39/300 | Train Loss: 26493.047412\t\t Val Loss: 4787.271484\n",
      "Epoch 40/300 | Train Loss: 26482.661792\t\t Val Loss: 4784.733398\n",
      "Epoch 41/300 | Train Loss: 26471.770971\t\t Val Loss: 4769.555176\n",
      "Epoch 42/300 | Train Loss: 26462.488085\t\t Val Loss: 4769.141602\n",
      "Epoch 43/300 | Train Loss: 26451.005447\t\t Val Loss: 4776.896484\n",
      "Epoch 44/300 | Train Loss: 26439.127663\t\t Val Loss: 4765.786133\n",
      "Epoch 45/300 | Train Loss: 26426.138212\t\t Val Loss: 4771.097656\n",
      "Epoch 46/300 | Train Loss: 26418.523157\t\t Val Loss: 4757.917480\n",
      "Epoch 47/300 | Train Loss: 26408.008340\t\t Val Loss: 4746.600098\n",
      "Epoch 48/300 | Train Loss: 26391.638537\t\t Val Loss: 4751.018555\n",
      "Epoch 49/300 | Train Loss: 26381.080436\t\t Val Loss: 4748.330078\n",
      "Epoch 50/300 | Train Loss: 26366.461113\t\t Val Loss: 4751.616699\n",
      "Epoch 51/300 | Train Loss: 26353.185020\t\t Val Loss: 4740.061523\n",
      "Epoch 52/300 | Train Loss: 26344.288400\t\t Val Loss: 4752.056152\n",
      "Epoch 53/300 | Train Loss: 26332.983780\t\t Val Loss: 4736.262207\n",
      "Epoch 54/300 | Train Loss: 26315.674173\t\t Val Loss: 4715.340820\n",
      "Epoch 55/300 | Train Loss: 26303.029885\t\t Val Loss: 4722.849609\n",
      "Epoch 56/300 | Train Loss: 26292.438216\t\t Val Loss: 4716.860840\n",
      "Epoch 57/300 | Train Loss: 26281.266774\t\t Val Loss: 4723.097656\n",
      "Epoch 58/300 | Train Loss: 26264.789127\t\t Val Loss: 4725.026855\n",
      "Epoch 59/300 | Train Loss: 26251.937811\t\t Val Loss: 4712.823730\n",
      "Epoch 60/300 | Train Loss: 26239.697597\t\t Val Loss: 4693.647461\n",
      "Epoch 61/300 | Train Loss: 26223.580842\t\t Val Loss: 4677.522461\n",
      "Epoch 62/300 | Train Loss: 26211.463857\t\t Val Loss: 4692.157227\n",
      "Epoch 63/300 | Train Loss: 26192.935229\t\t Val Loss: 4695.271973\n",
      "Epoch 64/300 | Train Loss: 26183.831288\t\t Val Loss: 4664.074707\n",
      "Epoch 65/300 | Train Loss: 26164.383259\t\t Val Loss: 4687.792480\n",
      "Epoch 66/300 | Train Loss: 26151.665049\t\t Val Loss: 4671.991211\n",
      "Epoch 67/300 | Train Loss: 26143.530868\t\t Val Loss: 4657.778320\n",
      "Epoch 68/300 | Train Loss: 26120.835127\t\t Val Loss: 4667.838867\n",
      "Epoch 69/300 | Train Loss: 26107.351339\t\t Val Loss: 4678.371094\n",
      "Epoch 70/300 | Train Loss: 26095.264334\t\t Val Loss: 4656.243164\n",
      "Epoch 71/300 | Train Loss: 26078.931350\t\t Val Loss: 4651.770020\n",
      "Epoch 72/300 | Train Loss: 26065.457085\t\t Val Loss: 4626.380859\n",
      "Epoch 73/300 | Train Loss: 26043.868309\t\t Val Loss: 4645.382324\n",
      "Epoch 74/300 | Train Loss: 26032.292719\t\t Val Loss: 4624.162598\n",
      "Epoch 75/300 | Train Loss: 26028.526533\t\t Val Loss: 4620.951660\n",
      "Epoch 76/300 | Train Loss: 26002.333349\t\t Val Loss: 4625.458008\n",
      "Epoch 77/300 | Train Loss: 25981.613450\t\t Val Loss: 4602.759277\n",
      "Epoch 78/300 | Train Loss: 25963.233496\t\t Val Loss: 4608.325195\n",
      "Epoch 79/300 | Train Loss: 25945.540374\t\t Val Loss: 4581.526855\n",
      "Epoch 80/300 | Train Loss: 25932.314980\t\t Val Loss: 4611.575195\n",
      "Epoch 81/300 | Train Loss: 25920.046834\t\t Val Loss: 4595.474121\n",
      "Epoch 82/300 | Train Loss: 25899.513949\t\t Val Loss: 4587.462891\n",
      "Epoch 83/300 | Train Loss: 25883.495959\t\t Val Loss: 4605.056641\n",
      "Epoch 84/300 | Train Loss: 25860.418888\t\t Val Loss: 4551.777344\n",
      "Epoch 85/300 | Train Loss: 25841.958180\t\t Val Loss: 4547.772461\n",
      "Epoch 86/300 | Train Loss: 25832.388733\t\t Val Loss: 4585.568848\n",
      "Epoch 87/300 | Train Loss: 25823.005663\t\t Val Loss: 4533.216309\n",
      "Epoch 88/300 | Train Loss: 25786.360314\t\t Val Loss: 4574.728027\n",
      "Epoch 89/300 | Train Loss: 25775.639409\t\t Val Loss: 4584.188477\n",
      "Epoch 90/300 | Train Loss: 25756.550335\t\t Val Loss: 4541.826172\n",
      "Epoch 91/300 | Train Loss: 25740.800967\t\t Val Loss: 4542.548828\n",
      "Epoch 92/300 | Train Loss: 25722.286508\t\t Val Loss: 4541.073730\n",
      "Epoch 93/300 | Train Loss: 25703.866414\t\t Val Loss: 4509.072754\n",
      "Epoch 94/300 | Train Loss: 25685.272775\t\t Val Loss: 4581.291504\n",
      "Epoch 95/300 | Train Loss: 25663.364035\t\t Val Loss: 4505.145996\n",
      "Epoch 96/300 | Train Loss: 25648.956906\t\t Val Loss: 4510.436523\n",
      "Epoch 97/300 | Train Loss: 25625.548034\t\t Val Loss: 4525.180664\n",
      "Epoch 98/300 | Train Loss: 25607.171395\t\t Val Loss: 4505.929199\n",
      "Epoch 99/300 | Train Loss: 25581.349082\t\t Val Loss: 4503.169434\n",
      "Epoch 100/300 | Train Loss: 25565.622597\t\t Val Loss: 4474.349121\n",
      "Epoch 101/300 | Train Loss: 25547.524343\t\t Val Loss: 4475.241699\n",
      "Epoch 102/300 | Train Loss: 25530.453416\t\t Val Loss: 4431.855469\n",
      "Epoch 103/300 | Train Loss: 25507.900181\t\t Val Loss: 4479.712402\n",
      "Epoch 104/300 | Train Loss: 25492.750750\t\t Val Loss: 4450.747559\n",
      "Epoch 105/300 | Train Loss: 25459.514841\t\t Val Loss: 4405.111816\n",
      "Epoch 106/300 | Train Loss: 25439.266882\t\t Val Loss: 4450.613770\n",
      "Epoch 107/300 | Train Loss: 25421.321610\t\t Val Loss: 4455.057617\n",
      "Epoch 108/300 | Train Loss: 25417.815567\t\t Val Loss: 4396.793457\n",
      "Epoch 109/300 | Train Loss: 25379.945975\t\t Val Loss: 4450.390625\n",
      "Epoch 110/300 | Train Loss: 25352.050023\t\t Val Loss: 4463.258301\n",
      "Epoch 111/300 | Train Loss: 25338.331004\t\t Val Loss: 4413.656738\n",
      "Epoch 112/300 | Train Loss: 25309.764446\t\t Val Loss: 4430.438477\n",
      "Epoch 113/300 | Train Loss: 25282.923078\t\t Val Loss: 4364.608398\n",
      "Epoch 114/300 | Train Loss: 25276.583599\t\t Val Loss: 4370.535645\n",
      "Epoch 115/300 | Train Loss: 25248.452848\t\t Val Loss: 4366.023438\n",
      "Epoch 116/300 | Train Loss: 25240.162988\t\t Val Loss: 4391.341309\n",
      "Epoch 117/300 | Train Loss: 25217.465445\t\t Val Loss: 4386.629883\n",
      "Epoch 118/300 | Train Loss: 25192.271694\t\t Val Loss: 4369.177734\n",
      "Epoch 119/300 | Train Loss: 25171.470953\t\t Val Loss: 4392.164551\n",
      "Epoch 120/300 | Train Loss: 25143.224440\t\t Val Loss: 4298.602539\n",
      "Epoch 121/300 | Train Loss: 25120.863038\t\t Val Loss: 4319.990234\n",
      "Epoch 122/300 | Train Loss: 25079.373500\t\t Val Loss: 4325.943848\n",
      "Epoch 123/300 | Train Loss: 25068.949097\t\t Val Loss: 4336.715332\n",
      "Epoch 124/300 | Train Loss: 25063.391314\t\t Val Loss: 4327.393555\n",
      "Epoch 125/300 | Train Loss: 25025.437155\t\t Val Loss: 4303.572266\n",
      "Epoch 126/300 | Train Loss: 25000.023985\t\t Val Loss: 4311.702637\n",
      "Epoch 127/300 | Train Loss: 24970.474211\t\t Val Loss: 4270.635254\n",
      "Epoch 128/300 | Train Loss: 24944.234517\t\t Val Loss: 4281.150391\n",
      "Epoch 129/300 | Train Loss: 24927.023498\t\t Val Loss: 4309.444336\n",
      "Epoch 130/300 | Train Loss: 24913.263895\t\t Val Loss: 4302.029297\n",
      "Epoch 131/300 | Train Loss: 24893.182188\t\t Val Loss: 4301.796387\n",
      "Epoch 132/300 | Train Loss: 24869.908859\t\t Val Loss: 4257.506348\n",
      "Epoch 133/300 | Train Loss: 24822.784426\t\t Val Loss: 4195.299805\n",
      "Epoch 134/300 | Train Loss: 24793.282216\t\t Val Loss: 4167.991211\n",
      "Epoch 135/300 | Train Loss: 24795.931968\t\t Val Loss: 4275.975586\n",
      "Epoch 136/300 | Train Loss: 24770.208146\t\t Val Loss: 4223.829102\n",
      "Epoch 137/300 | Train Loss: 24739.856118\t\t Val Loss: 4191.373047\n",
      "Epoch 138/300 | Train Loss: 24714.480942\t\t Val Loss: 4187.565918\n",
      "Epoch 139/300 | Train Loss: 24676.782777\t\t Val Loss: 4208.163574\n",
      "Epoch 140/300 | Train Loss: 24663.558560\t\t Val Loss: 4178.592285\n",
      "Epoch 141/300 | Train Loss: 24645.232888\t\t Val Loss: 4126.464355\n",
      "Epoch 142/300 | Train Loss: 24604.252507\t\t Val Loss: 4172.158691\n",
      "Epoch 143/300 | Train Loss: 24579.055823\t\t Val Loss: 4192.651855\n",
      "Epoch 144/300 | Train Loss: 24575.515415\t\t Val Loss: 4163.351074\n",
      "Epoch 145/300 | Train Loss: 24546.552271\t\t Val Loss: 4056.042725\n",
      "Epoch 146/300 | Train Loss: 24506.113876\t\t Val Loss: 4157.029785\n",
      "Epoch 147/300 | Train Loss: 24473.629947\t\t Val Loss: 4150.567871\n",
      "Epoch 148/300 | Train Loss: 24456.474238\t\t Val Loss: 4082.847656\n",
      "Epoch 149/300 | Train Loss: 24406.532034\t\t Val Loss: 4136.092285\n",
      "Epoch 150/300 | Train Loss: 24390.276465\t\t Val Loss: 4107.202148\n",
      "Epoch 151/300 | Train Loss: 24359.149499\t\t Val Loss: 4186.640625\n",
      "Epoch 152/300 | Train Loss: 24333.470953\t\t Val Loss: 4052.254395\n",
      "Epoch 153/300 | Train Loss: 24310.921037\t\t Val Loss: 4041.952393\n",
      "Epoch 154/300 | Train Loss: 24283.903939\t\t Val Loss: 3982.299316\n",
      "Epoch 155/300 | Train Loss: 24262.173091\t\t Val Loss: 4003.019043\n",
      "Epoch 156/300 | Train Loss: 24238.490309\t\t Val Loss: 4108.063477\n",
      "Epoch 157/300 | Train Loss: 24211.707410\t\t Val Loss: 4076.683350\n",
      "Epoch 158/300 | Train Loss: 24195.103097\t\t Val Loss: 4021.917236\n",
      "Epoch 159/300 | Train Loss: 24148.783328\t\t Val Loss: 4006.844727\n",
      "Epoch 160/300 | Train Loss: 24133.956180\t\t Val Loss: 4106.434570\n",
      "Epoch 161/300 | Train Loss: 24110.473575\t\t Val Loss: 4020.430908\n",
      "Epoch 162/300 | Train Loss: 24068.555742\t\t Val Loss: 3995.503906\n",
      "Epoch 163/300 | Train Loss: 24033.979496\t\t Val Loss: 4031.256348\n",
      "Epoch 164/300 | Train Loss: 23996.627021\t\t Val Loss: 4045.145508\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr = learning_rate)\n",
    "log_data = []\n",
    "\n",
    "if loss_name == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "elif loss_name == \"mae\":\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "elif loss_name == \"SMAPE\":\n",
    "    loss_fn = SMAPE\n",
    "elif loss_name == \"mape\":\n",
    "    loss_fn = MAPE\n",
    "elif loss_name == \"MASE\":\n",
    "    loss_fn = MASE(target_y, target_y.shape[1])\n",
    "else:\n",
    "    raise Exception(\"Your loss name is not valid.\")\n",
    "\n",
    "## early stopping\n",
    "PATIENCE = 10\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "for epoc in range(num_train_epochs):\n",
    "    model_instance.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model_instance(X)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()*X.shape[0]\n",
    "\n",
    "    avg_train_loss = total_train_loss/len(train_dataloader.dataset)\n",
    "\n",
    "    model_instance.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yys = []\n",
    "        yyhats = []\n",
    "\n",
    "        for XX, yy in val_dataloader:\n",
    "            XX = XX.to(device)\n",
    "            yys.append(yy.to(device))\n",
    "            yyhats.append(model_instance(XX))\n",
    "\n",
    "        yyhat = torch.concat(yyhats)\n",
    "        yy = torch.concat(yys)\n",
    "\n",
    "        val_loss = loss_fn(yyhat, yy).item()\n",
    "\n",
    "    print(f\"Epoch {epoc+1}/{num_train_epochs} | Train Loss: {avg_train_loss:.6f}\\t\\t Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    log_data.append({\"epoch\": epoc, \"loss\": avg_train_loss, \"eval_loss\": val_loss})\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state_dict = model_instance.state_dict()   ## 저장 없이 결과물만 산출...\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c25f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(log_data).to_csv(os.path.join(log_dir, f\"transfer_{loss_name}_lr{learning_rate}_run{1}.csv\"))\n",
    "\n",
    "model_instance.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yys = []\n",
    "    yyhats = []\n",
    "\n",
    "    for XX, yy in test_dataloader:\n",
    "        XX = XX.to(device)\n",
    "        yys.append(yy.to(device))\n",
    "        yyhats.append(model_instance(XX))\n",
    "\n",
    "    yyhat = torch.concat(yyhats)\n",
    "    yy = torch.concat(yys)\n",
    "\n",
    "    test_loss = loss_fn(yyhat, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE: 58.111000061035156\n",
      "test MAE: 54.474143981933594\n",
      "test SMAPE: 53.27073287963867\n"
     ]
    }
   ],
   "source": [
    "mseLoss = torch.nn.MSELoss()\n",
    "maeLoss = torch.nn.L1Loss()\n",
    "\n",
    "def smape(yy, yyhat):\n",
    "    numerator = 100*abs(yy - yyhat)\n",
    "    denominator = (abs(yy) + abs(yyhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "print(f\"test RMSE: {torch.sqrt(mseLoss(yyhat, yy))}\")\n",
    "print(f\"test MAE: {maeLoss(yyhat, yy)}\")\n",
    "print(f\"test SMAPE: {smape(yy, yyhat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e98fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
