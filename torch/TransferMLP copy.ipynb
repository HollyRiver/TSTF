{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c44d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 23:32:07.179558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import PatchTSTForPrediction\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "\n",
    "output_dir = \"saved_models\"\n",
    "log_dir = os.path.join('logstf', data)\n",
    "\n",
    "loss_name = \"mse\"\n",
    "\n",
    "num_train_epochs = 300\n",
    "model_num = 1\n",
    "model_path = \"./saved_models\"\n",
    "learning_rate = 5e-5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e10038dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "target_y = pd.read_csv(f\"../data/{data}/train_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "target_X_val = target_X[-round(target_X.shape[0] * 0.2):, :].astype(np.float32)\n",
    "target_y_val = target_y[-round(target_y.shape[0] * 0.2):].astype(np.float32)\n",
    "target_X = target_X[:-round(target_X.shape[0] * 0.2), :].astype(np.float32)\n",
    "target_y = target_y[:-round(target_y.shape[0] * 0.2)].astype(np.float32)\n",
    "\n",
    "test_X  = pd.read_csv(f\"../data/{data}/val_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "test_y  = pd.read_csv(f\"../data/{data}/val_output_7.csv\").iloc[:, 1:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87c87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_dataset(X, y):\n",
    "    X, y = torch.tensor(X), torch.tensor(y)\n",
    "    X = X.reshape(-1, X.shape[1], 1)\n",
    "    y = y.reshape(-1, y.shape[1], 1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_dataset = array_to_dataset(target_X, target_y)\n",
    "val_dataset = array_to_dataset(target_X_val, target_y_val)\n",
    "test_dataset = array_to_dataset(test_X, test_y)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 64)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314ebf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, model_num+1):\n",
    "    current_path = os.path.join(model_path, f\"model_{loss_name}_{k}.pth\")\n",
    "\n",
    "    backbone_model = PatchTSTForPrediction.from_pretrained(os.path.join(model_path, \"PatchTSTBackbone\")).to(device)\n",
    "    backbone_model.load_state_dict(torch.load(current_path))\n",
    "    backbone = backbone_model.model     ## 헤드 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6340533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferAll(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(24, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 24)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        yhat = self.net(x.prediction_outputs.squeeze()).unsqueeze(-1)\n",
    "\n",
    "        return yhat\n",
    "    \n",
    "model_instance = torch.nn.Sequential(\n",
    "    backbone_model,\n",
    "    TransferAll()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db8f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_OUT = target_y.shape[1]\n",
    "C_NEW = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9352c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransferMLP(torch.nn.Module):\n",
    "#     def __init__(self, body, t_out, c_new):\n",
    "#         super().__init__()\n",
    "#         self.body = body\n",
    "#         self.t_out = t_out\n",
    "#         self.c_new = c_new\n",
    "#         body_out_features = body.encoder.layers[-1].ff[-1].out_features\n",
    "\n",
    "#         self.flatten = torch.nn.Flatten(start_dim = 2, end_dim = -1)\n",
    "#         self.adapter = torch.nn.Linear(body_out_features*7, self.t_out * self.c_new)  ## Dense(128)\n",
    "\n",
    "#         self.head = torch.nn.Sequential(\n",
    "#             torch.nn.Dropout(0.2),\n",
    "#             torch.nn.Linear(128, 64),\n",
    "#             torch.nn.Dropout(0.2),\n",
    "#             torch.nn.Linear(64, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features = self.body(past_values=x).last_hidden_state\n",
    "#         flat_feat = self.flatten(features)  ## (B, body_out_features*10)\n",
    "#         adapted_feat = self.adapter(flat_feat)\n",
    "#         head_input = adapted_feat.view(-1, self.t_out, self.c_new)  ## (B, 24, 128)\n",
    "#         output = self.head(head_input)\n",
    "\n",
    "#         return output\n",
    "\n",
    "\n",
    "# model_instance = TransferMLP(backbone, T_OUT, C_NEW).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19fe7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom loss function\n",
    "def SMAPE(yhat, y):\n",
    "    numerator = 100*torch.abs(y - yhat)\n",
    "    denominator = (torch.abs(y) + torch.abs(yhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "def MAPE(y_pred, y_true, epsilon=1e-7):\n",
    "    denominator = torch.clamp(torch.abs(y_true), min=epsilon)       ## 분모에 0이 들어오는 것을 방지\n",
    "    abs_percent_error = torch.abs((y_true - y_pred) / denominator)\n",
    "\n",
    "    return torch.mean(100. * abs_percent_error)\n",
    "\n",
    "\n",
    "class MASE(torch.nn.Module):\n",
    "    def __init__(self, training_data, period = 1):\n",
    "        super().__init__()\n",
    "        ## 원본 코드 구현, 사실상 MAE와 동일, 잘못 짜여진 코드, 일단은 하던대로 할 것.\n",
    "        self.scale = torch.mean(torch.abs(torch.tensor(training_data[period:] - training_data[:-period])))\n",
    "    \n",
    "    def forward(self, yhat, y):\n",
    "        error = torch.abs(y - yhat)\n",
    "        return torch.mean(error) / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e7de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 | Train Loss: 27034.300903\t\t Val Loss: 4839.958496\n",
      "Epoch 2/300 | Train Loss: 25668.222163\t\t Val Loss: 4584.832031\n",
      "Epoch 3/300 | Train Loss: 23266.442508\t\t Val Loss: 4115.758789\n",
      "Epoch 4/300 | Train Loss: 18411.634340\t\t Val Loss: 3231.677246\n",
      "Epoch 5/300 | Train Loss: 13990.411707\t\t Val Loss: 2794.347168\n",
      "Epoch 6/300 | Train Loss: 11110.073215\t\t Val Loss: 2320.848389\n",
      "Epoch 7/300 | Train Loss: 8967.936697\t\t Val Loss: 2034.925293\n",
      "Epoch 8/300 | Train Loss: 7147.844703\t\t Val Loss: 1909.369629\n",
      "Epoch 9/300 | Train Loss: 6153.691898\t\t Val Loss: 1470.545654\n",
      "Epoch 10/300 | Train Loss: 5068.477552\t\t Val Loss: 1365.906372\n",
      "Epoch 11/300 | Train Loss: 4716.503840\t\t Val Loss: 1325.486450\n",
      "Epoch 12/300 | Train Loss: 3640.812431\t\t Val Loss: 1079.680786\n",
      "Epoch 13/300 | Train Loss: 3406.379871\t\t Val Loss: 888.507446\n",
      "Epoch 14/300 | Train Loss: 4437.472443\t\t Val Loss: 912.155151\n",
      "Epoch 15/300 | Train Loss: 3072.488445\t\t Val Loss: 968.279541\n",
      "Epoch 16/300 | Train Loss: 2534.745957\t\t Val Loss: 849.386047\n",
      "Epoch 17/300 | Train Loss: 2520.444417\t\t Val Loss: 825.595825\n",
      "Epoch 18/300 | Train Loss: 2135.711762\t\t Val Loss: 788.983643\n",
      "Epoch 19/300 | Train Loss: 2417.946774\t\t Val Loss: 694.308899\n",
      "Epoch 20/300 | Train Loss: 2323.030706\t\t Val Loss: 678.271606\n",
      "Epoch 21/300 | Train Loss: 2281.804937\t\t Val Loss: 865.985718\n",
      "Epoch 22/300 | Train Loss: 1965.869671\t\t Val Loss: 759.854309\n",
      "Epoch 23/300 | Train Loss: 1848.018806\t\t Val Loss: 775.811157\n",
      "Epoch 24/300 | Train Loss: 1775.642045\t\t Val Loss: 566.953247\n",
      "Epoch 25/300 | Train Loss: 1687.631009\t\t Val Loss: 647.117310\n",
      "Epoch 26/300 | Train Loss: 1590.959287\t\t Val Loss: 597.861572\n",
      "Epoch 27/300 | Train Loss: 1877.665014\t\t Val Loss: 630.141113\n",
      "Epoch 28/300 | Train Loss: 1690.410740\t\t Val Loss: 700.720520\n",
      "Epoch 29/300 | Train Loss: 1699.793829\t\t Val Loss: 713.211182\n",
      "Epoch 30/300 | Train Loss: 1746.798812\t\t Val Loss: 652.265686\n",
      "Epoch 31/300 | Train Loss: 1691.323186\t\t Val Loss: 631.481445\n",
      "Epoch 32/300 | Train Loss: 1491.463398\t\t Val Loss: 599.439453\n",
      "Epoch 33/300 | Train Loss: 1454.416770\t\t Val Loss: 626.754150\n",
      "Epoch 34/300 | Train Loss: 1382.619490\t\t Val Loss: 527.568054\n",
      "Epoch 35/300 | Train Loss: 1350.603008\t\t Val Loss: 686.908569\n",
      "Epoch 36/300 | Train Loss: 1309.418751\t\t Val Loss: 554.480408\n",
      "Epoch 37/300 | Train Loss: 1108.948543\t\t Val Loss: 582.269043\n",
      "Epoch 38/300 | Train Loss: 1461.685499\t\t Val Loss: 539.633606\n",
      "Epoch 39/300 | Train Loss: 1706.356417\t\t Val Loss: 514.221436\n",
      "Epoch 40/300 | Train Loss: 1478.913814\t\t Val Loss: 487.830597\n",
      "Epoch 41/300 | Train Loss: 1456.029890\t\t Val Loss: 501.522186\n",
      "Epoch 42/300 | Train Loss: 1199.422688\t\t Val Loss: 535.944763\n",
      "Epoch 43/300 | Train Loss: 1061.127273\t\t Val Loss: 537.520325\n",
      "Epoch 44/300 | Train Loss: 1167.392715\t\t Val Loss: 470.952271\n",
      "Epoch 45/300 | Train Loss: 1305.635208\t\t Val Loss: 598.067688\n",
      "Epoch 46/300 | Train Loss: 1134.610323\t\t Val Loss: 506.735046\n",
      "Epoch 47/300 | Train Loss: 988.065560\t\t Val Loss: 526.540222\n",
      "Epoch 48/300 | Train Loss: 1117.733242\t\t Val Loss: 515.712219\n",
      "Epoch 49/300 | Train Loss: 1233.716415\t\t Val Loss: 489.072723\n",
      "Epoch 50/300 | Train Loss: 1050.295818\t\t Val Loss: 494.400848\n",
      "Epoch 51/300 | Train Loss: 1034.675918\t\t Val Loss: 536.020264\n",
      "Epoch 52/300 | Train Loss: 1109.349251\t\t Val Loss: 503.509308\n",
      "Epoch 53/300 | Train Loss: 928.154473\t\t Val Loss: 551.220032\n",
      "Epoch 54/300 | Train Loss: 904.013497\t\t Val Loss: 584.858704\n",
      "Epoch 55/300 | Train Loss: 797.917053\t\t Val Loss: 469.694244\n",
      "Epoch 56/300 | Train Loss: 956.967609\t\t Val Loss: 485.639893\n",
      "Epoch 57/300 | Train Loss: 1206.035028\t\t Val Loss: 529.379517\n",
      "Epoch 58/300 | Train Loss: 1399.970421\t\t Val Loss: 446.576874\n",
      "Epoch 59/300 | Train Loss: 1083.478846\t\t Val Loss: 529.120422\n",
      "Epoch 60/300 | Train Loss: 1028.494873\t\t Val Loss: 491.099060\n",
      "Epoch 61/300 | Train Loss: 859.711884\t\t Val Loss: 469.126190\n",
      "Epoch 62/300 | Train Loss: 862.614686\t\t Val Loss: 456.291412\n",
      "Epoch 63/300 | Train Loss: 951.723338\t\t Val Loss: 508.559753\n",
      "Epoch 64/300 | Train Loss: 1102.747205\t\t Val Loss: 541.810303\n",
      "Epoch 65/300 | Train Loss: 900.304749\t\t Val Loss: 471.792053\n",
      "Epoch 66/300 | Train Loss: 800.802878\t\t Val Loss: 441.414154\n",
      "Epoch 67/300 | Train Loss: 759.514363\t\t Val Loss: 465.147858\n",
      "Epoch 68/300 | Train Loss: 766.707082\t\t Val Loss: 447.790314\n",
      "Epoch 69/300 | Train Loss: 648.670943\t\t Val Loss: 470.520050\n",
      "Epoch 70/300 | Train Loss: 710.291531\t\t Val Loss: 445.832855\n",
      "Epoch 71/300 | Train Loss: 823.253059\t\t Val Loss: 432.456818\n",
      "Epoch 72/300 | Train Loss: 711.951988\t\t Val Loss: 487.827789\n",
      "Epoch 73/300 | Train Loss: 770.921642\t\t Val Loss: 419.574890\n",
      "Epoch 74/300 | Train Loss: 625.478955\t\t Val Loss: 473.276367\n",
      "Epoch 75/300 | Train Loss: 587.945776\t\t Val Loss: 414.514862\n",
      "Epoch 76/300 | Train Loss: 633.104988\t\t Val Loss: 421.106537\n",
      "Epoch 77/300 | Train Loss: 558.944092\t\t Val Loss: 422.431183\n",
      "Epoch 78/300 | Train Loss: 530.378739\t\t Val Loss: 399.735413\n",
      "Epoch 79/300 | Train Loss: 566.580716\t\t Val Loss: 398.799927\n",
      "Epoch 80/300 | Train Loss: 619.446530\t\t Val Loss: 453.592743\n",
      "Epoch 81/300 | Train Loss: 571.875296\t\t Val Loss: 431.554443\n",
      "Epoch 82/300 | Train Loss: 580.056532\t\t Val Loss: 438.969910\n",
      "Epoch 83/300 | Train Loss: 599.251191\t\t Val Loss: 510.034760\n",
      "Epoch 84/300 | Train Loss: 499.092175\t\t Val Loss: 489.510803\n",
      "Epoch 85/300 | Train Loss: 554.453815\t\t Val Loss: 368.891510\n",
      "Epoch 86/300 | Train Loss: 484.370284\t\t Val Loss: 429.968964\n",
      "Epoch 87/300 | Train Loss: 691.088271\t\t Val Loss: 423.970032\n",
      "Epoch 88/300 | Train Loss: 549.546306\t\t Val Loss: 400.650085\n",
      "Epoch 89/300 | Train Loss: 479.887281\t\t Val Loss: 398.650330\n",
      "Epoch 90/300 | Train Loss: 490.236395\t\t Val Loss: 367.577057\n",
      "Epoch 91/300 | Train Loss: 574.640401\t\t Val Loss: 367.037476\n",
      "Epoch 92/300 | Train Loss: 434.184036\t\t Val Loss: 372.849579\n",
      "Epoch 93/300 | Train Loss: 461.025512\t\t Val Loss: 381.358185\n",
      "Epoch 94/300 | Train Loss: 477.505504\t\t Val Loss: 409.200684\n",
      "Epoch 95/300 | Train Loss: 439.026201\t\t Val Loss: 385.174286\n",
      "Epoch 96/300 | Train Loss: 515.869857\t\t Val Loss: 395.170135\n",
      "Epoch 97/300 | Train Loss: 458.250890\t\t Val Loss: 360.994690\n",
      "Epoch 98/300 | Train Loss: 404.602510\t\t Val Loss: 360.754944\n",
      "Epoch 99/300 | Train Loss: 460.538728\t\t Val Loss: 365.734192\n",
      "Epoch 100/300 | Train Loss: 447.671106\t\t Val Loss: 390.346130\n",
      "Epoch 101/300 | Train Loss: 711.626851\t\t Val Loss: 377.365814\n",
      "Epoch 102/300 | Train Loss: 441.304613\t\t Val Loss: 388.210693\n",
      "Epoch 103/300 | Train Loss: 413.149524\t\t Val Loss: 364.271973\n",
      "Epoch 104/300 | Train Loss: 368.784636\t\t Val Loss: 372.912689\n",
      "Epoch 105/300 | Train Loss: 431.310948\t\t Val Loss: 355.353210\n",
      "Epoch 106/300 | Train Loss: 349.796187\t\t Val Loss: 377.149414\n",
      "Epoch 107/300 | Train Loss: 369.118937\t\t Val Loss: 365.816162\n",
      "Epoch 108/300 | Train Loss: 318.158205\t\t Val Loss: 345.325867\n",
      "Epoch 109/300 | Train Loss: 379.820745\t\t Val Loss: 350.275635\n",
      "Epoch 110/300 | Train Loss: 355.697144\t\t Val Loss: 351.904449\n",
      "Epoch 111/300 | Train Loss: 345.721908\t\t Val Loss: 332.529022\n",
      "Epoch 112/300 | Train Loss: 316.944769\t\t Val Loss: 353.820496\n",
      "Epoch 113/300 | Train Loss: 397.023157\t\t Val Loss: 341.038422\n",
      "Epoch 114/300 | Train Loss: 343.868239\t\t Val Loss: 337.012329\n",
      "Epoch 115/300 | Train Loss: 335.689738\t\t Val Loss: 353.939880\n",
      "Epoch 116/300 | Train Loss: 323.418874\t\t Val Loss: 365.720581\n",
      "Epoch 117/300 | Train Loss: 391.252504\t\t Val Loss: 384.856964\n",
      "Epoch 118/300 | Train Loss: 366.969047\t\t Val Loss: 328.124054\n",
      "Epoch 119/300 | Train Loss: 468.979653\t\t Val Loss: 361.576477\n",
      "Epoch 120/300 | Train Loss: 310.881539\t\t Val Loss: 333.078278\n",
      "Epoch 121/300 | Train Loss: 310.632194\t\t Val Loss: 343.102234\n",
      "Epoch 122/300 | Train Loss: 349.617894\t\t Val Loss: 320.485413\n",
      "Epoch 123/300 | Train Loss: 257.623176\t\t Val Loss: 321.276123\n",
      "Epoch 124/300 | Train Loss: 287.881346\t\t Val Loss: 364.348480\n",
      "Epoch 125/300 | Train Loss: 297.860527\t\t Val Loss: 350.357208\n",
      "Epoch 126/300 | Train Loss: 290.214799\t\t Val Loss: 336.860840\n",
      "Epoch 127/300 | Train Loss: 291.363020\t\t Val Loss: 341.485138\n",
      "Epoch 128/300 | Train Loss: 301.258108\t\t Val Loss: 320.915131\n",
      "Epoch 129/300 | Train Loss: 306.005060\t\t Val Loss: 327.377380\n",
      "Epoch 130/300 | Train Loss: 360.874882\t\t Val Loss: 340.092590\n",
      "Epoch 131/300 | Train Loss: 298.292459\t\t Val Loss: 310.385132\n",
      "Epoch 132/300 | Train Loss: 269.527788\t\t Val Loss: 318.385712\n",
      "Epoch 133/300 | Train Loss: 247.390998\t\t Val Loss: 314.651520\n",
      "Epoch 134/300 | Train Loss: 255.252955\t\t Val Loss: 309.015167\n",
      "Epoch 135/300 | Train Loss: 254.612779\t\t Val Loss: 303.566803\n",
      "Epoch 136/300 | Train Loss: 226.429746\t\t Val Loss: 306.367218\n",
      "Epoch 137/300 | Train Loss: 265.320686\t\t Val Loss: 297.440674\n",
      "Epoch 138/300 | Train Loss: 257.598375\t\t Val Loss: 324.322632\n",
      "Epoch 139/300 | Train Loss: 264.174134\t\t Val Loss: 322.243591\n",
      "Epoch 140/300 | Train Loss: 283.501312\t\t Val Loss: 342.237640\n",
      "Epoch 141/300 | Train Loss: 240.752539\t\t Val Loss: 295.125824\n",
      "Epoch 142/300 | Train Loss: 257.792343\t\t Val Loss: 309.505768\n",
      "Epoch 143/300 | Train Loss: 339.476106\t\t Val Loss: 309.827667\n",
      "Epoch 144/300 | Train Loss: 259.178122\t\t Val Loss: 306.849487\n",
      "Epoch 145/300 | Train Loss: 252.262365\t\t Val Loss: 304.065399\n",
      "Epoch 146/300 | Train Loss: 345.859832\t\t Val Loss: 289.177948\n",
      "Epoch 147/300 | Train Loss: 518.490074\t\t Val Loss: 268.012268\n",
      "Epoch 148/300 | Train Loss: 240.339559\t\t Val Loss: 285.587585\n",
      "Epoch 149/300 | Train Loss: 222.947249\t\t Val Loss: 289.789734\n",
      "Epoch 150/300 | Train Loss: 233.141395\t\t Val Loss: 295.210297\n",
      "Epoch 151/300 | Train Loss: 247.726017\t\t Val Loss: 287.030884\n",
      "Epoch 152/300 | Train Loss: 264.110754\t\t Val Loss: 302.196350\n",
      "Epoch 153/300 | Train Loss: 228.466723\t\t Val Loss: 282.737366\n",
      "Epoch 154/300 | Train Loss: 218.568595\t\t Val Loss: 291.314911\n",
      "Epoch 155/300 | Train Loss: 215.624113\t\t Val Loss: 271.822968\n",
      "Epoch 156/300 | Train Loss: 218.298666\t\t Val Loss: 288.758667\n",
      "Epoch 157/300 | Train Loss: 208.791628\t\t Val Loss: 280.012756\n",
      "Epoch 158/300 | Train Loss: 191.228389\t\t Val Loss: 289.280090\n",
      "Epoch 159/300 | Train Loss: 247.867248\t\t Val Loss: 289.265350\n",
      "Epoch 160/300 | Train Loss: 321.016794\t\t Val Loss: 292.094330\n",
      "Epoch 161/300 | Train Loss: 221.055793\t\t Val Loss: 272.996185\n",
      "Epoch 162/300 | Train Loss: 202.197196\t\t Val Loss: 271.905518\n",
      "Epoch 163/300 | Train Loss: 198.567923\t\t Val Loss: 270.833069\n",
      "Epoch 164/300 | Train Loss: 207.625854\t\t Val Loss: 272.929962\n",
      "Epoch 165/300 | Train Loss: 187.431469\t\t Val Loss: 264.086090\n",
      "Epoch 166/300 | Train Loss: 184.691311\t\t Val Loss: 266.591095\n",
      "Epoch 167/300 | Train Loss: 179.595575\t\t Val Loss: 261.466187\n",
      "Epoch 168/300 | Train Loss: 202.225243\t\t Val Loss: 297.513672\n",
      "Epoch 169/300 | Train Loss: 209.700004\t\t Val Loss: 259.884216\n",
      "Epoch 170/300 | Train Loss: 213.482366\t\t Val Loss: 261.649963\n",
      "Epoch 171/300 | Train Loss: 171.921260\t\t Val Loss: 256.230988\n",
      "Epoch 172/300 | Train Loss: 200.539097\t\t Val Loss: 252.434952\n",
      "Epoch 173/300 | Train Loss: 196.456148\t\t Val Loss: 244.545486\n",
      "Epoch 174/300 | Train Loss: 190.740118\t\t Val Loss: 246.766937\n",
      "Epoch 175/300 | Train Loss: 171.408038\t\t Val Loss: 256.925568\n",
      "Epoch 176/300 | Train Loss: 176.497060\t\t Val Loss: 261.682098\n",
      "Epoch 177/300 | Train Loss: 182.111636\t\t Val Loss: 239.987823\n",
      "Epoch 178/300 | Train Loss: 163.907948\t\t Val Loss: 257.135223\n",
      "Epoch 179/300 | Train Loss: 175.336851\t\t Val Loss: 247.999588\n",
      "Epoch 180/300 | Train Loss: 181.772384\t\t Val Loss: 264.596436\n",
      "Epoch 181/300 | Train Loss: 207.409241\t\t Val Loss: 257.580566\n",
      "Epoch 182/300 | Train Loss: 209.600418\t\t Val Loss: 256.319336\n",
      "Epoch 183/300 | Train Loss: 171.894066\t\t Val Loss: 252.184021\n",
      "Epoch 184/300 | Train Loss: 178.899834\t\t Val Loss: 246.847321\n",
      "Epoch 185/300 | Train Loss: 263.187905\t\t Val Loss: 224.695038\n",
      "Epoch 186/300 | Train Loss: 171.179419\t\t Val Loss: 231.814865\n",
      "Epoch 187/300 | Train Loss: 168.889314\t\t Val Loss: 226.245621\n",
      "Epoch 188/300 | Train Loss: 170.705068\t\t Val Loss: 234.765335\n",
      "Epoch 189/300 | Train Loss: 189.979503\t\t Val Loss: 268.170410\n",
      "Epoch 190/300 | Train Loss: 168.212504\t\t Val Loss: 218.335419\n",
      "Epoch 191/300 | Train Loss: 185.544240\t\t Val Loss: 244.569305\n",
      "Epoch 192/300 | Train Loss: 165.796239\t\t Val Loss: 245.488312\n",
      "Epoch 193/300 | Train Loss: 161.281271\t\t Val Loss: 222.471268\n",
      "Epoch 194/300 | Train Loss: 157.236782\t\t Val Loss: 227.093384\n",
      "Epoch 195/300 | Train Loss: 186.340535\t\t Val Loss: 248.559296\n",
      "Epoch 196/300 | Train Loss: 159.493773\t\t Val Loss: 230.879089\n",
      "Epoch 197/300 | Train Loss: 193.509117\t\t Val Loss: 242.808517\n",
      "Epoch 198/300 | Train Loss: 164.497184\t\t Val Loss: 232.132217\n",
      "Epoch 199/300 | Train Loss: 151.050584\t\t Val Loss: 230.872086\n",
      "Epoch 200/300 | Train Loss: 158.340607\t\t Val Loss: 218.382828\n",
      "Epoch 201/300 | Train Loss: 165.294547\t\t Val Loss: 221.431915\n",
      "Epoch 202/300 | Train Loss: 150.736168\t\t Val Loss: 227.825256\n",
      "Epoch 203/300 | Train Loss: 146.403516\t\t Val Loss: 217.446472\n",
      "Epoch 204/300 | Train Loss: 150.462721\t\t Val Loss: 216.828812\n",
      "Epoch 205/300 | Train Loss: 184.242151\t\t Val Loss: 219.921310\n",
      "Epoch 206/300 | Train Loss: 159.172456\t\t Val Loss: 256.193054\n",
      "Epoch 207/300 | Train Loss: 157.299847\t\t Val Loss: 232.535278\n",
      "Epoch 208/300 | Train Loss: 150.367213\t\t Val Loss: 219.302536\n",
      "Epoch 209/300 | Train Loss: 142.789051\t\t Val Loss: 219.104294\n",
      "Epoch 210/300 | Train Loss: 155.060200\t\t Val Loss: 216.231064\n",
      "Epoch 211/300 | Train Loss: 133.108908\t\t Val Loss: 215.545380\n",
      "Epoch 212/300 | Train Loss: 159.269284\t\t Val Loss: 211.356339\n",
      "Epoch 213/300 | Train Loss: 143.059577\t\t Val Loss: 229.183075\n",
      "Epoch 214/300 | Train Loss: 155.271002\t\t Val Loss: 214.836960\n",
      "Epoch 215/300 | Train Loss: 157.644610\t\t Val Loss: 215.382111\n",
      "Epoch 216/300 | Train Loss: 144.204566\t\t Val Loss: 217.377884\n",
      "Epoch 217/300 | Train Loss: 133.531430\t\t Val Loss: 214.100998\n",
      "Epoch 218/300 | Train Loss: 160.788515\t\t Val Loss: 213.804916\n",
      "Epoch 219/300 | Train Loss: 138.561628\t\t Val Loss: 207.115982\n",
      "Epoch 220/300 | Train Loss: 142.208241\t\t Val Loss: 208.995255\n",
      "Epoch 221/300 | Train Loss: 161.081396\t\t Val Loss: 215.816040\n",
      "Epoch 222/300 | Train Loss: 137.074501\t\t Val Loss: 212.130203\n",
      "Epoch 223/300 | Train Loss: 123.617209\t\t Val Loss: 225.225388\n",
      "Epoch 224/300 | Train Loss: 136.179753\t\t Val Loss: 204.097687\n",
      "Epoch 225/300 | Train Loss: 124.317803\t\t Val Loss: 209.467667\n",
      "Epoch 226/300 | Train Loss: 133.164081\t\t Val Loss: 214.734009\n",
      "Epoch 227/300 | Train Loss: 132.696622\t\t Val Loss: 208.320221\n",
      "Epoch 228/300 | Train Loss: 123.613678\t\t Val Loss: 202.597931\n",
      "Epoch 229/300 | Train Loss: 114.553546\t\t Val Loss: 207.411255\n",
      "Epoch 230/300 | Train Loss: 120.836889\t\t Val Loss: 209.227722\n",
      "Epoch 231/300 | Train Loss: 134.197964\t\t Val Loss: 220.211929\n",
      "Epoch 232/300 | Train Loss: 133.583817\t\t Val Loss: 189.439453\n",
      "Epoch 233/300 | Train Loss: 143.833260\t\t Val Loss: 198.213867\n",
      "Epoch 234/300 | Train Loss: 128.747995\t\t Val Loss: 219.117096\n",
      "Epoch 235/300 | Train Loss: 123.903275\t\t Val Loss: 195.563110\n",
      "Epoch 236/300 | Train Loss: 136.534699\t\t Val Loss: 201.217529\n",
      "Epoch 237/300 | Train Loss: 123.586346\t\t Val Loss: 204.193100\n",
      "Epoch 238/300 | Train Loss: 127.494871\t\t Val Loss: 215.573059\n",
      "Epoch 239/300 | Train Loss: 134.810134\t\t Val Loss: 194.503448\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr = learning_rate)\n",
    "log_data = []\n",
    "\n",
    "if loss_name == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "elif loss_name == \"mae\":\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "elif loss_name == \"SMAPE\":\n",
    "    loss_fn = SMAPE\n",
    "elif loss_name == \"mape\":\n",
    "    loss_fn = MAPE\n",
    "elif loss_name == \"MASE\":\n",
    "    loss_fn = MASE(target_y, target_y.shape[1])\n",
    "else:\n",
    "    raise Exception(\"Your loss name is not valid.\")\n",
    "\n",
    "## early stopping\n",
    "PATIENCE = 20\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "for epoc in range(num_train_epochs):\n",
    "    model_instance.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model_instance(X)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()*X.shape[0]\n",
    "\n",
    "    avg_train_loss = total_train_loss/len(train_dataloader.dataset)\n",
    "\n",
    "    model_instance.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yys = []\n",
    "        yyhats = []\n",
    "\n",
    "        for XX, yy in val_dataloader:\n",
    "            XX = XX.to(device)\n",
    "            yys.append(yy.to(device))\n",
    "            yyhats.append(model_instance(XX))\n",
    "\n",
    "        yyhat = torch.concat(yyhats)\n",
    "        yy = torch.concat(yys)\n",
    "\n",
    "        val_loss = loss_fn(yyhat, yy).item()\n",
    "\n",
    "    print(f\"Epoch {epoc+1}/{num_train_epochs} | Train Loss: {avg_train_loss:.6f}\\t\\t Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    log_data.append({\"epoch\": epoc, \"loss\": avg_train_loss, \"eval_loss\": val_loss})\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state_dict = model_instance.state_dict()   ## 저장 없이 결과물만 산출...\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c25f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(log_data).to_csv(os.path.join(log_dir, f\"transfer_{loss_name}_lr{learning_rate}_run{1}.csv\"))\n",
    "\n",
    "model_instance.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yys = []\n",
    "    yyhats = []\n",
    "\n",
    "    for XX, yy in test_dataloader:\n",
    "        XX = XX.to(device)\n",
    "        yys.append(yy.to(device))\n",
    "        yyhats.append(model_instance(XX))\n",
    "\n",
    "    yyhat = torch.concat(yyhats)\n",
    "    yy = torch.concat(yys)\n",
    "\n",
    "    test_loss = loss_fn(yyhat, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE: 33.30413055419922\n",
      "test MAE: 29.78490447998047\n",
      "test SMAPE: 50.43315887451172\n"
     ]
    }
   ],
   "source": [
    "mseLoss = torch.nn.MSELoss()\n",
    "maeLoss = torch.nn.L1Loss()\n",
    "\n",
    "def smape(yy, yyhat):\n",
    "    numerator = 100*abs(yy - yyhat)\n",
    "    denominator = (abs(yy) + abs(yyhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "print(f\"test RMSE: {torch.sqrt(mseLoss(yyhat, yy))}\")\n",
    "print(f\"test MAE: {maeLoss(yyhat, yy)}\")\n",
    "print(f\"test SMAPE: {smape(yy, yyhat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e98fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
