{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c44d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 14:17:21.982089: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    PatchTSTConfig, PatchTSTForPrediction,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "88c5a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "output_dir = \"./pretrained/MAE\"\n",
    "logging_dir = \"./logs/MAE\"\n",
    "loss = \"mae\"\n",
    "learning_rate = 5e-6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e10038dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "target_y = pd.read_csv(f\"../data/{data}/train_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "target_X_val = target_X[-round(target_X.shape[0] * 0.2):, :].astype(np.float32)\n",
    "target_y_val = target_y[-round(target_y.shape[0] * 0.2):].astype(np.float32)\n",
    "target_X = target_X[:-round(target_X.shape[0] * 0.2), :].astype(np.float32)\n",
    "target_y = target_y[:-round(target_y.shape[0] * 0.2)].astype(np.float32)\n",
    "\n",
    "test_X  = pd.read_csv(f\"../data/{data}/val_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "test_y  = pd.read_csv(f\"../data/{data}/val_output_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "np.random.seed(2)\n",
    "random_indices1 = np.random.choice(pd.read_csv(\"../data/M4_train.csv\").iloc[:, (1):].index,\n",
    "                                   size=target_X.shape[0] * 20, replace=True)\n",
    "\n",
    "X_data = pd.read_csv(\"../data/M4_train.csv\").iloc[:, 1 + (24 * 0):].loc[random_indices1].values.astype(np.float32)\n",
    "y_data = pd.read_csv(\"../data/M4_test.csv\").iloc[:, 1:].loc[random_indices1].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b412e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_dataset(x, y):\n",
    "    x_list = [s[..., np.newaxis] for s in x]    ## (N, 168) -> (N, 168, 1)\n",
    "    y_list = [s[..., np.newaxis] for s in y]    ## (N, 24) -> (N, 24, 1)\n",
    "\n",
    "    data_dict = {\n",
    "        \"past_values\": x_list,\n",
    "        \"future_values\": y_list\n",
    "    }\n",
    "\n",
    "    return Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae446d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "select = np.random.choice(len(X_data), size=len(X_data), replace=True)\n",
    "X_bootstrap = X_data[select]\n",
    "y_bootstrap = y_data[select]\n",
    "\n",
    "val_split_index = int(len(X_bootstrap) * 0.8)\n",
    "X_train, X_valid = X_bootstrap[:val_split_index], X_bootstrap[val_split_index:]\n",
    "y_train, y_valid = y_bootstrap[:val_split_index], y_bootstrap[val_split_index:]\n",
    "\n",
    "train_dataset = create_hf_dataset(X_train, y_train)\n",
    "test_dataset = create_hf_dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5784648",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"./pretrained/checkpoint-12851\" # 베스트 모델 경로\n",
    "best_model = PatchTSTForPrediction.from_pretrained(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf69379",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_body = best_model.model\n",
    "config = best_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52519830",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PATCHES = config.patch_length\n",
    "D_MODEL = config.d_model\n",
    "T_OUT = target_y.shape[1]\n",
    "C_NEW = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "99275f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_hf_dataset(target_X, target_y)\n",
    "val_dataset = create_hf_dataset(target_X_val, target_y_val)\n",
    "test_dataset = create_hf_dataset(test_X, test_y)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['past_values', 'future_values'])\n",
    "val_dataset.set_format(type='torch', columns=['past_values', 'future_values'])\n",
    "test_dataset.set_format(type='torch', columns=['past_values', 'future_values'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 8)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c900586",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "be91d914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 168, 1])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"past_values\"].shape  ## (B, t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5c4444f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 10, 128])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.model(past_values = batch[\"past_values\"].to(\"cuda:0\")).last_hidden_state.shape    ## (B, 1, 10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8ce8a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferModel(torch.nn.Module):\n",
    "    def __init__(self, body, t_out):\n",
    "        super().__init__()\n",
    "        self.body = body\n",
    "        self.t_out = t_out\n",
    "        body_out_features = 1280 \n",
    "        self.c_new = 128 \n",
    "        \n",
    "        self.flatten = torch.nn.Flatten(start_dim=1, end_dim=-1) \n",
    "        self.adapter = torch.nn.Linear(body_out_features, self.t_out * self.c_new)  ## Dense(128)\n",
    "\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.body(past_values=x).last_hidden_state\n",
    "        flat_feat = self.flatten(features)\n",
    "        adapted_feat = self.adapter(flat_feat)\n",
    "        head_input = adapted_feat.view(-1, self.t_out, self.c_new)  ## (B, 24, 128)\n",
    "        output = self.head(head_input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = TransferModel(pretrained_body, 24).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3886e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 24, 1])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_instance(batch[\"past_values\"].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "19fe7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_instance.parameters(), lr=1e-6)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2b9129d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "aeab2cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['past_values', 'future_values'])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cfa48b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000 - Train Loss: 26625.612097 - Val Loss: 4458.620117\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4458.620117)\n",
      "Epoch 2/2000 - Train Loss: 26624.784263 - Val Loss: 4458.211751\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4458.211751)\n",
      "Epoch 3/2000 - Train Loss: 26623.466446 - Val Loss: 4457.773926\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4457.773926)\n",
      "Epoch 4/2000 - Train Loss: 26622.312547 - Val Loss: 4457.316732\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4457.316732)\n",
      "Epoch 5/2000 - Train Loss: 26620.800912 - Val Loss: 4456.836507\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4456.836507)\n",
      "Epoch 6/2000 - Train Loss: 26619.376666 - Val Loss: 4456.332113\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4456.332113)\n",
      "Epoch 7/2000 - Train Loss: 26618.360663 - Val Loss: 4455.821045\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4455.821045)\n",
      "Epoch 8/2000 - Train Loss: 26617.004679 - Val Loss: 4455.268555\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4455.268555)\n",
      "Epoch 9/2000 - Train Loss: 26614.999913 - Val Loss: 4454.708659\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4454.708659)\n",
      "Epoch 10/2000 - Train Loss: 26613.888562 - Val Loss: 4454.116781\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4454.116781)\n",
      "Epoch 11/2000 - Train Loss: 26612.033210 - Val Loss: 4453.509684\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4453.509684)\n",
      "Epoch 12/2000 - Train Loss: 26611.069918 - Val Loss: 4452.881755\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4452.881755)\n",
      "Epoch 13/2000 - Train Loss: 26608.641622 - Val Loss: 4452.216390\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4452.216390)\n",
      "Epoch 14/2000 - Train Loss: 26607.130110 - Val Loss: 4451.512126\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4451.512126)\n",
      "Epoch 15/2000 - Train Loss: 26605.281628 - Val Loss: 4450.798014\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4450.798014)\n",
      "Epoch 16/2000 - Train Loss: 26603.453242 - Val Loss: 4450.064941\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4450.064941)\n",
      "Epoch 17/2000 - Train Loss: 26601.232402 - Val Loss: 4449.290120\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4449.290120)\n",
      "Epoch 18/2000 - Train Loss: 26598.995706 - Val Loss: 4448.490560\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4448.490560)\n",
      "Epoch 19/2000 - Train Loss: 26596.940333 - Val Loss: 4447.638753\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4447.638753)\n",
      "Epoch 20/2000 - Train Loss: 26594.720138 - Val Loss: 4446.767008\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4446.767008)\n",
      "Epoch 21/2000 - Train Loss: 26592.377077 - Val Loss: 4445.830729\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4445.830729)\n",
      "Epoch 22/2000 - Train Loss: 26590.149518 - Val Loss: 4444.877686\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4444.877686)\n",
      "Epoch 23/2000 - Train Loss: 26587.174049 - Val Loss: 4443.861084\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4443.861084)\n",
      "Epoch 24/2000 - Train Loss: 26583.765866 - Val Loss: 4442.792155\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4442.792155)\n",
      "Epoch 25/2000 - Train Loss: 26581.594690 - Val Loss: 4441.748210\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4441.748210)\n",
      "Epoch 26/2000 - Train Loss: 26578.764682 - Val Loss: 4440.580160\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4440.580160)\n",
      "Epoch 27/2000 - Train Loss: 26575.413772 - Val Loss: 4439.436361\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4439.436361)\n",
      "Epoch 28/2000 - Train Loss: 26572.669323 - Val Loss: 4438.239827\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4438.239827)\n",
      "Epoch 29/2000 - Train Loss: 26569.832630 - Val Loss: 4437.039795\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4437.039795)\n",
      "Epoch 30/2000 - Train Loss: 26565.749522 - Val Loss: 4435.719889\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4435.719889)\n",
      "Epoch 31/2000 - Train Loss: 26562.656227 - Val Loss: 4434.448242\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4434.448242)\n",
      "Epoch 32/2000 - Train Loss: 26559.076436 - Val Loss: 4433.168701\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4433.168701)\n",
      "Epoch 33/2000 - Train Loss: 26555.569266 - Val Loss: 4431.740479\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4431.740479)\n",
      "Epoch 34/2000 - Train Loss: 26552.513826 - Val Loss: 4430.410726\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4430.410726)\n",
      "Epoch 35/2000 - Train Loss: 26548.192657 - Val Loss: 4428.974609\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4428.974609)\n",
      "Epoch 36/2000 - Train Loss: 26545.358007 - Val Loss: 4427.555339\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4427.555339)\n",
      "Epoch 37/2000 - Train Loss: 26540.141632 - Val Loss: 4426.121663\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4426.121663)\n",
      "Epoch 38/2000 - Train Loss: 26536.363462 - Val Loss: 4424.625163\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4424.625163)\n",
      "Epoch 39/2000 - Train Loss: 26533.103987 - Val Loss: 4423.098796\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4423.098796)\n",
      "Epoch 40/2000 - Train Loss: 26528.547614 - Val Loss: 4421.563314\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4421.563314)\n",
      "Epoch 41/2000 - Train Loss: 26524.775966 - Val Loss: 4420.008626\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4420.008626)\n",
      "Epoch 42/2000 - Train Loss: 26519.833904 - Val Loss: 4418.457275\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4418.457275)\n",
      "Epoch 43/2000 - Train Loss: 26515.529859 - Val Loss: 4416.786458\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4416.786458)\n",
      "Epoch 44/2000 - Train Loss: 26511.977823 - Val Loss: 4415.242025\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4415.242025)\n",
      "Epoch 45/2000 - Train Loss: 26506.797661 - Val Loss: 4413.543783\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4413.543783)\n",
      "Epoch 46/2000 - Train Loss: 26503.132244 - Val Loss: 4411.841715\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4411.841715)\n",
      "Epoch 47/2000 - Train Loss: 26497.313677 - Val Loss: 4410.064860\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4410.064860)\n",
      "Epoch 48/2000 - Train Loss: 26493.495853 - Val Loss: 4408.426270\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4408.426270)\n",
      "Epoch 49/2000 - Train Loss: 26487.969217 - Val Loss: 4406.565755\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4406.565755)\n",
      "Epoch 50/2000 - Train Loss: 26484.134662 - Val Loss: 4404.818604\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4404.818604)\n",
      "Epoch 51/2000 - Train Loss: 26478.659471 - Val Loss: 4402.980550\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4402.980550)\n",
      "Epoch 52/2000 - Train Loss: 26472.763341 - Val Loss: 4401.131673\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4401.131673)\n",
      "Epoch 53/2000 - Train Loss: 26469.245963 - Val Loss: 4399.277913\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4399.277913)\n",
      "Epoch 54/2000 - Train Loss: 26464.019100 - Val Loss: 4397.448812\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4397.448812)\n",
      "Epoch 55/2000 - Train Loss: 26458.957175 - Val Loss: 4395.473145\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4395.473145)\n",
      "Epoch 56/2000 - Train Loss: 26453.309835 - Val Loss: 4393.546305\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4393.546305)\n",
      "Epoch 57/2000 - Train Loss: 26448.600939 - Val Loss: 4391.583577\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4391.583577)\n",
      "Epoch 58/2000 - Train Loss: 26442.070784 - Val Loss: 4389.571208\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4389.571208)\n",
      "Epoch 59/2000 - Train Loss: 26436.999682 - Val Loss: 4387.531331\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4387.531331)\n",
      "Epoch 60/2000 - Train Loss: 26432.594984 - Val Loss: 4385.432129\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4385.432129)\n",
      "Epoch 61/2000 - Train Loss: 26427.061246 - Val Loss: 4383.371175\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4383.371175)\n",
      "Epoch 62/2000 - Train Loss: 26421.149735 - Val Loss: 4381.222982\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4381.222982)\n",
      "Epoch 63/2000 - Train Loss: 26416.591882 - Val Loss: 4379.074463\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4379.074463)\n",
      "Epoch 64/2000 - Train Loss: 26410.185910 - Val Loss: 4376.931803\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4376.931803)\n",
      "Epoch 65/2000 - Train Loss: 26403.503072 - Val Loss: 4374.755290\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4374.755290)\n",
      "Epoch 66/2000 - Train Loss: 26399.271688 - Val Loss: 4372.538493\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4372.538493)\n",
      "Epoch 67/2000 - Train Loss: 26392.849491 - Val Loss: 4370.242106\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4370.242106)\n",
      "Epoch 68/2000 - Train Loss: 26386.771086 - Val Loss: 4367.983724\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4367.983724)\n",
      "Epoch 69/2000 - Train Loss: 26380.132050 - Val Loss: 4365.589193\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4365.589193)\n",
      "Epoch 70/2000 - Train Loss: 26375.950045 - Val Loss: 4363.164795\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4363.164795)\n",
      "Epoch 71/2000 - Train Loss: 26368.208148 - Val Loss: 4360.839518\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4360.839518)\n",
      "Epoch 72/2000 - Train Loss: 26362.328811 - Val Loss: 4358.487142\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4358.487142)\n",
      "Epoch 73/2000 - Train Loss: 26356.357347 - Val Loss: 4356.079834\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4356.079834)\n",
      "Epoch 74/2000 - Train Loss: 26349.238064 - Val Loss: 4353.645752\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4353.645752)\n",
      "Epoch 75/2000 - Train Loss: 26343.742293 - Val Loss: 4351.058675\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4351.058675)\n",
      "Epoch 76/2000 - Train Loss: 26336.577789 - Val Loss: 4348.622233\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4348.622233)\n",
      "Epoch 77/2000 - Train Loss: 26330.647677 - Val Loss: 4346.026530\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4346.026530)\n",
      "Epoch 78/2000 - Train Loss: 26324.395133 - Val Loss: 4343.397461\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4343.397461)\n",
      "Epoch 79/2000 - Train Loss: 26314.378858 - Val Loss: 4340.888346\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4340.888346)\n",
      "Epoch 80/2000 - Train Loss: 26309.458625 - Val Loss: 4338.126221\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4338.126221)\n",
      "Epoch 81/2000 - Train Loss: 26304.212635 - Val Loss: 4335.561198\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4335.561198)\n",
      "Epoch 82/2000 - Train Loss: 26296.122687 - Val Loss: 4332.735514\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4332.735514)\n",
      "Epoch 83/2000 - Train Loss: 26288.723578 - Val Loss: 4330.011556\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4330.011556)\n",
      "Epoch 84/2000 - Train Loss: 26282.162136 - Val Loss: 4327.276042\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4327.276042)\n",
      "Epoch 85/2000 - Train Loss: 26274.831821 - Val Loss: 4324.549316\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4324.549316)\n",
      "Epoch 86/2000 - Train Loss: 26268.331747 - Val Loss: 4321.734538\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4321.734538)\n",
      "Epoch 87/2000 - Train Loss: 26259.561977 - Val Loss: 4318.812581\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4318.812581)\n",
      "Epoch 88/2000 - Train Loss: 26250.151924 - Val Loss: 4316.073161\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4316.073161)\n",
      "Epoch 89/2000 - Train Loss: 26246.570670 - Val Loss: 4313.147705\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4313.147705)\n",
      "Epoch 90/2000 - Train Loss: 26236.704501 - Val Loss: 4310.293376\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4310.293376)\n",
      "Epoch 91/2000 - Train Loss: 26229.058229 - Val Loss: 4307.262533\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4307.262533)\n",
      "Epoch 92/2000 - Train Loss: 26224.012849 - Val Loss: 4304.225830\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4304.225830)\n",
      "Epoch 93/2000 - Train Loss: 26214.691895 - Val Loss: 4301.235107\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4301.235107)\n",
      "Epoch 94/2000 - Train Loss: 26208.790360 - Val Loss: 4298.109212\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4298.109212)\n",
      "Epoch 95/2000 - Train Loss: 26202.624436 - Val Loss: 4294.989746\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4294.989746)\n",
      "Epoch 96/2000 - Train Loss: 26190.418589 - Val Loss: 4292.171305\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4292.171305)\n",
      "Epoch 97/2000 - Train Loss: 26182.449861 - Val Loss: 4288.824870\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4288.824870)\n",
      "Epoch 98/2000 - Train Loss: 26174.062985 - Val Loss: 4285.828044\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4285.828044)\n",
      "Epoch 99/2000 - Train Loss: 26167.435059 - Val Loss: 4282.538574\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4282.538574)\n",
      "Epoch 100/2000 - Train Loss: 26160.135971 - Val Loss: 4279.324788\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4279.324788)\n",
      "Epoch 101/2000 - Train Loss: 26151.939079 - Val Loss: 4276.250081\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4276.250081)\n",
      "Epoch 102/2000 - Train Loss: 26139.975909 - Val Loss: 4273.049072\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4273.049072)\n",
      "Epoch 103/2000 - Train Loss: 26133.664173 - Val Loss: 4269.541748\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4269.541748)\n",
      "Epoch 104/2000 - Train Loss: 26126.619480 - Val Loss: 4266.272786\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4266.272786)\n",
      "Epoch 105/2000 - Train Loss: 26115.141720 - Val Loss: 4263.120361\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4263.120361)\n",
      "Epoch 106/2000 - Train Loss: 26104.290006 - Val Loss: 4259.859049\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4259.859049)\n",
      "Epoch 107/2000 - Train Loss: 26099.907931 - Val Loss: 4256.372884\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4256.372884)\n",
      "Epoch 108/2000 - Train Loss: 26090.312547 - Val Loss: 4253.268148\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4253.268148)\n",
      "Epoch 109/2000 - Train Loss: 26078.684119 - Val Loss: 4249.724447\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4249.724447)\n",
      "Epoch 110/2000 - Train Loss: 26072.290278 - Val Loss: 4246.045492\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4246.045492)\n",
      "Epoch 111/2000 - Train Loss: 26063.933184 - Val Loss: 4242.730794\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4242.730794)\n",
      "Epoch 112/2000 - Train Loss: 26056.226589 - Val Loss: 4239.283529\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4239.283529)\n",
      "Epoch 113/2000 - Train Loss: 26047.285267 - Val Loss: 4235.900798\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4235.900798)\n",
      "Epoch 114/2000 - Train Loss: 26036.994030 - Val Loss: 4232.510986\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4232.510986)\n",
      "Epoch 115/2000 - Train Loss: 26027.962683 - Val Loss: 4228.717773\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4228.717773)\n",
      "Epoch 116/2000 - Train Loss: 26016.928139 - Val Loss: 4224.919515\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4224.919515)\n",
      "Epoch 117/2000 - Train Loss: 26008.744535 - Val Loss: 4221.565348\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4221.565348)\n",
      "Epoch 118/2000 - Train Loss: 25998.898593 - Val Loss: 4218.011068\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4218.011068)\n",
      "Epoch 119/2000 - Train Loss: 25987.133221 - Val Loss: 4214.093506\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4214.093506)\n",
      "Epoch 120/2000 - Train Loss: 25978.536598 - Val Loss: 4210.576986\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4210.576986)\n",
      "Epoch 121/2000 - Train Loss: 25971.360949 - Val Loss: 4206.789225\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4206.789225)\n",
      "Epoch 122/2000 - Train Loss: 25959.971723 - Val Loss: 4203.026286\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4203.026286)\n",
      "Epoch 123/2000 - Train Loss: 25949.943072 - Val Loss: 4199.428792\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4199.428792)\n",
      "Epoch 124/2000 - Train Loss: 25943.757090 - Val Loss: 4195.683594\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4195.683594)\n",
      "Epoch 125/2000 - Train Loss: 25935.530041 - Val Loss: 4192.107422\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4192.107422)\n",
      "Epoch 126/2000 - Train Loss: 25920.144513 - Val Loss: 4188.169027\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4188.169027)\n",
      "Epoch 127/2000 - Train Loss: 25908.251719 - Val Loss: 4184.313721\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4184.313721)\n",
      "Epoch 128/2000 - Train Loss: 25902.593780 - Val Loss: 4180.542643\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4180.542643)\n",
      "Epoch 129/2000 - Train Loss: 25890.902824 - Val Loss: 4176.749023\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4176.749023)\n",
      "Epoch 130/2000 - Train Loss: 25877.080543 - Val Loss: 4173.319661\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4173.319661)\n",
      "Epoch 131/2000 - Train Loss: 25876.759933 - Val Loss: 4168.916585\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4168.916585)\n",
      "Epoch 132/2000 - Train Loss: 25859.455264 - Val Loss: 4165.055257\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4165.055257)\n",
      "Epoch 133/2000 - Train Loss: 25844.367497 - Val Loss: 4161.099854\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4161.099854)\n",
      "Epoch 134/2000 - Train Loss: 25839.216125 - Val Loss: 4157.169922\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4157.169922)\n",
      "Epoch 135/2000 - Train Loss: 25833.882710 - Val Loss: 4153.437174\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4153.437174)\n",
      "Epoch 136/2000 - Train Loss: 25820.145856 - Val Loss: 4149.279948\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4149.279948)\n",
      "Epoch 137/2000 - Train Loss: 25803.729149 - Val Loss: 4145.000081\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4145.000081)\n",
      "Epoch 138/2000 - Train Loss: 25797.186492 - Val Loss: 4141.074137\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4141.074137)\n",
      "Epoch 139/2000 - Train Loss: 25790.526468 - Val Loss: 4136.852946\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4136.852946)\n",
      "Epoch 140/2000 - Train Loss: 25775.769020 - Val Loss: 4132.841797\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4132.841797)\n",
      "Epoch 141/2000 - Train Loss: 25767.516160 - Val Loss: 4128.550374\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4128.550374)\n",
      "Epoch 142/2000 - Train Loss: 25751.533778 - Val Loss: 4124.642660\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4124.642660)\n",
      "Epoch 143/2000 - Train Loss: 25737.332270 - Val Loss: 4120.216797\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4120.216797)\n",
      "Epoch 144/2000 - Train Loss: 25730.343053 - Val Loss: 4116.222738\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4116.222738)\n",
      "Epoch 145/2000 - Train Loss: 25717.537929 - Val Loss: 4111.573649\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4111.573649)\n",
      "Epoch 146/2000 - Train Loss: 25711.002826 - Val Loss: 4107.896322\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4107.896322)\n",
      "Epoch 147/2000 - Train Loss: 25697.632629 - Val Loss: 4103.545492\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4103.545492)\n",
      "Epoch 148/2000 - Train Loss: 25688.940284 - Val Loss: 4099.614258\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4099.614258)\n",
      "Epoch 149/2000 - Train Loss: 25674.059164 - Val Loss: 4095.065674\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4095.065674)\n",
      "Epoch 150/2000 - Train Loss: 25661.994162 - Val Loss: 4091.094482\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4091.094482)\n",
      "Epoch 151/2000 - Train Loss: 25651.915930 - Val Loss: 4086.700602\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4086.700602)\n",
      "Epoch 152/2000 - Train Loss: 25641.350509 - Val Loss: 4082.462809\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4082.462809)\n",
      "Epoch 153/2000 - Train Loss: 25634.227290 - Val Loss: 4077.986084\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4077.986084)\n",
      "Epoch 154/2000 - Train Loss: 25621.474546 - Val Loss: 4073.675293\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4073.675293)\n",
      "Epoch 155/2000 - Train Loss: 25604.648976 - Val Loss: 4069.607666\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4069.607666)\n",
      "Epoch 156/2000 - Train Loss: 25594.096718 - Val Loss: 4064.754150\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4064.754150)\n",
      "Epoch 157/2000 - Train Loss: 25581.640702 - Val Loss: 4060.460124\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4060.460124)\n",
      "Epoch 158/2000 - Train Loss: 25573.572553 - Val Loss: 4056.115560\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4056.115560)\n",
      "Epoch 159/2000 - Train Loss: 25554.975355 - Val Loss: 4051.722738\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4051.722738)\n",
      "Epoch 160/2000 - Train Loss: 25541.702133 - Val Loss: 4046.896973\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4046.896973)\n",
      "Epoch 161/2000 - Train Loss: 25533.064627 - Val Loss: 4042.649984\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4042.649984)\n",
      "Epoch 162/2000 - Train Loss: 25519.633687 - Val Loss: 4038.001872\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4038.001872)\n",
      "Epoch 163/2000 - Train Loss: 25504.384281 - Val Loss: 4033.245117\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4033.245117)\n",
      "Epoch 164/2000 - Train Loss: 25502.756644 - Val Loss: 4028.917318\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4028.917318)\n",
      "Epoch 165/2000 - Train Loss: 25484.830725 - Val Loss: 4024.560384\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4024.560384)\n",
      "Epoch 166/2000 - Train Loss: 25469.418186 - Val Loss: 4019.645182\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4019.645182)\n",
      "Epoch 167/2000 - Train Loss: 25459.457821 - Val Loss: 4015.094320\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4015.094320)\n",
      "Epoch 168/2000 - Train Loss: 25448.582792 - Val Loss: 4010.498128\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4010.498128)\n",
      "Epoch 169/2000 - Train Loss: 25436.809692 - Val Loss: 4005.866048\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4005.866048)\n",
      "Epoch 170/2000 - Train Loss: 25418.193809 - Val Loss: 4001.410400\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 4001.410400)\n",
      "Epoch 171/2000 - Train Loss: 25409.169658 - Val Loss: 3996.452962\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3996.452962)\n",
      "Epoch 172/2000 - Train Loss: 25394.911838 - Val Loss: 3992.002279\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3992.002279)\n",
      "Epoch 173/2000 - Train Loss: 25388.854947 - Val Loss: 3987.461100\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3987.461100)\n",
      "Epoch 174/2000 - Train Loss: 25368.324112 - Val Loss: 3982.425863\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3982.425863)\n",
      "Epoch 175/2000 - Train Loss: 25356.023626 - Val Loss: 3977.853109\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3977.853109)\n",
      "Epoch 176/2000 - Train Loss: 25346.096589 - Val Loss: 3973.305339\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3973.305339)\n",
      "Epoch 177/2000 - Train Loss: 25327.423260 - Val Loss: 3968.234131\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3968.234131)\n",
      "Epoch 178/2000 - Train Loss: 25315.950483 - Val Loss: 3963.426595\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3963.426595)\n",
      "Epoch 179/2000 - Train Loss: 25304.288430 - Val Loss: 3959.069499\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3959.069499)\n",
      "Epoch 180/2000 - Train Loss: 25287.046358 - Val Loss: 3954.086833\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3954.086833)\n",
      "Epoch 181/2000 - Train Loss: 25272.249968 - Val Loss: 3949.022135\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3949.022135)\n",
      "Epoch 182/2000 - Train Loss: 25264.810141 - Val Loss: 3944.259928\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3944.259928)\n",
      "Epoch 183/2000 - Train Loss: 25253.237957 - Val Loss: 3939.328206\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3939.328206)\n",
      "Epoch 184/2000 - Train Loss: 25231.928888 - Val Loss: 3934.119303\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3934.119303)\n",
      "Epoch 185/2000 - Train Loss: 25222.072075 - Val Loss: 3929.497965\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3929.497965)\n",
      "Epoch 186/2000 - Train Loss: 25201.666606 - Val Loss: 3924.388835\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3924.388835)\n",
      "Epoch 187/2000 - Train Loss: 25197.283198 - Val Loss: 3919.140788\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3919.140788)\n",
      "Epoch 188/2000 - Train Loss: 25175.048012 - Val Loss: 3914.198161\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3914.198161)\n",
      "Epoch 189/2000 - Train Loss: 25168.381378 - Val Loss: 3909.228597\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3909.228597)\n",
      "Epoch 190/2000 - Train Loss: 25153.987375 - Val Loss: 3904.359131\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3904.359131)\n",
      "Epoch 191/2000 - Train Loss: 25142.080374 - Val Loss: 3899.052734\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3899.052734)\n",
      "Epoch 192/2000 - Train Loss: 25128.382117 - Val Loss: 3894.367757\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3894.367757)\n",
      "Epoch 193/2000 - Train Loss: 25105.227564 - Val Loss: 3889.835938\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3889.835938)\n",
      "Epoch 194/2000 - Train Loss: 25100.282934 - Val Loss: 3884.286947\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3884.286947)\n",
      "Epoch 195/2000 - Train Loss: 25085.242264 - Val Loss: 3879.615967\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3879.615967)\n",
      "Epoch 196/2000 - Train Loss: 25063.303353 - Val Loss: 3874.190837\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3874.190837)\n",
      "Epoch 197/2000 - Train Loss: 25044.956219 - Val Loss: 3869.376058\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3869.376058)\n",
      "Epoch 198/2000 - Train Loss: 25036.405173 - Val Loss: 3863.424072\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3863.424072)\n",
      "Epoch 199/2000 - Train Loss: 25020.824144 - Val Loss: 3859.304199\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3859.304199)\n",
      "Epoch 200/2000 - Train Loss: 25013.719710 - Val Loss: 3853.793620\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3853.793620)\n",
      "Epoch 201/2000 - Train Loss: 24999.954147 - Val Loss: 3848.817464\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3848.817464)\n",
      "Epoch 202/2000 - Train Loss: 24986.197792 - Val Loss: 3843.826660\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3843.826660)\n",
      "Epoch 203/2000 - Train Loss: 24973.271591 - Val Loss: 3838.323161\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3838.323161)\n",
      "Epoch 204/2000 - Train Loss: 24951.098456 - Val Loss: 3833.341878\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3833.341878)\n",
      "Epoch 205/2000 - Train Loss: 24932.942077 - Val Loss: 3828.463379\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3828.463379)\n",
      "Epoch 206/2000 - Train Loss: 24927.745870 - Val Loss: 3823.233805\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3823.233805)\n",
      "Epoch 207/2000 - Train Loss: 24907.366114 - Val Loss: 3817.921631\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3817.921631)\n",
      "Epoch 208/2000 - Train Loss: 24896.993027 - Val Loss: 3811.714681\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3811.714681)\n",
      "Epoch 209/2000 - Train Loss: 24874.839230 - Val Loss: 3807.200765\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3807.200765)\n",
      "Epoch 210/2000 - Train Loss: 24859.586697 - Val Loss: 3801.438639\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3801.438639)\n",
      "Epoch 211/2000 - Train Loss: 24847.930034 - Val Loss: 3795.753011\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3795.753011)\n",
      "Epoch 212/2000 - Train Loss: 24839.133836 - Val Loss: 3790.770996\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3790.770996)\n",
      "Epoch 213/2000 - Train Loss: 24821.298504 - Val Loss: 3785.052734\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3785.052734)\n",
      "Epoch 214/2000 - Train Loss: 24806.240534 - Val Loss: 3779.334880\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3779.334880)\n",
      "Epoch 215/2000 - Train Loss: 24786.994080 - Val Loss: 3774.680257\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3774.680257)\n",
      "Epoch 216/2000 - Train Loss: 24776.777564 - Val Loss: 3768.664714\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3768.664714)\n",
      "Epoch 217/2000 - Train Loss: 24761.528354 - Val Loss: 3763.678630\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3763.678630)\n",
      "Epoch 218/2000 - Train Loss: 24743.538377 - Val Loss: 3758.404867\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3758.404867)\n",
      "Epoch 219/2000 - Train Loss: 24724.140904 - Val Loss: 3753.086751\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3753.086751)\n",
      "Epoch 220/2000 - Train Loss: 24713.606002 - Val Loss: 3747.133219\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3747.133219)\n",
      "Epoch 221/2000 - Train Loss: 24702.684355 - Val Loss: 3741.529704\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3741.529704)\n",
      "Epoch 222/2000 - Train Loss: 24681.120624 - Val Loss: 3736.169271\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3736.169271)\n",
      "Epoch 223/2000 - Train Loss: 24665.622447 - Val Loss: 3731.206299\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3731.206299)\n",
      "Epoch 224/2000 - Train Loss: 24640.187803 - Val Loss: 3725.295492\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3725.295492)\n",
      "Epoch 225/2000 - Train Loss: 24638.463434 - Val Loss: 3720.133952\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3720.133952)\n",
      "Epoch 226/2000 - Train Loss: 24616.291608 - Val Loss: 3714.233480\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3714.233480)\n",
      "Epoch 227/2000 - Train Loss: 24594.801010 - Val Loss: 3708.444336\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3708.444336)\n",
      "Epoch 228/2000 - Train Loss: 24593.472676 - Val Loss: 3703.971354\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3703.971354)\n",
      "Epoch 229/2000 - Train Loss: 24573.149478 - Val Loss: 3697.569092\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3697.569092)\n",
      "Epoch 230/2000 - Train Loss: 24553.996868 - Val Loss: 3691.557699\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3691.557699)\n",
      "Epoch 231/2000 - Train Loss: 24534.747600 - Val Loss: 3685.945719\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3685.945719)\n",
      "Epoch 232/2000 - Train Loss: 24507.417873 - Val Loss: 3680.301839\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3680.301839)\n",
      "Epoch 233/2000 - Train Loss: 24509.259394 - Val Loss: 3674.229899\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3674.229899)\n",
      "Epoch 234/2000 - Train Loss: 24497.686373 - Val Loss: 3669.803630\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3669.803630)\n",
      "Epoch 235/2000 - Train Loss: 24464.631597 - Val Loss: 3663.334473\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3663.334473)\n",
      "Epoch 236/2000 - Train Loss: 24451.043776 - Val Loss: 3657.764404\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3657.764404)\n",
      "Epoch 237/2000 - Train Loss: 24438.826842 - Val Loss: 3652.235596\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3652.235596)\n",
      "Epoch 238/2000 - Train Loss: 24411.624344 - Val Loss: 3646.670898\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3646.670898)\n",
      "Epoch 239/2000 - Train Loss: 24406.953448 - Val Loss: 3640.975342\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3640.975342)\n",
      "Epoch 240/2000 - Train Loss: 24385.080520 - Val Loss: 3635.032796\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3635.032796)\n",
      "Epoch 241/2000 - Train Loss: 24375.689711 - Val Loss: 3629.197347\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3629.197347)\n",
      "Epoch 242/2000 - Train Loss: 24363.905168 - Val Loss: 3624.394857\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3624.394857)\n",
      "Epoch 243/2000 - Train Loss: 24334.774705 - Val Loss: 3618.150553\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3618.150553)\n",
      "Epoch 244/2000 - Train Loss: 24318.170867 - Val Loss: 3612.539714\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3612.539714)\n",
      "Epoch 245/2000 - Train Loss: 24303.755517 - Val Loss: 3606.270833\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3606.270833)\n",
      "Epoch 246/2000 - Train Loss: 24282.314916 - Val Loss: 3600.457926\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3600.457926)\n",
      "Epoch 247/2000 - Train Loss: 24252.356577 - Val Loss: 3594.196045\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3594.196045)\n",
      "Epoch 248/2000 - Train Loss: 24250.602233 - Val Loss: 3588.565755\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3588.565755)\n",
      "Epoch 249/2000 - Train Loss: 24231.657583 - Val Loss: 3582.674967\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3582.674967)\n",
      "Epoch 250/2000 - Train Loss: 24222.527882 - Val Loss: 3576.406901\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3576.406901)\n",
      "Epoch 251/2000 - Train Loss: 24203.702730 - Val Loss: 3570.616618\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3570.616618)\n",
      "Epoch 252/2000 - Train Loss: 24182.770399 - Val Loss: 3564.121989\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3564.121989)\n",
      "Epoch 253/2000 - Train Loss: 24162.348340 - Val Loss: 3558.758138\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3558.758138)\n",
      "Epoch 254/2000 - Train Loss: 24152.175761 - Val Loss: 3552.182699\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3552.182699)\n",
      "Epoch 255/2000 - Train Loss: 24150.316692 - Val Loss: 3546.179199\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3546.179199)\n",
      "Epoch 256/2000 - Train Loss: 24120.198774 - Val Loss: 3540.166504\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3540.166504)\n",
      "Epoch 257/2000 - Train Loss: 24102.128992 - Val Loss: 3534.206706\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3534.206706)\n",
      "Epoch 258/2000 - Train Loss: 24075.984661 - Val Loss: 3528.421712\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3528.421712)\n",
      "Epoch 259/2000 - Train Loss: 24058.426270 - Val Loss: 3523.488281\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3523.488281)\n",
      "Epoch 260/2000 - Train Loss: 24059.838745 - Val Loss: 3517.878255\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3517.878255)\n",
      "Epoch 261/2000 - Train Loss: 24022.679306 - Val Loss: 3511.947103\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3511.947103)\n",
      "Epoch 262/2000 - Train Loss: 24018.509172 - Val Loss: 3505.369466\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3505.369466)\n",
      "Epoch 263/2000 - Train Loss: 23987.968787 - Val Loss: 3500.495605\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3500.495605)\n",
      "Epoch 264/2000 - Train Loss: 23968.231092 - Val Loss: 3494.169759\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3494.169759)\n",
      "Epoch 265/2000 - Train Loss: 23954.566082 - Val Loss: 3486.279948\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3486.279948)\n",
      "Epoch 266/2000 - Train Loss: 23939.424019 - Val Loss: 3482.119059\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3482.119059)\n",
      "Epoch 267/2000 - Train Loss: 23919.695908 - Val Loss: 3474.896729\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3474.896729)\n",
      "Epoch 268/2000 - Train Loss: 23902.189637 - Val Loss: 3468.020589\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3468.020589)\n",
      "Epoch 269/2000 - Train Loss: 23885.915820 - Val Loss: 3462.578532\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3462.578532)\n",
      "Epoch 270/2000 - Train Loss: 23859.873122 - Val Loss: 3456.684652\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3456.684652)\n",
      "Epoch 271/2000 - Train Loss: 23844.777454 - Val Loss: 3451.282227\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3451.282227)\n",
      "Epoch 272/2000 - Train Loss: 23847.166235 - Val Loss: 3444.796794\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3444.796794)\n",
      "Epoch 273/2000 - Train Loss: 23813.758689 - Val Loss: 3439.064453\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3439.064453)\n",
      "Epoch 274/2000 - Train Loss: 23791.668833 - Val Loss: 3431.867513\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3431.867513)\n",
      "Epoch 275/2000 - Train Loss: 23777.576816 - Val Loss: 3427.026611\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3427.026611)\n",
      "Epoch 276/2000 - Train Loss: 23767.462595 - Val Loss: 3419.606527\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3419.606527)\n",
      "Epoch 277/2000 - Train Loss: 23737.556172 - Val Loss: 3413.674479\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3413.674479)\n",
      "Epoch 278/2000 - Train Loss: 23723.312323 - Val Loss: 3407.781331\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3407.781331)\n",
      "Epoch 279/2000 - Train Loss: 23705.717621 - Val Loss: 3400.911133\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3400.911133)\n",
      "Epoch 280/2000 - Train Loss: 23678.739482 - Val Loss: 3395.240072\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3395.240072)\n",
      "Epoch 281/2000 - Train Loss: 23661.305177 - Val Loss: 3389.131022\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3389.131022)\n",
      "Epoch 282/2000 - Train Loss: 23648.339434 - Val Loss: 3382.668457\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3382.668457)\n",
      "Epoch 283/2000 - Train Loss: 23623.301669 - Val Loss: 3377.027344\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3377.027344)\n",
      "Epoch 284/2000 - Train Loss: 23619.370738 - Val Loss: 3370.474528\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3370.474528)\n",
      "Epoch 285/2000 - Train Loss: 23593.995896 - Val Loss: 3363.176514\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3363.176514)\n",
      "Epoch 286/2000 - Train Loss: 23561.500701 - Val Loss: 3357.919515\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3357.919515)\n",
      "Epoch 287/2000 - Train Loss: 23561.398504 - Val Loss: 3351.050618\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3351.050618)\n",
      "Epoch 288/2000 - Train Loss: 23533.643063 - Val Loss: 3343.508626\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3343.508626)\n",
      "Epoch 289/2000 - Train Loss: 23527.637573 - Val Loss: 3338.720540\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3338.720540)\n",
      "Epoch 290/2000 - Train Loss: 23491.883843 - Val Loss: 3332.579427\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3332.579427)\n",
      "Epoch 291/2000 - Train Loss: 23480.145765 - Val Loss: 3325.328613\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3325.328613)\n",
      "Epoch 292/2000 - Train Loss: 23447.910844 - Val Loss: 3319.950033\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3319.950033)\n",
      "Epoch 293/2000 - Train Loss: 23435.842944 - Val Loss: 3314.223796\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3314.223796)\n",
      "Epoch 294/2000 - Train Loss: 23417.316537 - Val Loss: 3308.097575\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3308.097575)\n",
      "Epoch 295/2000 - Train Loss: 23411.919206 - Val Loss: 3301.782796\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3301.782796)\n",
      "Epoch 296/2000 - Train Loss: 23383.412474 - Val Loss: 3294.085693\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3294.085693)\n",
      "Epoch 297/2000 - Train Loss: 23355.641658 - Val Loss: 3288.629801\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3288.629801)\n",
      "Epoch 298/2000 - Train Loss: 23350.328038 - Val Loss: 3281.979818\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3281.979818)\n",
      "Epoch 299/2000 - Train Loss: 23338.561386 - Val Loss: 3275.081462\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3275.081462)\n",
      "Epoch 300/2000 - Train Loss: 23320.833622 - Val Loss: 3270.076986\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3270.076986)\n",
      "Epoch 301/2000 - Train Loss: 23287.375480 - Val Loss: 3262.848633\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3262.848633)\n",
      "Epoch 302/2000 - Train Loss: 23271.584424 - Val Loss: 3256.119303\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3256.119303)\n",
      "Epoch 303/2000 - Train Loss: 23263.500092 - Val Loss: 3249.560465\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3249.560465)\n",
      "Epoch 304/2000 - Train Loss: 23233.108052 - Val Loss: 3244.126058\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3244.126058)\n",
      "Epoch 305/2000 - Train Loss: 23207.288842 - Val Loss: 3237.547648\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3237.547648)\n",
      "Epoch 306/2000 - Train Loss: 23192.774118 - Val Loss: 3230.787354\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3230.787354)\n",
      "Epoch 307/2000 - Train Loss: 23160.197419 - Val Loss: 3224.606974\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3224.606974)\n",
      "Epoch 308/2000 - Train Loss: 23138.646145 - Val Loss: 3218.256022\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3218.256022)\n",
      "Epoch 309/2000 - Train Loss: 23140.670209 - Val Loss: 3212.076213\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3212.076213)\n",
      "Epoch 310/2000 - Train Loss: 23126.482128 - Val Loss: 3206.302653\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3206.302653)\n",
      "Epoch 311/2000 - Train Loss: 23084.551141 - Val Loss: 3199.570719\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3199.570719)\n",
      "Epoch 312/2000 - Train Loss: 23078.215616 - Val Loss: 3192.411296\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3192.411296)\n",
      "Epoch 313/2000 - Train Loss: 23061.316018 - Val Loss: 3184.949504\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3184.949504)\n",
      "Epoch 314/2000 - Train Loss: 23026.369032 - Val Loss: 3179.252808\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3179.252808)\n",
      "Epoch 315/2000 - Train Loss: 23007.863459 - Val Loss: 3172.782104\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3172.782104)\n",
      "Epoch 316/2000 - Train Loss: 23004.524098 - Val Loss: 3167.036458\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3167.036458)\n",
      "Epoch 317/2000 - Train Loss: 22984.314547 - Val Loss: 3161.104248\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3161.104248)\n",
      "Epoch 318/2000 - Train Loss: 22946.628099 - Val Loss: 3154.992432\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3154.992432)\n",
      "Epoch 319/2000 - Train Loss: 22935.078108 - Val Loss: 3146.093709\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3146.093709)\n",
      "Epoch 320/2000 - Train Loss: 22909.229507 - Val Loss: 3140.269735\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3140.269735)\n",
      "Epoch 321/2000 - Train Loss: 22889.622346 - Val Loss: 3133.743449\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3133.743449)\n",
      "Epoch 322/2000 - Train Loss: 22875.853308 - Val Loss: 3126.701823\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3126.701823)\n",
      "Epoch 323/2000 - Train Loss: 22861.419855 - Val Loss: 3119.039917\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3119.039917)\n",
      "Epoch 324/2000 - Train Loss: 22834.347487 - Val Loss: 3113.956950\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3113.956950)\n",
      "Epoch 325/2000 - Train Loss: 22806.255826 - Val Loss: 3107.523966\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3107.523966)\n",
      "Epoch 326/2000 - Train Loss: 22768.906638 - Val Loss: 3101.717773\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3101.717773)\n",
      "Epoch 327/2000 - Train Loss: 22767.979805 - Val Loss: 3094.388468\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3094.388468)\n",
      "Epoch 328/2000 - Train Loss: 22741.725009 - Val Loss: 3087.455811\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3087.455811)\n",
      "Epoch 329/2000 - Train Loss: 22733.531962 - Val Loss: 3081.782756\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3081.782756)\n",
      "Epoch 330/2000 - Train Loss: 22699.995950 - Val Loss: 3074.633382\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3074.633382)\n",
      "Epoch 331/2000 - Train Loss: 22672.215523 - Val Loss: 3068.013062\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3068.013062)\n",
      "Epoch 332/2000 - Train Loss: 22657.706484 - Val Loss: 3062.030192\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3062.030192)\n",
      "Epoch 333/2000 - Train Loss: 22644.986056 - Val Loss: 3056.352498\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3056.352498)\n",
      "Epoch 334/2000 - Train Loss: 22616.840218 - Val Loss: 3048.625814\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3048.625814)\n",
      "Epoch 335/2000 - Train Loss: 22611.092723 - Val Loss: 3042.555379\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3042.555379)\n",
      "Epoch 336/2000 - Train Loss: 22600.113303 - Val Loss: 3036.173747\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3036.173747)\n",
      "Epoch 337/2000 - Train Loss: 22575.095579 - Val Loss: 3030.548381\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3030.548381)\n",
      "Epoch 338/2000 - Train Loss: 22523.956761 - Val Loss: 3022.533203\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3022.533203)\n",
      "Epoch 339/2000 - Train Loss: 22530.804833 - Val Loss: 3016.291829\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3016.291829)\n",
      "Epoch 340/2000 - Train Loss: 22507.834031 - Val Loss: 3008.112427\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3008.112427)\n",
      "Epoch 341/2000 - Train Loss: 22469.600240 - Val Loss: 3001.893473\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 3001.893473)\n",
      "Epoch 342/2000 - Train Loss: 22461.462228 - Val Loss: 2995.258586\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2995.258586)\n",
      "Epoch 343/2000 - Train Loss: 22445.634831 - Val Loss: 2987.356567\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2987.356567)\n",
      "Epoch 344/2000 - Train Loss: 22422.836463 - Val Loss: 2982.463053\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2982.463053)\n",
      "Epoch 345/2000 - Train Loss: 22408.539193 - Val Loss: 2976.183187\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2976.183187)\n",
      "Epoch 346/2000 - Train Loss: 22365.276595 - Val Loss: 2968.337280\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2968.337280)\n",
      "Epoch 347/2000 - Train Loss: 22353.058698 - Val Loss: 2961.609701\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2961.609701)\n",
      "Epoch 348/2000 - Train Loss: 22351.983102 - Val Loss: 2955.251750\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2955.251750)\n",
      "Epoch 349/2000 - Train Loss: 22318.650056 - Val Loss: 2949.518962\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2949.518962)\n",
      "Epoch 350/2000 - Train Loss: 22279.856240 - Val Loss: 2940.545939\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2940.545939)\n",
      "Epoch 351/2000 - Train Loss: 22264.602729 - Val Loss: 2935.098836\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2935.098836)\n",
      "Epoch 352/2000 - Train Loss: 22259.130992 - Val Loss: 2930.485352\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2930.485352)\n",
      "Epoch 353/2000 - Train Loss: 22219.259853 - Val Loss: 2921.049642\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2921.049642)\n",
      "Epoch 354/2000 - Train Loss: 22183.962233 - Val Loss: 2914.620931\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2914.620931)\n",
      "Epoch 355/2000 - Train Loss: 22178.165236 - Val Loss: 2907.500936\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2907.500936)\n",
      "Epoch 356/2000 - Train Loss: 22118.469696 - Val Loss: 2900.711100\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2900.711100)\n",
      "Epoch 357/2000 - Train Loss: 22138.280775 - Val Loss: 2894.203044\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2894.203044)\n",
      "Epoch 358/2000 - Train Loss: 22128.771409 - Val Loss: 2888.260864\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2888.260864)\n",
      "Epoch 359/2000 - Train Loss: 22107.235397 - Val Loss: 2879.725993\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2879.725993)\n",
      "Epoch 360/2000 - Train Loss: 22086.250075 - Val Loss: 2874.166707\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2874.166707)\n",
      "Epoch 361/2000 - Train Loss: 22042.468723 - Val Loss: 2867.022746\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2867.022746)\n",
      "Epoch 362/2000 - Train Loss: 22030.888477 - Val Loss: 2860.983032\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2860.983032)\n",
      "Epoch 363/2000 - Train Loss: 22033.390859 - Val Loss: 2853.387451\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2853.387451)\n",
      "Epoch 364/2000 - Train Loss: 21984.704472 - Val Loss: 2846.851400\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2846.851400)\n",
      "Epoch 365/2000 - Train Loss: 21965.626436 - Val Loss: 2838.939412\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2838.939412)\n",
      "Epoch 366/2000 - Train Loss: 21948.439991 - Val Loss: 2833.824748\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2833.824748)\n",
      "Epoch 367/2000 - Train Loss: 21934.862741 - Val Loss: 2824.792684\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2824.792684)\n",
      "Epoch 368/2000 - Train Loss: 21907.159696 - Val Loss: 2819.283203\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2819.283203)\n",
      "Epoch 369/2000 - Train Loss: 21887.459065 - Val Loss: 2813.107666\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2813.107666)\n",
      "Epoch 370/2000 - Train Loss: 21861.403036 - Val Loss: 2806.179281\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2806.179281)\n",
      "Epoch 371/2000 - Train Loss: 21835.023553 - Val Loss: 2799.494751\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2799.494751)\n",
      "Epoch 372/2000 - Train Loss: 21841.965018 - Val Loss: 2793.429240\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2793.429240)\n",
      "Epoch 373/2000 - Train Loss: 21783.581989 - Val Loss: 2785.807048\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2785.807048)\n",
      "Epoch 374/2000 - Train Loss: 21764.894918 - Val Loss: 2778.545451\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2778.545451)\n",
      "Epoch 375/2000 - Train Loss: 21755.593375 - Val Loss: 2773.727620\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2773.727620)\n",
      "Epoch 376/2000 - Train Loss: 21727.353935 - Val Loss: 2765.347412\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2765.347412)\n",
      "Epoch 377/2000 - Train Loss: 21710.240199 - Val Loss: 2760.009603\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2760.009603)\n",
      "Epoch 378/2000 - Train Loss: 21686.443803 - Val Loss: 2752.013062\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2752.013062)\n",
      "Epoch 379/2000 - Train Loss: 21679.256415 - Val Loss: 2745.081177\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2745.081177)\n",
      "Epoch 380/2000 - Train Loss: 21650.657326 - Val Loss: 2738.808553\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2738.808553)\n",
      "Epoch 381/2000 - Train Loss: 21612.562232 - Val Loss: 2729.621257\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2729.621257)\n",
      "Epoch 382/2000 - Train Loss: 21547.063046 - Val Loss: 2723.530233\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2723.530233)\n",
      "Epoch 383/2000 - Train Loss: 21578.783498 - Val Loss: 2717.352824\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2717.352824)\n",
      "Epoch 384/2000 - Train Loss: 21544.252592 - Val Loss: 2710.968465\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2710.968465)\n",
      "Epoch 385/2000 - Train Loss: 21536.499211 - Val Loss: 2701.968302\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2701.968302)\n",
      "Epoch 386/2000 - Train Loss: 21513.385114 - Val Loss: 2696.973755\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2696.973755)\n",
      "Epoch 387/2000 - Train Loss: 21472.422610 - Val Loss: 2689.436483\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2689.436483)\n",
      "Epoch 388/2000 - Train Loss: 21458.807966 - Val Loss: 2684.653564\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2684.653564)\n",
      "Epoch 389/2000 - Train Loss: 21430.069434 - Val Loss: 2677.837484\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2677.837484)\n",
      "Epoch 390/2000 - Train Loss: 21421.514452 - Val Loss: 2670.348958\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2670.348958)\n",
      "Epoch 391/2000 - Train Loss: 21418.164150 - Val Loss: 2662.677775\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2662.677775)\n",
      "Epoch 392/2000 - Train Loss: 21341.583046 - Val Loss: 2656.668416\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2656.668416)\n",
      "Epoch 393/2000 - Train Loss: 21343.599479 - Val Loss: 2649.612671\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2649.612671)\n",
      "Epoch 394/2000 - Train Loss: 21345.431985 - Val Loss: 2642.548340\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2642.548340)\n",
      "Epoch 395/2000 - Train Loss: 21283.697830 - Val Loss: 2634.182170\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2634.182170)\n",
      "Epoch 396/2000 - Train Loss: 21277.470127 - Val Loss: 2627.371379\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2627.371379)\n",
      "Epoch 397/2000 - Train Loss: 21245.982666 - Val Loss: 2621.555461\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2621.555461)\n",
      "Epoch 398/2000 - Train Loss: 21227.179321 - Val Loss: 2615.736125\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2615.736125)\n",
      "Epoch 399/2000 - Train Loss: 21225.596820 - Val Loss: 2607.034871\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2607.034871)\n",
      "Epoch 400/2000 - Train Loss: 21165.375695 - Val Loss: 2600.737223\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2600.737223)\n",
      "Epoch 401/2000 - Train Loss: 21155.451546 - Val Loss: 2593.652995\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2593.652995)\n",
      "Epoch 402/2000 - Train Loss: 21180.047541 - Val Loss: 2586.977539\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2586.977539)\n",
      "Epoch 403/2000 - Train Loss: 21108.854123 - Val Loss: 2579.626139\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2579.626139)\n",
      "Epoch 404/2000 - Train Loss: 21043.984122 - Val Loss: 2572.800944\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2572.800944)\n",
      "Epoch 405/2000 - Train Loss: 21062.612041 - Val Loss: 2567.422648\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2567.422648)\n",
      "Epoch 406/2000 - Train Loss: 21061.924571 - Val Loss: 2559.615682\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2559.615682)\n",
      "Epoch 407/2000 - Train Loss: 21013.861665 - Val Loss: 2551.191121\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2551.191121)\n",
      "Epoch 408/2000 - Train Loss: 21017.081464 - Val Loss: 2545.537516\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2545.537516)\n",
      "Epoch 409/2000 - Train Loss: 20971.337173 - Val Loss: 2539.981201\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2539.981201)\n",
      "Epoch 410/2000 - Train Loss: 20958.859781 - Val Loss: 2529.649943\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2529.649943)\n",
      "Epoch 411/2000 - Train Loss: 20942.471648 - Val Loss: 2525.736410\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2525.736410)\n",
      "Epoch 412/2000 - Train Loss: 20898.672180 - Val Loss: 2518.110311\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2518.110311)\n",
      "Epoch 413/2000 - Train Loss: 20893.623831 - Val Loss: 2510.349609\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2510.349609)\n",
      "Epoch 414/2000 - Train Loss: 20856.550896 - Val Loss: 2505.954671\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2505.954671)\n",
      "Epoch 415/2000 - Train Loss: 20808.528901 - Val Loss: 2496.762573\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2496.762573)\n",
      "Epoch 416/2000 - Train Loss: 20836.055033 - Val Loss: 2489.821615\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2489.821615)\n",
      "Epoch 417/2000 - Train Loss: 20787.082556 - Val Loss: 2481.750326\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2481.750326)\n",
      "Epoch 418/2000 - Train Loss: 20787.726744 - Val Loss: 2475.023478\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2475.023478)\n",
      "Epoch 419/2000 - Train Loss: 20743.443019 - Val Loss: 2468.495280\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2468.495280)\n",
      "Epoch 420/2000 - Train Loss: 20705.081044 - Val Loss: 2459.061198\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2459.061198)\n",
      "Epoch 421/2000 - Train Loss: 20672.363128 - Val Loss: 2454.604207\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2454.604207)\n",
      "Epoch 422/2000 - Train Loss: 20649.544174 - Val Loss: 2448.752319\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2448.752319)\n",
      "Epoch 423/2000 - Train Loss: 20624.787233 - Val Loss: 2438.353882\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2438.353882)\n",
      "Epoch 424/2000 - Train Loss: 20623.540971 - Val Loss: 2433.657552\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2433.657552)\n",
      "Epoch 425/2000 - Train Loss: 20618.499005 - Val Loss: 2425.003459\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2425.003459)\n",
      "Epoch 426/2000 - Train Loss: 20602.426066 - Val Loss: 2419.034587\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2419.034587)\n",
      "Epoch 427/2000 - Train Loss: 20562.897593 - Val Loss: 2412.684041\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2412.684041)\n",
      "Epoch 428/2000 - Train Loss: 20502.566007 - Val Loss: 2404.003418\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2404.003418)\n",
      "Epoch 429/2000 - Train Loss: 20528.348001 - Val Loss: 2398.749471\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2398.749471)\n",
      "Epoch 430/2000 - Train Loss: 20505.208170 - Val Loss: 2392.527222\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2392.527222)\n",
      "Epoch 431/2000 - Train Loss: 20500.827647 - Val Loss: 2383.911580\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2383.911580)\n",
      "Epoch 432/2000 - Train Loss: 20428.268354 - Val Loss: 2377.960449\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2377.960449)\n",
      "Epoch 433/2000 - Train Loss: 20406.216227 - Val Loss: 2371.444214\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2371.444214)\n",
      "Epoch 434/2000 - Train Loss: 20372.885288 - Val Loss: 2364.958862\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2364.958862)\n",
      "Epoch 435/2000 - Train Loss: 20353.747053 - Val Loss: 2357.256388\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2357.256388)\n",
      "Epoch 436/2000 - Train Loss: 20361.413059 - Val Loss: 2348.814616\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2348.814616)\n",
      "Epoch 437/2000 - Train Loss: 20327.597318 - Val Loss: 2341.436727\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2341.436727)\n",
      "Epoch 438/2000 - Train Loss: 20312.377553 - Val Loss: 2337.062663\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2337.062663)\n",
      "Epoch 439/2000 - Train Loss: 20256.897508 - Val Loss: 2329.577026\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2329.577026)\n",
      "Epoch 440/2000 - Train Loss: 20251.242881 - Val Loss: 2322.935343\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2322.935343)\n",
      "Epoch 441/2000 - Train Loss: 20220.207000 - Val Loss: 2315.736450\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2315.736450)\n",
      "Epoch 442/2000 - Train Loss: 20173.512022 - Val Loss: 2310.034668\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2310.034668)\n",
      "Epoch 443/2000 - Train Loss: 20157.233386 - Val Loss: 2303.908407\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2303.908407)\n",
      "Epoch 444/2000 - Train Loss: 20121.042498 - Val Loss: 2296.779948\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2296.779948)\n",
      "Epoch 445/2000 - Train Loss: 20109.745389 - Val Loss: 2285.996460\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2285.996460)\n",
      "Epoch 446/2000 - Train Loss: 20077.208211 - Val Loss: 2283.899699\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2283.899699)\n",
      "Epoch 447/2000 - Train Loss: 20068.621061 - Val Loss: 2277.189901\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2277.189901)\n",
      "Epoch 448/2000 - Train Loss: 20075.318778 - Val Loss: 2269.390584\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2269.390584)\n",
      "Epoch 449/2000 - Train Loss: 20041.876446 - Val Loss: 2259.245158\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2259.245158)\n",
      "Epoch 450/2000 - Train Loss: 19986.149988 - Val Loss: 2253.900472\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2253.900472)\n",
      "Epoch 451/2000 - Train Loss: 19989.228654 - Val Loss: 2247.531820\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2247.531820)\n",
      "Epoch 452/2000 - Train Loss: 19939.518559 - Val Loss: 2237.346924\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2237.346924)\n",
      "Epoch 453/2000 - Train Loss: 19922.617871 - Val Loss: 2232.820353\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2232.820353)\n",
      "Epoch 454/2000 - Train Loss: 19919.515964 - Val Loss: 2225.823893\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2225.823893)\n",
      "Epoch 455/2000 - Train Loss: 19881.775575 - Val Loss: 2220.053263\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2220.053263)\n",
      "Epoch 456/2000 - Train Loss: 19881.200701 - Val Loss: 2212.757487\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2212.757487)\n",
      "Epoch 457/2000 - Train Loss: 19813.093219 - Val Loss: 2204.901082\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2204.901082)\n",
      "Epoch 458/2000 - Train Loss: 19835.178361 - Val Loss: 2196.426351\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2196.426351)\n",
      "Epoch 459/2000 - Train Loss: 19793.844882 - Val Loss: 2190.036947\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2190.036947)\n",
      "Epoch 460/2000 - Train Loss: 19778.411656 - Val Loss: 2183.784424\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2183.784424)\n",
      "Epoch 461/2000 - Train Loss: 19735.146841 - Val Loss: 2176.732259\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2176.732259)\n",
      "Epoch 462/2000 - Train Loss: 19709.375142 - Val Loss: 2168.763021\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2168.763021)\n",
      "Epoch 463/2000 - Train Loss: 19663.852469 - Val Loss: 2162.986694\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2162.986694)\n",
      "Epoch 464/2000 - Train Loss: 19653.032834 - Val Loss: 2157.789795\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2157.789795)\n",
      "Epoch 465/2000 - Train Loss: 19612.465302 - Val Loss: 2150.642212\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2150.642212)\n",
      "Epoch 466/2000 - Train Loss: 19616.666237 - Val Loss: 2141.897217\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2141.897217)\n",
      "Epoch 467/2000 - Train Loss: 19569.029778 - Val Loss: 2137.011841\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2137.011841)\n",
      "Epoch 468/2000 - Train Loss: 19569.357818 - Val Loss: 2127.681763\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2127.681763)\n",
      "Epoch 469/2000 - Train Loss: 19562.350565 - Val Loss: 2121.563965\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2121.563965)\n",
      "Epoch 470/2000 - Train Loss: 19516.023579 - Val Loss: 2114.998413\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2114.998413)\n",
      "Epoch 471/2000 - Train Loss: 19458.976857 - Val Loss: 2109.494466\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2109.494466)\n",
      "Epoch 472/2000 - Train Loss: 19423.842860 - Val Loss: 2102.645752\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2102.645752)\n",
      "Epoch 473/2000 - Train Loss: 19405.876948 - Val Loss: 2094.422852\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2094.422852)\n",
      "Epoch 474/2000 - Train Loss: 19400.828241 - Val Loss: 2088.652466\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2088.652466)\n",
      "Epoch 475/2000 - Train Loss: 19392.704887 - Val Loss: 2079.466512\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2079.466512)\n",
      "Epoch 476/2000 - Train Loss: 19363.367606 - Val Loss: 2073.404134\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2073.404134)\n",
      "Epoch 477/2000 - Train Loss: 19348.452255 - Val Loss: 2068.040283\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2068.040283)\n",
      "Epoch 478/2000 - Train Loss: 19300.993493 - Val Loss: 2061.545207\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2061.545207)\n",
      "Epoch 479/2000 - Train Loss: 19303.640145 - Val Loss: 2052.409953\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2052.409953)\n",
      "Epoch 480/2000 - Train Loss: 19251.659153 - Val Loss: 2045.953613\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2045.953613)\n",
      "Epoch 481/2000 - Train Loss: 19245.975893 - Val Loss: 2040.407511\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2040.407511)\n",
      "Epoch 482/2000 - Train Loss: 19234.167812 - Val Loss: 2035.050903\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2035.050903)\n",
      "Epoch 483/2000 - Train Loss: 19181.433779 - Val Loss: 2024.938110\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2024.938110)\n",
      "Epoch 484/2000 - Train Loss: 19150.988104 - Val Loss: 2021.711833\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2021.711833)\n",
      "Epoch 485/2000 - Train Loss: 19153.640316 - Val Loss: 2012.997437\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2012.997437)\n",
      "Epoch 486/2000 - Train Loss: 19093.489288 - Val Loss: 2009.649292\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 2009.649292)\n",
      "Epoch 487/2000 - Train Loss: 19069.206163 - Val Loss: 1998.712972\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1998.712972)\n",
      "Epoch 488/2000 - Train Loss: 19020.303601 - Val Loss: 1991.808390\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1991.808390)\n",
      "Epoch 489/2000 - Train Loss: 19047.169369 - Val Loss: 1984.774251\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1984.774251)\n",
      "Epoch 490/2000 - Train Loss: 19031.885237 - Val Loss: 1980.532593\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1980.532593)\n",
      "Epoch 491/2000 - Train Loss: 18977.069677 - Val Loss: 1971.618693\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1971.618693)\n",
      "Epoch 492/2000 - Train Loss: 18979.005104 - Val Loss: 1966.061198\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1966.061198)\n",
      "Epoch 493/2000 - Train Loss: 18960.792494 - Val Loss: 1958.691325\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1958.691325)\n",
      "Epoch 494/2000 - Train Loss: 18903.826430 - Val Loss: 1951.571879\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1951.571879)\n",
      "Epoch 495/2000 - Train Loss: 18911.270586 - Val Loss: 1946.424601\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1946.424601)\n",
      "Epoch 496/2000 - Train Loss: 18895.061967 - Val Loss: 1937.895915\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1937.895915)\n",
      "Epoch 497/2000 - Train Loss: 18831.926655 - Val Loss: 1931.525614\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1931.525614)\n",
      "Epoch 498/2000 - Train Loss: 18821.650223 - Val Loss: 1923.392253\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1923.392253)\n",
      "Epoch 499/2000 - Train Loss: 18782.248771 - Val Loss: 1916.292603\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1916.292603)\n",
      "Epoch 500/2000 - Train Loss: 18756.805293 - Val Loss: 1910.713521\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1910.713521)\n",
      "Epoch 501/2000 - Train Loss: 18735.464275 - Val Loss: 1901.778300\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1901.778300)\n",
      "Epoch 502/2000 - Train Loss: 18725.117469 - Val Loss: 1897.921895\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1897.921895)\n",
      "Epoch 503/2000 - Train Loss: 18676.667950 - Val Loss: 1890.464111\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1890.464111)\n",
      "Epoch 504/2000 - Train Loss: 18646.793338 - Val Loss: 1883.763102\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1883.763102)\n",
      "Epoch 505/2000 - Train Loss: 18645.131665 - Val Loss: 1879.466532\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1879.466532)\n",
      "Epoch 506/2000 - Train Loss: 18595.472512 - Val Loss: 1871.547384\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1871.547384)\n",
      "Epoch 507/2000 - Train Loss: 18559.187141 - Val Loss: 1864.020040\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1864.020040)\n",
      "Epoch 508/2000 - Train Loss: 18565.595619 - Val Loss: 1857.872253\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1857.872253)\n",
      "Epoch 509/2000 - Train Loss: 18542.935471 - Val Loss: 1852.218140\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1852.218140)\n",
      "Epoch 510/2000 - Train Loss: 18468.207005 - Val Loss: 1843.653341\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1843.653341)\n",
      "Epoch 511/2000 - Train Loss: 18503.473710 - Val Loss: 1836.218974\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1836.218974)\n",
      "Epoch 512/2000 - Train Loss: 18441.275145 - Val Loss: 1832.356262\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1832.356262)\n",
      "Epoch 513/2000 - Train Loss: 18446.879939 - Val Loss: 1824.860046\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1824.860046)\n",
      "Epoch 514/2000 - Train Loss: 18434.165460 - Val Loss: 1819.626750\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1819.626750)\n",
      "Epoch 515/2000 - Train Loss: 18359.907057 - Val Loss: 1811.537211\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1811.537211)\n",
      "Epoch 516/2000 - Train Loss: 18352.100942 - Val Loss: 1804.938965\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1804.938965)\n",
      "Epoch 517/2000 - Train Loss: 18312.811656 - Val Loss: 1799.487081\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1799.487081)\n",
      "Epoch 518/2000 - Train Loss: 18314.560309 - Val Loss: 1791.271444\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1791.271444)\n",
      "Epoch 519/2000 - Train Loss: 18245.373739 - Val Loss: 1787.785706\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1787.785706)\n",
      "Epoch 520/2000 - Train Loss: 18215.192373 - Val Loss: 1775.926147\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1775.926147)\n",
      "Epoch 521/2000 - Train Loss: 18220.558910 - Val Loss: 1771.544922\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1771.544922)\n",
      "Epoch 522/2000 - Train Loss: 18195.564461 - Val Loss: 1764.923136\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1764.923136)\n",
      "Epoch 523/2000 - Train Loss: 18153.165252 - Val Loss: 1759.355001\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1759.355001)\n",
      "Epoch 524/2000 - Train Loss: 18157.187447 - Val Loss: 1752.723572\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1752.723572)\n",
      "Epoch 525/2000 - Train Loss: 18105.765796 - Val Loss: 1746.735616\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1746.735616)\n",
      "Epoch 526/2000 - Train Loss: 18121.800566 - Val Loss: 1738.781555\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1738.781555)\n",
      "Epoch 527/2000 - Train Loss: 18047.110169 - Val Loss: 1733.099630\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1733.099630)\n",
      "Epoch 528/2000 - Train Loss: 18050.240868 - Val Loss: 1727.297119\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1727.297119)\n",
      "Epoch 529/2000 - Train Loss: 18040.122407 - Val Loss: 1721.207825\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1721.207825)\n",
      "Epoch 530/2000 - Train Loss: 17953.510483 - Val Loss: 1715.006510\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1715.006510)\n",
      "Epoch 531/2000 - Train Loss: 17952.800711 - Val Loss: 1708.427124\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1708.427124)\n",
      "Epoch 532/2000 - Train Loss: 17960.467783 - Val Loss: 1701.122416\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1701.122416)\n",
      "Epoch 533/2000 - Train Loss: 17944.294954 - Val Loss: 1694.422201\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1694.422201)\n",
      "Epoch 534/2000 - Train Loss: 17920.943395 - Val Loss: 1689.833089\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1689.833089)\n",
      "Epoch 535/2000 - Train Loss: 17872.626225 - Val Loss: 1684.381775\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1684.381775)\n",
      "Epoch 536/2000 - Train Loss: 17892.754619 - Val Loss: 1673.909383\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1673.909383)\n",
      "Epoch 537/2000 - Train Loss: 17751.746282 - Val Loss: 1667.883097\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1667.883097)\n",
      "Epoch 538/2000 - Train Loss: 17822.880287 - Val Loss: 1663.375549\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1663.375549)\n",
      "Epoch 539/2000 - Train Loss: 17778.160151 - Val Loss: 1655.134135\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1655.134135)\n",
      "Epoch 540/2000 - Train Loss: 17730.473260 - Val Loss: 1649.734049\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1649.734049)\n",
      "Epoch 541/2000 - Train Loss: 17710.047643 - Val Loss: 1644.171712\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1644.171712)\n",
      "Epoch 542/2000 - Train Loss: 17674.928140 - Val Loss: 1638.158630\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1638.158630)\n",
      "Epoch 543/2000 - Train Loss: 17664.661578 - Val Loss: 1628.674764\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1628.674764)\n",
      "Epoch 544/2000 - Train Loss: 17596.665032 - Val Loss: 1625.305623\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1625.305623)\n",
      "Epoch 545/2000 - Train Loss: 17610.651191 - Val Loss: 1618.929667\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1618.929667)\n",
      "Epoch 546/2000 - Train Loss: 17560.245904 - Val Loss: 1612.210917\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1612.210917)\n",
      "Epoch 547/2000 - Train Loss: 17563.216635 - Val Loss: 1605.872416\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1605.872416)\n",
      "Epoch 548/2000 - Train Loss: 17550.768382 - Val Loss: 1597.523661\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1597.523661)\n",
      "Epoch 549/2000 - Train Loss: 17486.768020 - Val Loss: 1593.246948\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1593.246948)\n",
      "Epoch 550/2000 - Train Loss: 17473.485512 - Val Loss: 1585.963216\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1585.963216)\n",
      "Epoch 551/2000 - Train Loss: 17485.393454 - Val Loss: 1578.781555\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1578.781555)\n",
      "Epoch 552/2000 - Train Loss: 17400.662037 - Val Loss: 1574.784078\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1574.784078)\n",
      "Epoch 553/2000 - Train Loss: 17400.255058 - Val Loss: 1568.021484\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1568.021484)\n",
      "Epoch 554/2000 - Train Loss: 17360.758769 - Val Loss: 1561.773051\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1561.773051)\n",
      "Epoch 555/2000 - Train Loss: 17313.830709 - Val Loss: 1553.302633\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1553.302633)\n",
      "Epoch 556/2000 - Train Loss: 17327.493921 - Val Loss: 1550.812052\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1550.812052)\n",
      "Epoch 557/2000 - Train Loss: 17302.359432 - Val Loss: 1544.526754\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1544.526754)\n",
      "Epoch 558/2000 - Train Loss: 17220.636744 - Val Loss: 1539.041016\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1539.041016)\n",
      "Epoch 559/2000 - Train Loss: 17282.669527 - Val Loss: 1531.233866\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1531.233866)\n",
      "Epoch 560/2000 - Train Loss: 17250.158817 - Val Loss: 1524.845785\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1524.845785)\n",
      "Epoch 561/2000 - Train Loss: 17198.140798 - Val Loss: 1519.827413\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1519.827413)\n",
      "Epoch 562/2000 - Train Loss: 17175.156859 - Val Loss: 1513.417704\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1513.417704)\n",
      "Epoch 563/2000 - Train Loss: 17180.803358 - Val Loss: 1503.857768\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1503.857768)\n",
      "Epoch 564/2000 - Train Loss: 17091.976089 - Val Loss: 1499.195841\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1499.195841)\n",
      "Epoch 565/2000 - Train Loss: 17057.291298 - Val Loss: 1494.766256\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1494.766256)\n",
      "Epoch 566/2000 - Train Loss: 17083.427367 - Val Loss: 1488.728048\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1488.728048)\n",
      "Epoch 567/2000 - Train Loss: 17072.293763 - Val Loss: 1483.875997\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1483.875997)\n",
      "Epoch 568/2000 - Train Loss: 17044.324351 - Val Loss: 1475.871602\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1475.871602)\n",
      "Epoch 569/2000 - Train Loss: 16986.079041 - Val Loss: 1470.665894\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1470.665894)\n",
      "Epoch 570/2000 - Train Loss: 16965.610818 - Val Loss: 1464.457845\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1464.457845)\n",
      "Epoch 571/2000 - Train Loss: 16945.393495 - Val Loss: 1458.313619\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1458.313619)\n",
      "Epoch 572/2000 - Train Loss: 16902.684013 - Val Loss: 1454.075195\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1454.075195)\n",
      "Epoch 573/2000 - Train Loss: 16860.142453 - Val Loss: 1448.016703\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1448.016703)\n",
      "Epoch 574/2000 - Train Loss: 16850.940234 - Val Loss: 1440.752808\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1440.752808)\n",
      "Epoch 575/2000 - Train Loss: 16830.117386 - Val Loss: 1435.879415\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1435.879415)\n",
      "Epoch 576/2000 - Train Loss: 16792.812305 - Val Loss: 1431.006185\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1431.006185)\n",
      "Epoch 577/2000 - Train Loss: 16812.616810 - Val Loss: 1422.176453\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1422.176453)\n",
      "Epoch 578/2000 - Train Loss: 16741.937211 - Val Loss: 1417.773051\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1417.773051)\n",
      "Epoch 579/2000 - Train Loss: 16693.354549 - Val Loss: 1410.323059\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1410.323059)\n",
      "Epoch 580/2000 - Train Loss: 16716.639593 - Val Loss: 1405.793355\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1405.793355)\n",
      "Epoch 581/2000 - Train Loss: 16658.575643 - Val Loss: 1398.497335\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1398.497335)\n",
      "Epoch 582/2000 - Train Loss: 16654.339327 - Val Loss: 1391.267253\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1391.267253)\n",
      "Epoch 583/2000 - Train Loss: 16650.491364 - Val Loss: 1390.091512\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1390.091512)\n",
      "Epoch 584/2000 - Train Loss: 16596.235786 - Val Loss: 1382.401571\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1382.401571)\n",
      "Epoch 585/2000 - Train Loss: 16565.087562 - Val Loss: 1377.638468\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1377.638468)\n",
      "Epoch 586/2000 - Train Loss: 16544.542966 - Val Loss: 1372.895528\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1372.895528)\n",
      "Epoch 587/2000 - Train Loss: 16526.129805 - Val Loss: 1364.168091\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1364.168091)\n",
      "Epoch 588/2000 - Train Loss: 16475.414389 - Val Loss: 1360.601542\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1360.601542)\n",
      "Epoch 589/2000 - Train Loss: 16504.024035 - Val Loss: 1352.953206\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1352.953206)\n",
      "Epoch 590/2000 - Train Loss: 16485.054765 - Val Loss: 1346.423340\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1346.423340)\n",
      "Epoch 591/2000 - Train Loss: 16422.479117 - Val Loss: 1342.463582\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1342.463582)\n",
      "Epoch 592/2000 - Train Loss: 16376.086454 - Val Loss: 1336.326640\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1336.326640)\n",
      "Epoch 593/2000 - Train Loss: 16320.677303 - Val Loss: 1332.064290\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1332.064290)\n",
      "Epoch 594/2000 - Train Loss: 16372.275060 - Val Loss: 1323.770264\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1323.770264)\n",
      "Epoch 595/2000 - Train Loss: 16325.486820 - Val Loss: 1318.025635\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1318.025635)\n",
      "Epoch 596/2000 - Train Loss: 16289.837427 - Val Loss: 1312.943461\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1312.943461)\n",
      "Epoch 597/2000 - Train Loss: 16252.353151 - Val Loss: 1308.856059\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1308.856059)\n",
      "Epoch 598/2000 - Train Loss: 16210.992591 - Val Loss: 1302.812195\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1302.812195)\n",
      "Epoch 599/2000 - Train Loss: 16212.512960 - Val Loss: 1298.430908\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1298.430908)\n",
      "Epoch 600/2000 - Train Loss: 16172.685805 - Val Loss: 1290.651917\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1290.651917)\n",
      "Epoch 601/2000 - Train Loss: 16152.069616 - Val Loss: 1287.215922\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1287.215922)\n",
      "Epoch 602/2000 - Train Loss: 16126.564265 - Val Loss: 1281.043030\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1281.043030)\n",
      "Epoch 603/2000 - Train Loss: 16080.553017 - Val Loss: 1272.863200\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1272.863200)\n",
      "Epoch 604/2000 - Train Loss: 16099.099946 - Val Loss: 1273.311503\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 605/2000 - Train Loss: 16026.601643 - Val Loss: 1263.589294\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1263.589294)\n",
      "Epoch 606/2000 - Train Loss: 15995.073544 - Val Loss: 1256.946228\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1256.946228)\n",
      "Epoch 607/2000 - Train Loss: 16009.719497 - Val Loss: 1255.130005\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1255.130005)\n",
      "Epoch 608/2000 - Train Loss: 15939.522441 - Val Loss: 1246.499308\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1246.499308)\n",
      "Epoch 609/2000 - Train Loss: 15943.697962 - Val Loss: 1241.339518\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1241.339518)\n",
      "Epoch 610/2000 - Train Loss: 15923.371352 - Val Loss: 1238.121216\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1238.121216)\n",
      "Epoch 611/2000 - Train Loss: 15868.719689 - Val Loss: 1231.955933\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1231.955933)\n",
      "Epoch 612/2000 - Train Loss: 15871.369400 - Val Loss: 1226.203674\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1226.203674)\n",
      "Epoch 613/2000 - Train Loss: 15817.492881 - Val Loss: 1221.960866\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1221.960866)\n",
      "Epoch 614/2000 - Train Loss: 15782.742161 - Val Loss: 1214.736776\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1214.736776)\n",
      "Epoch 615/2000 - Train Loss: 15763.951610 - Val Loss: 1209.932200\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1209.932200)\n",
      "Epoch 616/2000 - Train Loss: 15763.591891 - Val Loss: 1206.310069\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1206.310069)\n",
      "Epoch 617/2000 - Train Loss: 15696.449261 - Val Loss: 1201.182048\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1201.182048)\n",
      "Epoch 618/2000 - Train Loss: 15694.337808 - Val Loss: 1194.094564\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1194.094564)\n",
      "Epoch 619/2000 - Train Loss: 15747.773471 - Val Loss: 1190.902710\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1190.902710)\n",
      "Epoch 620/2000 - Train Loss: 15668.420421 - Val Loss: 1184.704712\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1184.704712)\n",
      "Epoch 621/2000 - Train Loss: 15653.559500 - Val Loss: 1180.364248\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1180.364248)\n",
      "Epoch 622/2000 - Train Loss: 15534.743724 - Val Loss: 1173.193563\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1173.193563)\n",
      "Epoch 623/2000 - Train Loss: 15586.856695 - Val Loss: 1167.244853\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1167.244853)\n",
      "Epoch 624/2000 - Train Loss: 15554.795917 - Val Loss: 1163.904175\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1163.904175)\n",
      "Epoch 625/2000 - Train Loss: 15509.940209 - Val Loss: 1158.764699\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1158.764699)\n",
      "Epoch 626/2000 - Train Loss: 15506.293065 - Val Loss: 1154.221863\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1154.221863)\n",
      "Epoch 627/2000 - Train Loss: 15482.846303 - Val Loss: 1151.422231\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1151.422231)\n",
      "Epoch 628/2000 - Train Loss: 15481.156224 - Val Loss: 1146.357198\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1146.357198)\n",
      "Epoch 629/2000 - Train Loss: 15432.801427 - Val Loss: 1138.475596\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1138.475596)\n",
      "Epoch 630/2000 - Train Loss: 15379.565212 - Val Loss: 1134.093455\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1134.093455)\n",
      "Epoch 631/2000 - Train Loss: 15321.023636 - Val Loss: 1127.819051\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1127.819051)\n",
      "Epoch 632/2000 - Train Loss: 15382.877064 - Val Loss: 1123.261027\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1123.261027)\n",
      "Epoch 633/2000 - Train Loss: 15330.452484 - Val Loss: 1118.970968\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1118.970968)\n",
      "Epoch 634/2000 - Train Loss: 15327.627857 - Val Loss: 1114.112142\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1114.112142)\n",
      "Epoch 635/2000 - Train Loss: 15275.288379 - Val Loss: 1110.783620\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1110.783620)\n",
      "Epoch 636/2000 - Train Loss: 15284.899977 - Val Loss: 1105.918762\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1105.918762)\n",
      "Epoch 637/2000 - Train Loss: 15219.138431 - Val Loss: 1100.225505\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1100.225505)\n",
      "Epoch 638/2000 - Train Loss: 15139.442529 - Val Loss: 1096.213684\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1096.213684)\n",
      "Epoch 639/2000 - Train Loss: 15160.991603 - Val Loss: 1090.451202\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1090.451202)\n",
      "Epoch 640/2000 - Train Loss: 15132.850558 - Val Loss: 1086.367767\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1086.367767)\n",
      "Epoch 641/2000 - Train Loss: 15119.559788 - Val Loss: 1081.351359\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1081.351359)\n",
      "Epoch 642/2000 - Train Loss: 15066.655601 - Val Loss: 1080.911346\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1080.911346)\n",
      "Epoch 643/2000 - Train Loss: 15087.354113 - Val Loss: 1071.398142\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1071.398142)\n",
      "Epoch 644/2000 - Train Loss: 15066.132584 - Val Loss: 1067.213308\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1067.213308)\n",
      "Epoch 645/2000 - Train Loss: 15010.351729 - Val Loss: 1065.080566\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1065.080566)\n",
      "Epoch 646/2000 - Train Loss: 14946.332600 - Val Loss: 1060.116994\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1060.116994)\n",
      "Epoch 647/2000 - Train Loss: 14929.744601 - Val Loss: 1055.896271\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1055.896271)\n",
      "Epoch 648/2000 - Train Loss: 14912.269443 - Val Loss: 1051.569244\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1051.569244)\n",
      "Epoch 649/2000 - Train Loss: 14872.445818 - Val Loss: 1045.159963\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1045.159963)\n",
      "Epoch 650/2000 - Train Loss: 14906.963031 - Val Loss: 1040.511098\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1040.511098)\n",
      "Epoch 651/2000 - Train Loss: 14882.092011 - Val Loss: 1039.501933\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1039.501933)\n",
      "Epoch 652/2000 - Train Loss: 14809.000071 - Val Loss: 1035.437510\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1035.437510)\n",
      "Epoch 653/2000 - Train Loss: 14888.415452 - Val Loss: 1029.319458\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1029.319458)\n",
      "Epoch 654/2000 - Train Loss: 14797.864831 - Val Loss: 1023.831940\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1023.831940)\n",
      "Epoch 655/2000 - Train Loss: 14740.355593 - Val Loss: 1022.250458\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1022.250458)\n",
      "Epoch 656/2000 - Train Loss: 14697.161427 - Val Loss: 1013.547882\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1013.547882)\n",
      "Epoch 657/2000 - Train Loss: 14723.548480 - Val Loss: 1009.312714\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1009.312714)\n",
      "Epoch 658/2000 - Train Loss: 14743.898902 - Val Loss: 1007.143911\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1007.143911)\n",
      "Epoch 659/2000 - Train Loss: 14720.379406 - Val Loss: 1002.421254\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 1002.421254)\n",
      "Epoch 660/2000 - Train Loss: 14635.141723 - Val Loss: 997.422882\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 997.422882)\n",
      "Epoch 661/2000 - Train Loss: 14596.231795 - Val Loss: 990.199402\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 990.199402)\n",
      "Epoch 662/2000 - Train Loss: 14587.303865 - Val Loss: 990.347626\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 663/2000 - Train Loss: 14515.540470 - Val Loss: 987.105723\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 987.105723)\n",
      "Epoch 664/2000 - Train Loss: 14577.876917 - Val Loss: 982.276744\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 982.276744)\n",
      "Epoch 665/2000 - Train Loss: 14487.629624 - Val Loss: 976.210622\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 976.210622)\n",
      "Epoch 666/2000 - Train Loss: 14448.658898 - Val Loss: 972.446737\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 972.446737)\n",
      "Epoch 667/2000 - Train Loss: 14488.086203 - Val Loss: 968.902293\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 968.902293)\n",
      "Epoch 668/2000 - Train Loss: 14446.504495 - Val Loss: 965.184428\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 965.184428)\n",
      "Epoch 669/2000 - Train Loss: 14418.356301 - Val Loss: 961.062449\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 961.062449)\n",
      "Epoch 670/2000 - Train Loss: 14401.470245 - Val Loss: 957.991079\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 957.991079)\n",
      "Epoch 671/2000 - Train Loss: 14372.967203 - Val Loss: 953.264598\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 953.264598)\n",
      "Epoch 672/2000 - Train Loss: 14356.504349 - Val Loss: 948.015198\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 948.015198)\n",
      "Epoch 673/2000 - Train Loss: 14264.359915 - Val Loss: 945.576996\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 945.576996)\n",
      "Epoch 674/2000 - Train Loss: 14312.029812 - Val Loss: 939.956309\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 939.956309)\n",
      "Epoch 675/2000 - Train Loss: 14268.874195 - Val Loss: 937.080475\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 937.080475)\n",
      "Epoch 676/2000 - Train Loss: 14220.066662 - Val Loss: 932.232656\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 932.232656)\n",
      "Epoch 677/2000 - Train Loss: 14200.829269 - Val Loss: 926.763489\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 926.763489)\n",
      "Epoch 678/2000 - Train Loss: 14110.687464 - Val Loss: 923.323720\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 923.323720)\n",
      "Epoch 679/2000 - Train Loss: 14152.408717 - Val Loss: 921.889699\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 921.889699)\n",
      "Epoch 680/2000 - Train Loss: 14186.210781 - Val Loss: 916.570414\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 916.570414)\n",
      "Epoch 681/2000 - Train Loss: 14173.083160 - Val Loss: 913.237732\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 913.237732)\n",
      "Epoch 682/2000 - Train Loss: 14098.906957 - Val Loss: 909.184753\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 909.184753)\n",
      "Epoch 683/2000 - Train Loss: 14062.835857 - Val Loss: 906.068502\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 906.068502)\n",
      "Epoch 684/2000 - Train Loss: 14030.955790 - Val Loss: 900.671163\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 900.671163)\n",
      "Epoch 685/2000 - Train Loss: 14037.090892 - Val Loss: 899.314646\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 899.314646)\n",
      "Epoch 686/2000 - Train Loss: 14053.710968 - Val Loss: 894.486094\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 894.486094)\n",
      "Epoch 687/2000 - Train Loss: 14003.172484 - Val Loss: 891.278666\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 891.278666)\n",
      "Epoch 688/2000 - Train Loss: 13906.240045 - Val Loss: 889.107646\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 889.107646)\n",
      "Epoch 689/2000 - Train Loss: 13883.934791 - Val Loss: 883.893860\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 883.893860)\n",
      "Epoch 690/2000 - Train Loss: 13881.247329 - Val Loss: 880.585663\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 880.585663)\n",
      "Epoch 691/2000 - Train Loss: 13901.166715 - Val Loss: 875.966054\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 875.966054)\n",
      "Epoch 692/2000 - Train Loss: 13870.710533 - Val Loss: 873.378265\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 873.378265)\n",
      "Epoch 693/2000 - Train Loss: 13844.822692 - Val Loss: 870.822388\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 870.822388)\n",
      "Epoch 694/2000 - Train Loss: 13815.675968 - Val Loss: 866.307058\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 866.307058)\n",
      "Epoch 695/2000 - Train Loss: 13765.194035 - Val Loss: 863.838409\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 863.838409)\n",
      "Epoch 696/2000 - Train Loss: 13742.705011 - Val Loss: 861.586660\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 861.586660)\n",
      "Epoch 697/2000 - Train Loss: 13739.850795 - Val Loss: 858.699382\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 858.699382)\n",
      "Epoch 698/2000 - Train Loss: 13655.928848 - Val Loss: 853.890066\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 853.890066)\n",
      "Epoch 699/2000 - Train Loss: 13712.233922 - Val Loss: 849.040070\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 849.040070)\n",
      "Epoch 700/2000 - Train Loss: 13724.764792 - Val Loss: 847.284047\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 847.284047)\n",
      "Epoch 701/2000 - Train Loss: 13640.443071 - Val Loss: 843.800639\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 843.800639)\n",
      "Epoch 702/2000 - Train Loss: 13607.493671 - Val Loss: 840.106405\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 840.106405)\n",
      "Epoch 703/2000 - Train Loss: 13592.917036 - Val Loss: 838.313283\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 838.313283)\n",
      "Epoch 704/2000 - Train Loss: 13519.032250 - Val Loss: 834.197225\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 834.197225)\n",
      "Epoch 705/2000 - Train Loss: 13512.299528 - Val Loss: 831.436605\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 831.436605)\n",
      "Epoch 706/2000 - Train Loss: 13501.795245 - Val Loss: 827.200572\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 827.200572)\n",
      "Epoch 707/2000 - Train Loss: 13426.046179 - Val Loss: 825.216726\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 825.216726)\n",
      "Epoch 708/2000 - Train Loss: 13433.191945 - Val Loss: 819.908569\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 819.908569)\n",
      "Epoch 709/2000 - Train Loss: 13376.365721 - Val Loss: 816.766886\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 816.766886)\n",
      "Epoch 710/2000 - Train Loss: 13397.780765 - Val Loss: 814.289846\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 814.289846)\n",
      "Epoch 711/2000 - Train Loss: 13390.610981 - Val Loss: 812.216390\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 812.216390)\n",
      "Epoch 712/2000 - Train Loss: 13287.019753 - Val Loss: 808.751302\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 808.751302)\n",
      "Epoch 713/2000 - Train Loss: 13287.298222 - Val Loss: 807.400065\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 807.400065)\n",
      "Epoch 714/2000 - Train Loss: 13352.179407 - Val Loss: 805.337077\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 805.337077)\n",
      "Epoch 715/2000 - Train Loss: 13243.379237 - Val Loss: 802.052968\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 802.052968)\n",
      "Epoch 716/2000 - Train Loss: 13238.984980 - Val Loss: 797.634501\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 797.634501)\n",
      "Epoch 717/2000 - Train Loss: 13256.817793 - Val Loss: 794.933248\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 794.933248)\n",
      "Epoch 718/2000 - Train Loss: 13246.962099 - Val Loss: 791.308268\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 791.308268)\n",
      "Epoch 719/2000 - Train Loss: 13184.743381 - Val Loss: 790.611664\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 790.611664)\n",
      "Epoch 720/2000 - Train Loss: 13105.605945 - Val Loss: 787.599965\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 787.599965)\n",
      "Epoch 721/2000 - Train Loss: 13111.102449 - Val Loss: 783.294993\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 783.294993)\n",
      "Epoch 722/2000 - Train Loss: 13118.566320 - Val Loss: 779.985199\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 779.985199)\n",
      "Epoch 723/2000 - Train Loss: 13094.457341 - Val Loss: 777.261383\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 777.261383)\n",
      "Epoch 724/2000 - Train Loss: 13058.096688 - Val Loss: 773.519104\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 773.519104)\n",
      "Epoch 725/2000 - Train Loss: 13070.863112 - Val Loss: 772.124135\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 772.124135)\n",
      "Epoch 726/2000 - Train Loss: 12998.336481 - Val Loss: 768.659149\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 768.659149)\n",
      "Epoch 727/2000 - Train Loss: 13010.893741 - Val Loss: 767.168660\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 767.168660)\n",
      "Epoch 728/2000 - Train Loss: 12958.326133 - Val Loss: 765.035980\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 765.035980)\n",
      "Epoch 729/2000 - Train Loss: 12923.203138 - Val Loss: 764.092916\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 764.092916)\n",
      "Epoch 730/2000 - Train Loss: 12929.844794 - Val Loss: 760.788401\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 760.788401)\n",
      "Epoch 731/2000 - Train Loss: 12904.169771 - Val Loss: 759.270925\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 759.270925)\n",
      "Epoch 732/2000 - Train Loss: 12882.355855 - Val Loss: 756.520874\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 756.520874)\n",
      "Epoch 733/2000 - Train Loss: 12835.636313 - Val Loss: 752.751638\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 752.751638)\n",
      "Epoch 734/2000 - Train Loss: 12799.588866 - Val Loss: 752.212362\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 752.212362)\n",
      "Epoch 735/2000 - Train Loss: 12859.821179 - Val Loss: 750.405436\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 750.405436)\n",
      "Epoch 736/2000 - Train Loss: 12758.031727 - Val Loss: 747.001628\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 747.001628)\n",
      "Epoch 737/2000 - Train Loss: 12801.178065 - Val Loss: 745.269409\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 745.269409)\n",
      "Epoch 738/2000 - Train Loss: 12732.742664 - Val Loss: 741.674856\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 741.674856)\n",
      "Epoch 739/2000 - Train Loss: 12766.832448 - Val Loss: 740.125407\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 740.125407)\n",
      "Epoch 740/2000 - Train Loss: 12643.055825 - Val Loss: 738.379598\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 738.379598)\n",
      "Epoch 741/2000 - Train Loss: 12679.696737 - Val Loss: 735.930623\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 735.930623)\n",
      "Epoch 742/2000 - Train Loss: 12618.812768 - Val Loss: 732.824615\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 732.824615)\n",
      "Epoch 743/2000 - Train Loss: 12638.312403 - Val Loss: 730.775757\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 730.775757)\n",
      "Epoch 744/2000 - Train Loss: 12579.005436 - Val Loss: 726.984853\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 726.984853)\n",
      "Epoch 745/2000 - Train Loss: 12531.353013 - Val Loss: 726.841176\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 726.841176)\n",
      "Epoch 746/2000 - Train Loss: 12505.146009 - Val Loss: 725.582408\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 725.582408)\n",
      "Epoch 747/2000 - Train Loss: 12548.406962 - Val Loss: 722.835144\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 722.835144)\n",
      "Epoch 748/2000 - Train Loss: 12513.103352 - Val Loss: 720.136770\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 720.136770)\n",
      "Epoch 749/2000 - Train Loss: 12404.327296 - Val Loss: 717.766836\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 717.766836)\n",
      "Epoch 750/2000 - Train Loss: 12476.338347 - Val Loss: 715.470337\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 715.470337)\n",
      "Epoch 751/2000 - Train Loss: 12469.491662 - Val Loss: 715.343068\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 715.343068)\n",
      "Epoch 752/2000 - Train Loss: 12403.190213 - Val Loss: 712.018056\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 712.018056)\n",
      "Epoch 753/2000 - Train Loss: 12378.212745 - Val Loss: 709.977529\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 709.977529)\n",
      "Epoch 754/2000 - Train Loss: 12330.986984 - Val Loss: 708.616038\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 708.616038)\n",
      "Epoch 755/2000 - Train Loss: 12343.058533 - Val Loss: 706.637990\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 706.637990)\n",
      "Epoch 756/2000 - Train Loss: 12321.880322 - Val Loss: 705.090515\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 705.090515)\n",
      "Epoch 757/2000 - Train Loss: 12307.697929 - Val Loss: 703.893778\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 703.893778)\n",
      "Epoch 758/2000 - Train Loss: 12287.789766 - Val Loss: 700.331635\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 700.331635)\n",
      "Epoch 759/2000 - Train Loss: 12197.754937 - Val Loss: 700.926697\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 760/2000 - Train Loss: 12226.969632 - Val Loss: 697.609965\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 697.609965)\n",
      "Epoch 761/2000 - Train Loss: 12144.473423 - Val Loss: 696.456167\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 696.456167)\n",
      "Epoch 762/2000 - Train Loss: 12197.805540 - Val Loss: 695.784393\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 695.784393)\n",
      "Epoch 763/2000 - Train Loss: 12201.784127 - Val Loss: 693.627340\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 693.627340)\n",
      "Epoch 764/2000 - Train Loss: 12154.812506 - Val Loss: 694.203145\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 765/2000 - Train Loss: 12109.626308 - Val Loss: 690.332102\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 690.332102)\n",
      "Epoch 766/2000 - Train Loss: 12060.321604 - Val Loss: 690.848378\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 767/2000 - Train Loss: 12156.772925 - Val Loss: 688.019012\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 688.019012)\n",
      "Epoch 768/2000 - Train Loss: 12103.890280 - Val Loss: 686.289764\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 686.289764)\n",
      "Epoch 769/2000 - Train Loss: 12011.018619 - Val Loss: 685.282491\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 685.282491)\n",
      "Epoch 770/2000 - Train Loss: 11981.430634 - Val Loss: 683.087931\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 683.087931)\n",
      "Epoch 771/2000 - Train Loss: 11996.061646 - Val Loss: 681.736898\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 681.736898)\n",
      "Epoch 772/2000 - Train Loss: 11955.289578 - Val Loss: 680.876485\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 680.876485)\n",
      "Epoch 773/2000 - Train Loss: 11956.795022 - Val Loss: 679.733083\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 679.733083)\n",
      "Epoch 774/2000 - Train Loss: 11929.778924 - Val Loss: 679.253225\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 679.253225)\n",
      "Epoch 775/2000 - Train Loss: 11898.383014 - Val Loss: 678.158132\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 678.158132)\n",
      "Epoch 776/2000 - Train Loss: 11926.246280 - Val Loss: 675.112274\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 675.112274)\n",
      "Epoch 777/2000 - Train Loss: 11859.043037 - Val Loss: 675.117584\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 778/2000 - Train Loss: 11814.071229 - Val Loss: 673.298045\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 673.298045)\n",
      "Epoch 779/2000 - Train Loss: 11847.785036 - Val Loss: 672.293722\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 672.293722)\n",
      "Epoch 780/2000 - Train Loss: 11853.470296 - Val Loss: 672.069346\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 672.069346)\n",
      "Epoch 781/2000 - Train Loss: 11740.148474 - Val Loss: 671.272451\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 671.272451)\n",
      "Epoch 782/2000 - Train Loss: 11734.292954 - Val Loss: 670.371490\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 670.371490)\n",
      "Epoch 783/2000 - Train Loss: 11650.064964 - Val Loss: 671.219055\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 784/2000 - Train Loss: 11730.387402 - Val Loss: 668.889282\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 668.889282)\n",
      "Epoch 785/2000 - Train Loss: 11665.159515 - Val Loss: 666.506785\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 666.506785)\n",
      "Epoch 786/2000 - Train Loss: 11650.518830 - Val Loss: 667.508331\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 787/2000 - Train Loss: 11599.551095 - Val Loss: 665.538920\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 665.538920)\n",
      "Epoch 788/2000 - Train Loss: 11644.845518 - Val Loss: 665.221313\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 665.221313)\n",
      "Epoch 789/2000 - Train Loss: 11642.587347 - Val Loss: 664.251231\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 664.251231)\n",
      "Epoch 790/2000 - Train Loss: 11531.088196 - Val Loss: 665.236694\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 791/2000 - Train Loss: 11493.868813 - Val Loss: 663.032990\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 663.032990)\n",
      "Epoch 792/2000 - Train Loss: 11529.835303 - Val Loss: 663.098643\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 793/2000 - Train Loss: 11534.450235 - Val Loss: 661.835459\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 661.835459)\n",
      "Epoch 794/2000 - Train Loss: 11451.181856 - Val Loss: 659.766652\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 659.766652)\n",
      "Epoch 795/2000 - Train Loss: 11451.475171 - Val Loss: 660.842183\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 796/2000 - Train Loss: 11419.626717 - Val Loss: 661.111389\n",
      "  > Val loss 개선 없음. (Patience: 2/10)\n",
      "Epoch 797/2000 - Train Loss: 11435.428948 - Val Loss: 659.568665\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 659.568665)\n",
      "Epoch 798/2000 - Train Loss: 11411.652730 - Val Loss: 657.727702\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 657.727702)\n",
      "Epoch 799/2000 - Train Loss: 11466.379063 - Val Loss: 659.201864\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 800/2000 - Train Loss: 11375.656278 - Val Loss: 657.987610\n",
      "  > Val loss 개선 없음. (Patience: 2/10)\n",
      "Epoch 801/2000 - Train Loss: 11253.600599 - Val Loss: 657.242421\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 657.242421)\n",
      "Epoch 802/2000 - Train Loss: 11329.071030 - Val Loss: 658.096975\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 803/2000 - Train Loss: 11322.822298 - Val Loss: 657.246552\n",
      "  > Val loss 개선 없음. (Patience: 2/10)\n",
      "Epoch 804/2000 - Train Loss: 11300.624677 - Val Loss: 656.392873\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 656.392873)\n",
      "Epoch 805/2000 - Train Loss: 11293.065229 - Val Loss: 655.360514\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 655.360514)\n",
      "Epoch 806/2000 - Train Loss: 11188.640117 - Val Loss: 655.657918\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 807/2000 - Train Loss: 11237.526232 - Val Loss: 655.523427\n",
      "  > Val loss 개선 없음. (Patience: 2/10)\n",
      "Epoch 808/2000 - Train Loss: 11182.781583 - Val Loss: 655.692861\n",
      "  > Val loss 개선 없음. (Patience: 3/10)\n",
      "Epoch 809/2000 - Train Loss: 11122.867358 - Val Loss: 655.510691\n",
      "  > Val loss 개선 없음. (Patience: 4/10)\n",
      "Epoch 810/2000 - Train Loss: 11144.405418 - Val Loss: 654.841909\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 654.841909)\n",
      "Epoch 811/2000 - Train Loss: 11093.478668 - Val Loss: 655.155640\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 812/2000 - Train Loss: 11120.524705 - Val Loss: 654.444906\n",
      "  > Val loss 개선. 베스트 모델 저장. (Loss: 654.444906)\n",
      "Epoch 813/2000 - Train Loss: 11065.475097 - Val Loss: 655.525187\n",
      "  > Val loss 개선 없음. (Patience: 1/10)\n",
      "Epoch 814/2000 - Train Loss: 11124.277824 - Val Loss: 655.714467\n",
      "  > Val loss 개선 없음. (Patience: 2/10)\n",
      "Epoch 815/2000 - Train Loss: 11008.770571 - Val Loss: 654.650096\n",
      "  > Val loss 개선 없음. (Patience: 3/10)\n",
      "Epoch 816/2000 - Train Loss: 11013.562998 - Val Loss: 655.386810\n",
      "  > Val loss 개선 없음. (Patience: 4/10)\n",
      "Epoch 817/2000 - Train Loss: 11022.343995 - Val Loss: 655.610118\n",
      "  > Val loss 개선 없음. (Patience: 5/10)\n",
      "Epoch 818/2000 - Train Loss: 10962.525425 - Val Loss: 655.322032\n",
      "  > Val loss 개선 없음. (Patience: 6/10)\n",
      "Epoch 819/2000 - Train Loss: 10984.525489 - Val Loss: 655.169291\n",
      "  > Val loss 개선 없음. (Patience: 7/10)\n",
      "Epoch 820/2000 - Train Loss: 10944.451250 - Val Loss: 655.073883\n",
      "  > Val loss 개선 없음. (Patience: 8/10)\n",
      "Epoch 821/2000 - Train Loss: 10904.036527 - Val Loss: 655.133565\n",
      "  > Val loss 개선 없음. (Patience: 9/10)\n",
      "Epoch 822/2000 - Train Loss: 10930.666046 - Val Loss: 657.280416\n",
      "  > Val loss 개선 없음. (Patience: 10/10)\n",
      "--- 조기 종료 (Epoch 822) ---\n",
      "--- 훈련 종료 ---\n",
      "가장 좋았던 모델(Val Loss: 654.444906)을 로드했습니다.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2000\n",
    "PATIENCE = 10\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "BEST_MODEL_PATH = \"./result/best_transfer_model.pth\"\n",
    "\n",
    "model_instance.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_instance.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch[\"past_values\"].to(device), batch[\"future_values\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_instance(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # evaluation\n",
    "    model_instance.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = val_batch[\"past_values\"].to(device), val_batch[\"future_values\"].to(device)\n",
    "            \n",
    "            val_outputs = model_instance(X_val_batch)\n",
    "            val_loss = loss_fn(val_outputs, y_val_batch)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Keras의 restore_best_weights=True (모델 저장)\n",
    "        torch.save(model_instance.state_dict(), BEST_MODEL_PATH) \n",
    "        print(f\"  > Val loss 개선. 베스트 모델 저장. (Loss: {best_val_loss:.6f})\")\n",
    "        patience_counter = 0 # 인내심 초기화\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  > Val loss 개선 없음. (Patience: {patience_counter}/{PATIENCE})\")\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"--- 조기 종료 (Epoch {epoch+1}) ---\")\n",
    "        break\n",
    "\n",
    "print(\"--- 훈련 종료 ---\")\n",
    "# Keras의 restore_best_weights (베스트 모델 불러오기)\n",
    "model_instance.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "print(f\"가장 좋았던 모델(Val Loss: {best_val_loss:.6f})을 로드했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0f6c1775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 24, 1])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_instance(batch[\"past_values\"].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ab49e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    preds.append(model_instance(batch[\"past_values\"].to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0457fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[63.7900, 63.6200, 64.2500,  ..., 61.8000, 62.0900, 62.1600],\n",
       "        [61.2900, 62.5900, 61.7500,  ..., 61.1500, 61.0500, 61.3800],\n",
       "        [60.7600, 60.8100, 61.4700,  ..., 59.1000, 59.0200, 59.1000],\n",
       "        ...,\n",
       "        [69.9900, 69.9200, 70.2000,  ..., 70.5500, 70.9400, 70.8000],\n",
       "        [70.9300, 70.8000, 71.2400,  ..., 71.4700, 71.5500, 71.3000],\n",
       "        [71.0000, 70.8500, 71.0100,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "88a70088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.8484)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(abs(torch.tensor(test_y) - torch.concat(preds, axis = 0).squeeze().data.to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "443f3cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27.0861)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.mean((torch.tensor(test_y) - torch.concat(preds, axis = 0).squeeze().data.to('cpu'))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "50205e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(75.789345)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94ebff",
   "metadata": {},
   "source": [
    "> `27.0861, 21.8484`가 나옴. 처음보다야 낫지만, 썩 좋지는 않네."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddab20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
