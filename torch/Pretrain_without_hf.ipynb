{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad4991e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import PatchTSTForPrediction\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5fbee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "\n",
    "output_dir = \"saved_models\"\n",
    "log_dir = os.path.join('logstf', data)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "loss_name = \"MASE\"\n",
    "\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 400\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7961096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "np.random.seed(2)\n",
    "random_indices1 = np.random.choice(pd.read_csv(\"../data/M4_train.csv\").iloc[:, (1):].index,\n",
    "                                   size=target_X.shape[0] * 20, replace=True)\n",
    "\n",
    "X_data = pd.read_csv(\"../data/M4_train.csv\").iloc[:, 1 + (24 * 0):].loc[random_indices1].values.astype(np.float32)\n",
    "y_data = pd.read_csv(\"../data/M4_test.csv\").iloc[:, 1:].loc[random_indices1].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91eb7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bootstrap\n",
    "np.random.seed(42)\n",
    "select = np.random.choice(len(X_data), size=len(X_data), replace=True)\n",
    "X_bootstrap = X_data[select]\n",
    "y_bootstrap = y_data[select]\n",
    "\n",
    "val_split_index = int(len(X_bootstrap) * 0.8)\n",
    "\n",
    "def to_tensor_and_reshape(array):\n",
    "    result = torch.tensor(array)\n",
    "    result = result.reshape(-1, result.shape[1], 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "X_train, X_valid = to_tensor_and_reshape(X_bootstrap[:val_split_index]), to_tensor_and_reshape(X_bootstrap[val_split_index:])\n",
    "y_train, y_valid = to_tensor_and_reshape(y_bootstrap[:val_split_index]), to_tensor_and_reshape(y_bootstrap[val_split_index:])\n",
    "\n",
    "## setting dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 256, shuffle = True, num_workers = 16)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 256, num_workers = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "924c3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = PatchTSTForPrediction.from_pretrained(os.path.join(output_dir, \"PatchTSTBackbone\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "444eeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom loss function\n",
    "def SMAPE(yhat, y):\n",
    "    numerator = 100*torch.abs(y - yhat)\n",
    "    denominator = (torch.abs(y) + torch.abs(yhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "def MAPE(yhat, y):\n",
    "    return torch.mean(100*torch.abs((y - yhat) / y))\n",
    "\n",
    "class MASE(torch.nn.Module):\n",
    "    def __init__(self, training_data, period = 1):\n",
    "        super().__init__()\n",
    "        self.scale = torch.mean(torch.abs(torch.tensor(training_data[:, period:] - training_data[:, :-period])))    ## 모든 훈련 데이터에 대한 평균 스케일 계산\n",
    "    \n",
    "    def forward(self, yhat, y):\n",
    "        error = torch.abs(y - yhat)\n",
    "        return torch.mean(error / self.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400 | Train Loss: 2.575711\t\t Val Loss: 2.446912\n",
      "Epoch 2/400 | Train Loss: 2.001803\t\t Val Loss: 1.757823\n",
      "Epoch 3/400 | Train Loss: 1.911106\t\t Val Loss: 1.880399\n",
      "Epoch 4/400 | Train Loss: 1.855843\t\t Val Loss: 1.679141\n",
      "Epoch 5/400 | Train Loss: 1.786365\t\t Val Loss: 1.622896\n",
      "Epoch 6/400 | Train Loss: 1.737312\t\t Val Loss: 1.597142\n",
      "Epoch 7/400 | Train Loss: 1.698749\t\t Val Loss: 1.600184\n",
      "Epoch 8/400 | Train Loss: 1.653931\t\t Val Loss: 1.535675\n",
      "Epoch 9/400 | Train Loss: 1.605308\t\t Val Loss: 1.560574\n",
      "Epoch 10/400 | Train Loss: 1.574386\t\t Val Loss: 1.536518\n",
      "Epoch 11/400 | Train Loss: 1.549072\t\t Val Loss: 1.624128\n",
      "Epoch 12/400 | Train Loss: 1.508551\t\t Val Loss: 1.454669\n",
      "Epoch 13/400 | Train Loss: 1.479817\t\t Val Loss: 1.534351\n",
      "Epoch 14/400 | Train Loss: 1.444158\t\t Val Loss: 1.466596\n",
      "Epoch 15/400 | Train Loss: 1.408554\t\t Val Loss: 1.524756\n",
      "Epoch 16/400 | Train Loss: 1.410807\t\t Val Loss: 1.380227\n",
      "Epoch 17/400 | Train Loss: 1.363850\t\t Val Loss: 1.451997\n",
      "Epoch 18/400 | Train Loss: 1.351522\t\t Val Loss: 1.361436\n",
      "Epoch 19/400 | Train Loss: 1.328756\t\t Val Loss: 1.476350\n",
      "Epoch 20/400 | Train Loss: 1.314891\t\t Val Loss: 1.459417\n",
      "Epoch 21/400 | Train Loss: 1.261277\t\t Val Loss: 1.352360\n",
      "Epoch 22/400 | Train Loss: 1.254977\t\t Val Loss: 1.389425\n",
      "Epoch 23/400 | Train Loss: 1.248301\t\t Val Loss: 1.291024\n",
      "Epoch 24/400 | Train Loss: 1.211739\t\t Val Loss: 1.314643\n",
      "Epoch 25/400 | Train Loss: 1.191229\t\t Val Loss: 1.295783\n",
      "Epoch 26/400 | Train Loss: 1.173817\t\t Val Loss: 1.498305\n",
      "Epoch 27/400 | Train Loss: 1.170651\t\t Val Loss: 1.269503\n",
      "Epoch 28/400 | Train Loss: 1.149377\t\t Val Loss: 1.300602\n",
      "Epoch 29/400 | Train Loss: 1.141748\t\t Val Loss: 1.276875\n",
      "Epoch 30/400 | Train Loss: 1.110958\t\t Val Loss: 1.352559\n",
      "Epoch 31/400 | Train Loss: 1.109661\t\t Val Loss: 1.200977\n"
     ]
    }
   ],
   "source": [
    "if loss_name == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    lr = learning_rate\n",
    "else:\n",
    "    lr = learning_rate*2\n",
    "    if loss_name == \"mae\":\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "    elif loss_name == \"SMAPE\":\n",
    "        loss_fn = SMAPE\n",
    "    elif loss_name == \"mape\":\n",
    "        loss_fn = MAPE\n",
    "    elif loss_name == \"MASE\":\n",
    "        loss_fn = MASE(y_data, 1)\n",
    "    else:\n",
    "        raise Exception(\"Your loss name is not valid.\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(backbone_model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_train_epochs)\n",
    "log_data = []\n",
    "\n",
    "## early stopping\n",
    "PATIENCE = 15\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "for epoc in range(num_train_epochs):\n",
    "    backbone_model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = backbone_model(X).prediction_outputs\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(backbone_model.parameters(), max_norm = 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()*X.shape[0]\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
    "\n",
    "    backbone_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yys = []\n",
    "        yyhats = []\n",
    "\n",
    "        for XX, yy in test_dataloader:\n",
    "            XX = XX.to(device)\n",
    "            yys.append(yy.to(device))\n",
    "            yyhats.append(backbone_model(XX).prediction_outputs)\n",
    "\n",
    "        yyhat = torch.concat(yyhats)\n",
    "        yy = torch.concat(yys)\n",
    "\n",
    "        val_loss = loss_fn(yyhat, yy)\n",
    "\n",
    "    print(f\"Epoch {epoc+1}/{num_train_epochs} | Train Loss: {avg_train_loss:.6f}\\t\\t Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    log_data.append({\"epoch\": epoc, \"loss\": avg_train_loss, \"eval_loss\": val_loss.item()})\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(backbone_model.state_dict(), os.path.join(output_dir, f\"model_{loss_name}_{1}.pth\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        break\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d282dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save log\n",
    "pd.DataFrame(log_data).to_csv(os.path.join(log_dir, f\"pretrain_{loss_name}_model{1}.csv\"))\n",
    "\n",
    "## load best model\n",
    "backbone_model.load_state_dict(torch.load(os.path.join(output_dir, f\"model_{loss_name}_{1}.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b045298",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhats = []\n",
    "yys = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for XX, yy in test_dataloader:\n",
    "        XX = XX.to(device)\n",
    "        yys.append(yy.to(device))\n",
    "        yyhats.append(backbone_model(XX).prediction_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fce1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhat, yy = torch.concat(yyhats).squeeze(), torch.concat(yys).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c926be7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE: 635.1454467773438\n",
      "test MAE: 219.265869140625\n",
      "test SMAPE: 5.3464579582214355\n"
     ]
    }
   ],
   "source": [
    "mseLoss = torch.nn.MSELoss()\n",
    "maeLoss = torch.nn.L1Loss()\n",
    "\n",
    "def smape(yy, yyhat):\n",
    "    numerator = 100*abs(yy - yyhat)\n",
    "    denominator = (abs(yy) + abs(yyhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "print(f\"test RMSE: {torch.sqrt(mseLoss(yyhat, yy))}\")\n",
    "print(f\"test MAE: {maeLoss(yyhat, yy)}\")\n",
    "print(f\"test SMAPE: {smape(yy, yyhat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhat = pd.DataFrame(yyhat.to(\"cpu\"))\n",
    "yyhat.columns = [f\"{i}A\" for i in range(yyhat.shape[1])]\n",
    "yy = pd.DataFrame(yy.to(\"cpu\"))\n",
    "yy.columns = [f\"{i}B\" for i in range(yyhat.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result = pd.concat([yyhat, yy], axis = 1).sort_index(axis = 1)\n",
    "val_result.columns = [f\"prediction_{(i+1)//2}\" if i%2 == 1 else f\"ground_truth_{(i+1)//2}\" for i in range(1, val_result.shape[1]+1)]\n",
    "val_result.to_csv(os.path.join(log_dir, f\"prediction_val_results_{loss_name}_model{1}.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8e373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>ground_truth_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>ground_truth_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>ground_truth_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>ground_truth_4</th>\n",
       "      <th>prediction_5</th>\n",
       "      <th>ground_truth_5</th>\n",
       "      <th>...</th>\n",
       "      <th>prediction_20</th>\n",
       "      <th>ground_truth_20</th>\n",
       "      <th>prediction_21</th>\n",
       "      <th>ground_truth_21</th>\n",
       "      <th>prediction_22</th>\n",
       "      <th>ground_truth_22</th>\n",
       "      <th>prediction_23</th>\n",
       "      <th>ground_truth_23</th>\n",
       "      <th>prediction_24</th>\n",
       "      <th>ground_truth_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1764.744507</td>\n",
       "      <td>1745.800049</td>\n",
       "      <td>1810.076660</td>\n",
       "      <td>1869.199951</td>\n",
       "      <td>1817.699951</td>\n",
       "      <td>1854.199951</td>\n",
       "      <td>1791.267700</td>\n",
       "      <td>1863.800049</td>\n",
       "      <td>1827.923828</td>\n",
       "      <td>1853.800049</td>\n",
       "      <td>...</td>\n",
       "      <td>1797.349365</td>\n",
       "      <td>1833.199951</td>\n",
       "      <td>1779.291992</td>\n",
       "      <td>1851.099976</td>\n",
       "      <td>1775.888306</td>\n",
       "      <td>1854.900024</td>\n",
       "      <td>1769.098755</td>\n",
       "      <td>1854.199951</td>\n",
       "      <td>1791.199585</td>\n",
       "      <td>1870.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9977.685547</td>\n",
       "      <td>9980.000000</td>\n",
       "      <td>10281.222656</td>\n",
       "      <td>10070.000000</td>\n",
       "      <td>10415.597656</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10491.425781</td>\n",
       "      <td>10200.000000</td>\n",
       "      <td>10621.905273</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10064.826172</td>\n",
       "      <td>9940.000000</td>\n",
       "      <td>10103.845703</td>\n",
       "      <td>9960.000000</td>\n",
       "      <td>9935.583008</td>\n",
       "      <td>9950.000000</td>\n",
       "      <td>9991.794922</td>\n",
       "      <td>9970.000000</td>\n",
       "      <td>10191.219727</td>\n",
       "      <td>10050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1441.031860</td>\n",
       "      <td>1445.834961</td>\n",
       "      <td>1438.576416</td>\n",
       "      <td>1433.120972</td>\n",
       "      <td>1439.517456</td>\n",
       "      <td>1439.545044</td>\n",
       "      <td>1429.690430</td>\n",
       "      <td>1441.876953</td>\n",
       "      <td>1457.640625</td>\n",
       "      <td>1440.109009</td>\n",
       "      <td>...</td>\n",
       "      <td>1454.538086</td>\n",
       "      <td>1441.203003</td>\n",
       "      <td>1436.432129</td>\n",
       "      <td>1435.150024</td>\n",
       "      <td>1434.387817</td>\n",
       "      <td>1420.425049</td>\n",
       "      <td>1436.371948</td>\n",
       "      <td>1441.050049</td>\n",
       "      <td>1436.618286</td>\n",
       "      <td>1425.350952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6617.981445</td>\n",
       "      <td>6546.056641</td>\n",
       "      <td>6946.009277</td>\n",
       "      <td>6936.538574</td>\n",
       "      <td>6761.762695</td>\n",
       "      <td>6724.092285</td>\n",
       "      <td>6713.710938</td>\n",
       "      <td>6635.496582</td>\n",
       "      <td>6884.118652</td>\n",
       "      <td>6887.514160</td>\n",
       "      <td>...</td>\n",
       "      <td>6133.299805</td>\n",
       "      <td>6130.813477</td>\n",
       "      <td>6287.193359</td>\n",
       "      <td>6259.539551</td>\n",
       "      <td>6580.555176</td>\n",
       "      <td>6557.993164</td>\n",
       "      <td>6948.881836</td>\n",
       "      <td>6951.220703</td>\n",
       "      <td>7040.197754</td>\n",
       "      <td>7044.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4700.222168</td>\n",
       "      <td>4723.200195</td>\n",
       "      <td>5200.006836</td>\n",
       "      <td>5204.700195</td>\n",
       "      <td>5216.735352</td>\n",
       "      <td>5262.500000</td>\n",
       "      <td>5293.151855</td>\n",
       "      <td>5392.399902</td>\n",
       "      <td>5397.792969</td>\n",
       "      <td>5476.600098</td>\n",
       "      <td>...</td>\n",
       "      <td>5059.943359</td>\n",
       "      <td>5034.799805</td>\n",
       "      <td>5011.688477</td>\n",
       "      <td>5064.700195</td>\n",
       "      <td>5056.430176</td>\n",
       "      <td>5092.600098</td>\n",
       "      <td>5083.967285</td>\n",
       "      <td>5120.899902</td>\n",
       "      <td>5077.353516</td>\n",
       "      <td>5156.399902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>8784.845703</td>\n",
       "      <td>8830.000000</td>\n",
       "      <td>8848.226562</td>\n",
       "      <td>8800.000000</td>\n",
       "      <td>8803.655273</td>\n",
       "      <td>8780.000000</td>\n",
       "      <td>8695.550781</td>\n",
       "      <td>8710.000000</td>\n",
       "      <td>8765.865234</td>\n",
       "      <td>8790.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8852.857422</td>\n",
       "      <td>8810.000000</td>\n",
       "      <td>8764.726562</td>\n",
       "      <td>8690.000000</td>\n",
       "      <td>8791.627930</td>\n",
       "      <td>8710.000000</td>\n",
       "      <td>8825.441406</td>\n",
       "      <td>8810.000000</td>\n",
       "      <td>8794.834961</td>\n",
       "      <td>8810.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>955.064087</td>\n",
       "      <td>958.630005</td>\n",
       "      <td>1091.725464</td>\n",
       "      <td>1087.250000</td>\n",
       "      <td>1054.492676</td>\n",
       "      <td>1063.640015</td>\n",
       "      <td>1070.163818</td>\n",
       "      <td>1064.489990</td>\n",
       "      <td>1043.964966</td>\n",
       "      <td>1038.270020</td>\n",
       "      <td>...</td>\n",
       "      <td>1051.307373</td>\n",
       "      <td>1048.619995</td>\n",
       "      <td>1070.507568</td>\n",
       "      <td>1077.099976</td>\n",
       "      <td>1062.181030</td>\n",
       "      <td>1073.660034</td>\n",
       "      <td>1076.685791</td>\n",
       "      <td>1079.089966</td>\n",
       "      <td>1096.553223</td>\n",
       "      <td>1105.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>5602.021973</td>\n",
       "      <td>5577.244629</td>\n",
       "      <td>6176.615234</td>\n",
       "      <td>6268.951660</td>\n",
       "      <td>5921.100586</td>\n",
       "      <td>6390.646484</td>\n",
       "      <td>5889.652832</td>\n",
       "      <td>6173.075195</td>\n",
       "      <td>5650.695801</td>\n",
       "      <td>5789.339844</td>\n",
       "      <td>...</td>\n",
       "      <td>5242.411133</td>\n",
       "      <td>5299.357910</td>\n",
       "      <td>4735.445801</td>\n",
       "      <td>4838.526367</td>\n",
       "      <td>4700.993652</td>\n",
       "      <td>4506.627441</td>\n",
       "      <td>5949.955078</td>\n",
       "      <td>6024.740234</td>\n",
       "      <td>5968.816406</td>\n",
       "      <td>5739.643066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>6617.987793</td>\n",
       "      <td>6588.406250</td>\n",
       "      <td>6340.265625</td>\n",
       "      <td>6459.303711</td>\n",
       "      <td>6303.109375</td>\n",
       "      <td>6185.271484</td>\n",
       "      <td>6259.520508</td>\n",
       "      <td>6149.212402</td>\n",
       "      <td>6340.421387</td>\n",
       "      <td>6352.244629</td>\n",
       "      <td>...</td>\n",
       "      <td>6517.442871</td>\n",
       "      <td>6698.416016</td>\n",
       "      <td>6486.223145</td>\n",
       "      <td>6632.667480</td>\n",
       "      <td>6477.447266</td>\n",
       "      <td>6713.057617</td>\n",
       "      <td>6432.340820</td>\n",
       "      <td>6679.114746</td>\n",
       "      <td>6318.266602</td>\n",
       "      <td>6442.051758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>7897.890625</td>\n",
       "      <td>7986.000000</td>\n",
       "      <td>8383.297852</td>\n",
       "      <td>8291.000000</td>\n",
       "      <td>8381.000977</td>\n",
       "      <td>8141.000000</td>\n",
       "      <td>8262.447266</td>\n",
       "      <td>7544.000000</td>\n",
       "      <td>8460.873047</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8123.166016</td>\n",
       "      <td>7949.000000</td>\n",
       "      <td>8120.872070</td>\n",
       "      <td>8303.000000</td>\n",
       "      <td>8374.537109</td>\n",
       "      <td>8777.000000</td>\n",
       "      <td>8439.448242</td>\n",
       "      <td>8626.000000</td>\n",
       "      <td>8541.570312</td>\n",
       "      <td>9026.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2892 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prediction_1  ground_truth_1  prediction_2  ground_truth_2  \\\n",
       "0      1764.744507     1745.800049   1810.076660     1869.199951   \n",
       "1      9977.685547     9980.000000  10281.222656    10070.000000   \n",
       "2      1441.031860     1445.834961   1438.576416     1433.120972   \n",
       "3      6617.981445     6546.056641   6946.009277     6936.538574   \n",
       "4      4700.222168     4723.200195   5200.006836     5204.700195   \n",
       "...            ...             ...           ...             ...   \n",
       "2887   8784.845703     8830.000000   8848.226562     8800.000000   \n",
       "2888    955.064087      958.630005   1091.725464     1087.250000   \n",
       "2889   5602.021973     5577.244629   6176.615234     6268.951660   \n",
       "2890   6617.987793     6588.406250   6340.265625     6459.303711   \n",
       "2891   7897.890625     7986.000000   8383.297852     8291.000000   \n",
       "\n",
       "      prediction_3  ground_truth_3  prediction_4  ground_truth_4  \\\n",
       "0      1817.699951     1854.199951   1791.267700     1863.800049   \n",
       "1     10415.597656    10190.000000  10491.425781    10200.000000   \n",
       "2      1439.517456     1439.545044   1429.690430     1441.876953   \n",
       "3      6761.762695     6724.092285   6713.710938     6635.496582   \n",
       "4      5216.735352     5262.500000   5293.151855     5392.399902   \n",
       "...            ...             ...           ...             ...   \n",
       "2887   8803.655273     8780.000000   8695.550781     8710.000000   \n",
       "2888   1054.492676     1063.640015   1070.163818     1064.489990   \n",
       "2889   5921.100586     6390.646484   5889.652832     6173.075195   \n",
       "2890   6303.109375     6185.271484   6259.520508     6149.212402   \n",
       "2891   8381.000977     8141.000000   8262.447266     7544.000000   \n",
       "\n",
       "      prediction_5  ground_truth_5  ...  prediction_20  ground_truth_20  \\\n",
       "0      1827.923828     1853.800049  ...    1797.349365      1833.199951   \n",
       "1     10621.905273    10210.000000  ...   10064.826172      9940.000000   \n",
       "2      1457.640625     1440.109009  ...    1454.538086      1441.203003   \n",
       "3      6884.118652     6887.514160  ...    6133.299805      6130.813477   \n",
       "4      5397.792969     5476.600098  ...    5059.943359      5034.799805   \n",
       "...            ...             ...  ...            ...              ...   \n",
       "2887   8765.865234     8790.000000  ...    8852.857422      8810.000000   \n",
       "2888   1043.964966     1038.270020  ...    1051.307373      1048.619995   \n",
       "2889   5650.695801     5789.339844  ...    5242.411133      5299.357910   \n",
       "2890   6340.421387     6352.244629  ...    6517.442871      6698.416016   \n",
       "2891   8460.873047     8331.000000  ...    8123.166016      7949.000000   \n",
       "\n",
       "      prediction_21  ground_truth_21  prediction_22  ground_truth_22  \\\n",
       "0       1779.291992      1851.099976    1775.888306      1854.900024   \n",
       "1      10103.845703      9960.000000    9935.583008      9950.000000   \n",
       "2       1436.432129      1435.150024    1434.387817      1420.425049   \n",
       "3       6287.193359      6259.539551    6580.555176      6557.993164   \n",
       "4       5011.688477      5064.700195    5056.430176      5092.600098   \n",
       "...             ...              ...            ...              ...   \n",
       "2887    8764.726562      8690.000000    8791.627930      8710.000000   \n",
       "2888    1070.507568      1077.099976    1062.181030      1073.660034   \n",
       "2889    4735.445801      4838.526367    4700.993652      4506.627441   \n",
       "2890    6486.223145      6632.667480    6477.447266      6713.057617   \n",
       "2891    8120.872070      8303.000000    8374.537109      8777.000000   \n",
       "\n",
       "      prediction_23  ground_truth_23  prediction_24  ground_truth_24  \n",
       "0       1769.098755      1854.199951    1791.199585      1870.400024  \n",
       "1       9991.794922      9970.000000   10191.219727     10050.000000  \n",
       "2       1436.371948      1441.050049    1436.618286      1425.350952  \n",
       "3       6948.881836      6951.220703    7040.197754      7044.781250  \n",
       "4       5083.967285      5120.899902    5077.353516      5156.399902  \n",
       "...             ...              ...            ...              ...  \n",
       "2887    8825.441406      8810.000000    8794.834961      8810.000000  \n",
       "2888    1076.685791      1079.089966    1096.553223      1105.900024  \n",
       "2889    5949.955078      6024.740234    5968.816406      5739.643066  \n",
       "2890    6432.340820      6679.114746    6318.266602      6442.051758  \n",
       "2891    8439.448242      8626.000000    8541.570312      9026.000000  \n",
       "\n",
       "[2892 rows x 48 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "145ccf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1007"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7182a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
