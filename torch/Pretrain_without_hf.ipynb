{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4991e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 22:13:16.795728: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import PatchTSTForPrediction\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fbee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "\n",
    "output_dir = \"saved_models\"\n",
    "log_dir = os.path.join('logstf', data)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "loss_name = \"SMAPE\"\n",
    "\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 400\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7961096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "np.random.seed(2)\n",
    "random_indices1 = np.random.choice(pd.read_csv(\"../data/M4_train.csv\").iloc[:, (1):].index,\n",
    "                                   size=target_X.shape[0] * 20, replace=True)\n",
    "\n",
    "X_data = pd.read_csv(\"../data/M4_train.csv\").iloc[:, 1 + (24 * 0):].loc[random_indices1].values.astype(np.float32)\n",
    "y_data = pd.read_csv(\"../data/M4_test.csv\").iloc[:, 1:].loc[random_indices1].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91eb7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bootstrap\n",
    "np.random.seed(42)\n",
    "select = np.random.choice(len(X_data), size=len(X_data), replace=True)\n",
    "X_bootstrap = X_data[select]\n",
    "y_bootstrap = y_data[select]\n",
    "\n",
    "val_split_index = int(len(X_bootstrap) * 0.8)\n",
    "\n",
    "def to_tensor_and_reshape(array):\n",
    "    result = torch.tensor(array)\n",
    "    result = result.reshape(-1, result.shape[1], 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "X_train, X_valid = to_tensor_and_reshape(X_bootstrap[:val_split_index]), to_tensor_and_reshape(X_bootstrap[val_split_index:])\n",
    "y_train, y_valid = to_tensor_and_reshape(y_bootstrap[:val_split_index]), to_tensor_and_reshape(y_bootstrap[val_split_index:])\n",
    "\n",
    "## setting dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 256, shuffle = True, num_workers = 16)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 256, num_workers = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924c3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = PatchTSTForPrediction.from_pretrained(os.path.join(output_dir, \"PatchTSTBackbone\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444eeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom loss function\n",
    "def SMAPE(yhat, y):\n",
    "    numerator = 100*torch.abs(y - yhat)\n",
    "    denominator = (torch.abs(y) + torch.abs(yhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "def MAPE(yhat, y):\n",
    "    return torch.mean(100*torch.abs((y - yhat) / y))\n",
    "\n",
    "class MASE(torch.nn.Module):\n",
    "    def __init__(self, training_data, period = 1):\n",
    "        super().__init__()\n",
    "        ## 원본 코드 구현, 사실상 MAE와 동일, 잘못 짜여진 코드, 일단은 하던대로 할 것.\n",
    "        self.scale = torch.mean(torch.abs(torch.tensor(training_data[period:] - training_data[:-period])))\n",
    "    \n",
    "    def forward(self, yhat, y):\n",
    "        error = torch.abs(y - yhat)\n",
    "        return torch.mean(error) / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400 | Train Loss: 12.145416\t\t Val Loss: 11.327583\n",
      "Epoch 2/400 | Train Loss: 10.011508\t\t Val Loss: 9.162341\n",
      "Epoch 3/400 | Train Loss: 9.643101\t\t Val Loss: 9.140893\n",
      "Epoch 4/400 | Train Loss: 9.299493\t\t Val Loss: 8.922081\n",
      "Epoch 5/400 | Train Loss: 9.067393\t\t Val Loss: 8.389436\n",
      "Epoch 6/400 | Train Loss: 8.689944\t\t Val Loss: 8.493913\n",
      "Epoch 7/400 | Train Loss: 8.589314\t\t Val Loss: 9.540770\n",
      "Epoch 8/400 | Train Loss: 8.371845\t\t Val Loss: 8.119756\n",
      "Epoch 9/400 | Train Loss: 8.215777\t\t Val Loss: 8.044712\n",
      "Epoch 10/400 | Train Loss: 8.005516\t\t Val Loss: 8.354805\n",
      "Epoch 11/400 | Train Loss: 7.795427\t\t Val Loss: 7.873311\n",
      "Epoch 12/400 | Train Loss: 7.736048\t\t Val Loss: 7.944956\n",
      "Epoch 13/400 | Train Loss: 7.654992\t\t Val Loss: 7.823000\n",
      "Epoch 14/400 | Train Loss: 7.477790\t\t Val Loss: 8.021585\n",
      "Epoch 15/400 | Train Loss: 7.485159\t\t Val Loss: 7.544950\n",
      "Epoch 16/400 | Train Loss: 7.269270\t\t Val Loss: 7.458125\n",
      "Epoch 17/400 | Train Loss: 7.163961\t\t Val Loss: 7.437646\n",
      "Epoch 18/400 | Train Loss: 7.032316\t\t Val Loss: 8.040585\n",
      "Epoch 19/400 | Train Loss: 6.970752\t\t Val Loss: 7.105155\n",
      "Epoch 20/400 | Train Loss: 6.826464\t\t Val Loss: 7.226909\n",
      "Epoch 21/400 | Train Loss: 6.839355\t\t Val Loss: 7.129081\n",
      "Epoch 22/400 | Train Loss: 6.669203\t\t Val Loss: 7.024733\n",
      "Epoch 23/400 | Train Loss: 6.624331\t\t Val Loss: 7.296583\n",
      "Epoch 24/400 | Train Loss: 6.574254\t\t Val Loss: 6.984924\n",
      "Epoch 25/400 | Train Loss: 6.467235\t\t Val Loss: 7.012956\n",
      "Epoch 26/400 | Train Loss: 6.298455\t\t Val Loss: 6.858928\n",
      "Epoch 27/400 | Train Loss: 6.252494\t\t Val Loss: 6.983637\n",
      "Epoch 28/400 | Train Loss: 6.168593\t\t Val Loss: 6.942422\n",
      "Epoch 29/400 | Train Loss: 6.148787\t\t Val Loss: 8.129200\n",
      "Epoch 30/400 | Train Loss: 6.146292\t\t Val Loss: 7.016802\n",
      "Epoch 31/400 | Train Loss: 6.067700\t\t Val Loss: 6.542411\n",
      "Epoch 32/400 | Train Loss: 5.851731\t\t Val Loss: 6.801972\n",
      "Epoch 33/400 | Train Loss: 5.928809\t\t Val Loss: 6.959344\n",
      "Epoch 34/400 | Train Loss: 5.925710\t\t Val Loss: 6.509544\n",
      "Epoch 35/400 | Train Loss: 5.764681\t\t Val Loss: 6.551088\n",
      "Epoch 36/400 | Train Loss: 5.715078\t\t Val Loss: 6.613940\n",
      "Epoch 37/400 | Train Loss: 5.672790\t\t Val Loss: 6.566309\n",
      "Epoch 38/400 | Train Loss: 5.613082\t\t Val Loss: 6.754185\n",
      "Epoch 39/400 | Train Loss: 5.571554\t\t Val Loss: 6.656285\n",
      "Epoch 40/400 | Train Loss: 5.491601\t\t Val Loss: 6.376405\n",
      "Epoch 41/400 | Train Loss: 5.344177\t\t Val Loss: 6.465254\n",
      "Epoch 42/400 | Train Loss: 5.411754\t\t Val Loss: 6.491463\n",
      "Epoch 43/400 | Train Loss: 5.300552\t\t Val Loss: 6.523337\n",
      "Epoch 44/400 | Train Loss: 5.206817\t\t Val Loss: 6.254424\n",
      "Epoch 45/400 | Train Loss: 5.210140\t\t Val Loss: 6.649588\n",
      "Epoch 46/400 | Train Loss: 5.111920\t\t Val Loss: 6.152914\n",
      "Epoch 47/400 | Train Loss: 5.067497\t\t Val Loss: 6.397489\n",
      "Epoch 48/400 | Train Loss: 4.998882\t\t Val Loss: 6.202981\n",
      "Epoch 49/400 | Train Loss: 4.949674\t\t Val Loss: 5.963940\n",
      "Epoch 50/400 | Train Loss: 4.924864\t\t Val Loss: 5.882543\n",
      "Epoch 51/400 | Train Loss: 4.860652\t\t Val Loss: 5.940765\n",
      "Epoch 52/400 | Train Loss: 4.825076\t\t Val Loss: 5.928157\n",
      "Epoch 53/400 | Train Loss: 4.830900\t\t Val Loss: 5.993129\n",
      "Epoch 54/400 | Train Loss: 4.810538\t\t Val Loss: 6.133978\n",
      "Epoch 55/400 | Train Loss: 4.744877\t\t Val Loss: 5.813720\n",
      "Epoch 56/400 | Train Loss: 4.649963\t\t Val Loss: 5.861370\n",
      "Epoch 57/400 | Train Loss: 4.672394\t\t Val Loss: 5.698318\n",
      "Epoch 58/400 | Train Loss: 4.546314\t\t Val Loss: 6.098395\n",
      "Epoch 59/400 | Train Loss: 4.524149\t\t Val Loss: 5.688073\n",
      "Epoch 60/400 | Train Loss: 4.447735\t\t Val Loss: 5.762761\n",
      "Epoch 61/400 | Train Loss: 4.515926\t\t Val Loss: 5.623930\n",
      "Epoch 62/400 | Train Loss: 4.453910\t\t Val Loss: 5.701005\n",
      "Epoch 63/400 | Train Loss: 4.319790\t\t Val Loss: 5.699216\n",
      "Epoch 64/400 | Train Loss: 4.337653\t\t Val Loss: 5.678909\n",
      "Epoch 65/400 | Train Loss: 4.328165\t\t Val Loss: 5.561461\n",
      "Epoch 66/400 | Train Loss: 4.279188\t\t Val Loss: 5.346952\n",
      "Epoch 67/400 | Train Loss: 4.341361\t\t Val Loss: 5.512175\n",
      "Epoch 68/400 | Train Loss: 4.221034\t\t Val Loss: 5.491909\n",
      "Epoch 69/400 | Train Loss: 4.119685\t\t Val Loss: 5.501983\n",
      "Epoch 70/400 | Train Loss: 4.155929\t\t Val Loss: 5.579161\n",
      "Epoch 71/400 | Train Loss: 4.133639\t\t Val Loss: 5.541532\n",
      "Epoch 72/400 | Train Loss: 4.086883\t\t Val Loss: 5.474517\n",
      "Epoch 73/400 | Train Loss: 4.081744\t\t Val Loss: 5.648339\n",
      "Epoch 74/400 | Train Loss: 4.043763\t\t Val Loss: 5.601417\n",
      "Epoch 75/400 | Train Loss: 4.085308\t\t Val Loss: 5.386603\n",
      "Epoch 76/400 | Train Loss: 3.972989\t\t Val Loss: 5.368433\n"
     ]
    }
   ],
   "source": [
    "if loss_name == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    lr = learning_rate\n",
    "elif loss_name == \"mae\":\n",
    "    loss_fn = torch.nn.L1Loss() ## 2배면 잘 작동\n",
    "    lr = learning_rate * 2\n",
    "elif loss_name == \"SMAPE\":\n",
    "    loss_fn = SMAPE             ## 4배면 잘 작동\n",
    "    lr = learning_rate * 4\n",
    "elif loss_name == \"mape\":\n",
    "    loss_fn = MAPE              ## 2배면 잘 작동\n",
    "    lr = learning_rate * 2\n",
    "elif loss_name == \"MASE\":\n",
    "    loss_fn = MASE(y_data, y_data.shape[1])\n",
    "    lr = learning_rate * 14  ## 학습률 정상화... 그래도 잘 안됨\n",
    "else:\n",
    "    raise Exception(\"Your loss name is not valid.\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(backbone_model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_train_epochs)\n",
    "log_data = []\n",
    "\n",
    "## early stopping\n",
    "PATIENCE = 10\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "for epoc in range(num_train_epochs):\n",
    "    backbone_model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = backbone_model(X).prediction_outputs\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(backbone_model.parameters(), max_norm = 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()*X.shape[0]\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
    "\n",
    "    backbone_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yys = []\n",
    "        yyhats = []\n",
    "\n",
    "        for XX, yy in test_dataloader:\n",
    "            XX = XX.to(device)\n",
    "            yys.append(yy.to(device))\n",
    "            yyhats.append(backbone_model(XX).prediction_outputs)\n",
    "\n",
    "        yyhat = torch.concat(yyhats)\n",
    "        yy = torch.concat(yys)\n",
    "\n",
    "        val_loss = loss_fn(yyhat, yy)\n",
    "\n",
    "    print(f\"Epoch {epoc+1}/{num_train_epochs} | Train Loss: {avg_train_loss:.6f}\\t\\t Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    log_data.append({\"epoch\": epoc, \"loss\": avg_train_loss, \"eval_loss\": val_loss.item()})\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(backbone_model.state_dict(), os.path.join(output_dir, f\"model_{loss_name}_{1}.pth\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        break\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d282dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save log\n",
    "pd.DataFrame(log_data).to_csv(os.path.join(log_dir, f\"pretrain_{loss_name}_model{1}.csv\"))\n",
    "\n",
    "## load best model\n",
    "backbone_model.load_state_dict(torch.load(os.path.join(output_dir, f\"model_{loss_name}_{1}.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b045298",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhats = []\n",
    "yys = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for XX, yy in test_dataloader:\n",
    "        XX = XX.to(device)\n",
    "        yys.append(yy.to(device))\n",
    "        yyhats.append(backbone_model(XX).prediction_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fce1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhat, yy = torch.concat(yyhats).squeeze(), torch.concat(yys).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c926be7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE: 675.2691650390625\n",
      "test MAE: 224.3711395263672\n",
      "test SMAPE: 5.346951961517334\n"
     ]
    }
   ],
   "source": [
    "mseLoss = torch.nn.MSELoss()\n",
    "maeLoss = torch.nn.L1Loss()\n",
    "\n",
    "def smape(yy, yyhat):\n",
    "    numerator = 100*abs(yy - yyhat)\n",
    "    denominator = (abs(yy) + abs(yyhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape\n",
    "\n",
    "print(f\"test RMSE: {torch.sqrt(mseLoss(yyhat, yy))}\")\n",
    "print(f\"test MAE: {maeLoss(yyhat, yy)}\")\n",
    "print(f\"test SMAPE: {smape(yy, yyhat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327c8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhat = pd.DataFrame(yyhat.to(\"cpu\"))\n",
    "yyhat.columns = [f\"{i}A\" for i in range(yyhat.shape[1])]\n",
    "yy = pd.DataFrame(yy.to(\"cpu\"))\n",
    "yy.columns = [f\"{i}B\" for i in range(yyhat.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ded59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result = pd.concat([yyhat, yy], axis = 1).sort_index(axis = 1)\n",
    "val_result.columns = [f\"prediction_{(i+1)//2}\" if i%2 == 1 else f\"ground_truth_{(i+1)//2}\" for i in range(1, val_result.shape[1]+1)]\n",
    "val_result.to_csv(os.path.join(log_dir, f\"prediction_val_results_{loss_name}_model{1}.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b8e373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>ground_truth_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>ground_truth_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>ground_truth_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>ground_truth_4</th>\n",
       "      <th>prediction_5</th>\n",
       "      <th>ground_truth_5</th>\n",
       "      <th>...</th>\n",
       "      <th>prediction_20</th>\n",
       "      <th>ground_truth_20</th>\n",
       "      <th>prediction_21</th>\n",
       "      <th>ground_truth_21</th>\n",
       "      <th>prediction_22</th>\n",
       "      <th>ground_truth_22</th>\n",
       "      <th>prediction_23</th>\n",
       "      <th>ground_truth_23</th>\n",
       "      <th>prediction_24</th>\n",
       "      <th>ground_truth_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1752.652344</td>\n",
       "      <td>1745.800049</td>\n",
       "      <td>1792.139893</td>\n",
       "      <td>1869.199951</td>\n",
       "      <td>1801.667236</td>\n",
       "      <td>1854.199951</td>\n",
       "      <td>1800.565430</td>\n",
       "      <td>1863.800049</td>\n",
       "      <td>1823.670654</td>\n",
       "      <td>1853.800049</td>\n",
       "      <td>...</td>\n",
       "      <td>1770.563965</td>\n",
       "      <td>1833.199951</td>\n",
       "      <td>1770.020996</td>\n",
       "      <td>1851.099976</td>\n",
       "      <td>1792.988770</td>\n",
       "      <td>1854.900024</td>\n",
       "      <td>1779.965698</td>\n",
       "      <td>1854.199951</td>\n",
       "      <td>1797.135132</td>\n",
       "      <td>1870.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10133.837891</td>\n",
       "      <td>9980.000000</td>\n",
       "      <td>10864.724609</td>\n",
       "      <td>10070.000000</td>\n",
       "      <td>10841.255859</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10977.914062</td>\n",
       "      <td>10200.000000</td>\n",
       "      <td>10914.329102</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10184.203125</td>\n",
       "      <td>9940.000000</td>\n",
       "      <td>10418.611328</td>\n",
       "      <td>9960.000000</td>\n",
       "      <td>10535.654297</td>\n",
       "      <td>9950.000000</td>\n",
       "      <td>10472.678711</td>\n",
       "      <td>9970.000000</td>\n",
       "      <td>10483.978516</td>\n",
       "      <td>10050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1474.902954</td>\n",
       "      <td>1445.834961</td>\n",
       "      <td>1497.290283</td>\n",
       "      <td>1433.120972</td>\n",
       "      <td>1471.403198</td>\n",
       "      <td>1439.545044</td>\n",
       "      <td>1466.879883</td>\n",
       "      <td>1441.876953</td>\n",
       "      <td>1501.551147</td>\n",
       "      <td>1440.109009</td>\n",
       "      <td>...</td>\n",
       "      <td>1473.575684</td>\n",
       "      <td>1441.203003</td>\n",
       "      <td>1490.659790</td>\n",
       "      <td>1435.150024</td>\n",
       "      <td>1477.021240</td>\n",
       "      <td>1420.425049</td>\n",
       "      <td>1508.834961</td>\n",
       "      <td>1441.050049</td>\n",
       "      <td>1493.636108</td>\n",
       "      <td>1425.350952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6539.148926</td>\n",
       "      <td>6546.056641</td>\n",
       "      <td>6636.816895</td>\n",
       "      <td>6936.538574</td>\n",
       "      <td>6688.670410</td>\n",
       "      <td>6724.092285</td>\n",
       "      <td>6591.450684</td>\n",
       "      <td>6635.496582</td>\n",
       "      <td>6903.552734</td>\n",
       "      <td>6887.514160</td>\n",
       "      <td>...</td>\n",
       "      <td>6298.084961</td>\n",
       "      <td>6130.813477</td>\n",
       "      <td>6609.665039</td>\n",
       "      <td>6259.539551</td>\n",
       "      <td>6578.859863</td>\n",
       "      <td>6557.993164</td>\n",
       "      <td>6899.544434</td>\n",
       "      <td>6951.220703</td>\n",
       "      <td>6670.315430</td>\n",
       "      <td>7044.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4707.916992</td>\n",
       "      <td>4723.200195</td>\n",
       "      <td>5158.883301</td>\n",
       "      <td>5204.700195</td>\n",
       "      <td>5172.888672</td>\n",
       "      <td>5262.500000</td>\n",
       "      <td>5273.964844</td>\n",
       "      <td>5392.399902</td>\n",
       "      <td>5322.962891</td>\n",
       "      <td>5476.600098</td>\n",
       "      <td>...</td>\n",
       "      <td>4826.148438</td>\n",
       "      <td>5034.799805</td>\n",
       "      <td>4918.047852</td>\n",
       "      <td>5064.700195</td>\n",
       "      <td>5080.344727</td>\n",
       "      <td>5092.600098</td>\n",
       "      <td>5072.863281</td>\n",
       "      <td>5120.899902</td>\n",
       "      <td>5091.442383</td>\n",
       "      <td>5156.399902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>8843.205078</td>\n",
       "      <td>8830.000000</td>\n",
       "      <td>8878.408203</td>\n",
       "      <td>8800.000000</td>\n",
       "      <td>8889.751953</td>\n",
       "      <td>8780.000000</td>\n",
       "      <td>8757.480469</td>\n",
       "      <td>8710.000000</td>\n",
       "      <td>8800.332031</td>\n",
       "      <td>8790.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8858.031250</td>\n",
       "      <td>8810.000000</td>\n",
       "      <td>8791.740234</td>\n",
       "      <td>8690.000000</td>\n",
       "      <td>8805.286133</td>\n",
       "      <td>8710.000000</td>\n",
       "      <td>8900.219727</td>\n",
       "      <td>8810.000000</td>\n",
       "      <td>8906.587891</td>\n",
       "      <td>8810.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>970.803955</td>\n",
       "      <td>958.630005</td>\n",
       "      <td>1075.812622</td>\n",
       "      <td>1087.250000</td>\n",
       "      <td>1078.613159</td>\n",
       "      <td>1063.640015</td>\n",
       "      <td>1061.907227</td>\n",
       "      <td>1064.489990</td>\n",
       "      <td>1059.749146</td>\n",
       "      <td>1038.270020</td>\n",
       "      <td>...</td>\n",
       "      <td>1060.049194</td>\n",
       "      <td>1048.619995</td>\n",
       "      <td>1073.467407</td>\n",
       "      <td>1077.099976</td>\n",
       "      <td>1043.175537</td>\n",
       "      <td>1073.660034</td>\n",
       "      <td>1044.657593</td>\n",
       "      <td>1079.089966</td>\n",
       "      <td>1060.717041</td>\n",
       "      <td>1105.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>5688.923340</td>\n",
       "      <td>5577.244629</td>\n",
       "      <td>6481.854004</td>\n",
       "      <td>6268.951660</td>\n",
       "      <td>5857.679688</td>\n",
       "      <td>6390.646484</td>\n",
       "      <td>5947.586914</td>\n",
       "      <td>6173.075195</td>\n",
       "      <td>5833.675781</td>\n",
       "      <td>5789.339844</td>\n",
       "      <td>...</td>\n",
       "      <td>5318.253418</td>\n",
       "      <td>5299.357910</td>\n",
       "      <td>4812.864258</td>\n",
       "      <td>4838.526367</td>\n",
       "      <td>4834.447266</td>\n",
       "      <td>4506.627441</td>\n",
       "      <td>6158.281250</td>\n",
       "      <td>6024.740234</td>\n",
       "      <td>6011.474609</td>\n",
       "      <td>5739.643066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>6552.569336</td>\n",
       "      <td>6588.406250</td>\n",
       "      <td>6458.676758</td>\n",
       "      <td>6459.303711</td>\n",
       "      <td>6349.005859</td>\n",
       "      <td>6185.271484</td>\n",
       "      <td>6411.419922</td>\n",
       "      <td>6149.212402</td>\n",
       "      <td>6464.003418</td>\n",
       "      <td>6352.244629</td>\n",
       "      <td>...</td>\n",
       "      <td>6563.725586</td>\n",
       "      <td>6698.416016</td>\n",
       "      <td>6566.213867</td>\n",
       "      <td>6632.667480</td>\n",
       "      <td>6541.937988</td>\n",
       "      <td>6713.057617</td>\n",
       "      <td>6542.205566</td>\n",
       "      <td>6679.114746</td>\n",
       "      <td>6482.615723</td>\n",
       "      <td>6442.051758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>7672.152344</td>\n",
       "      <td>7986.000000</td>\n",
       "      <td>8470.106445</td>\n",
       "      <td>8291.000000</td>\n",
       "      <td>8648.580078</td>\n",
       "      <td>8141.000000</td>\n",
       "      <td>8478.899414</td>\n",
       "      <td>7544.000000</td>\n",
       "      <td>8598.875000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8276.465820</td>\n",
       "      <td>7949.000000</td>\n",
       "      <td>8034.422363</td>\n",
       "      <td>8303.000000</td>\n",
       "      <td>8197.935547</td>\n",
       "      <td>8777.000000</td>\n",
       "      <td>8288.975586</td>\n",
       "      <td>8626.000000</td>\n",
       "      <td>8227.101562</td>\n",
       "      <td>9026.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2892 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prediction_1  ground_truth_1  prediction_2  ground_truth_2  \\\n",
       "0      1752.652344     1745.800049   1792.139893     1869.199951   \n",
       "1     10133.837891     9980.000000  10864.724609    10070.000000   \n",
       "2      1474.902954     1445.834961   1497.290283     1433.120972   \n",
       "3      6539.148926     6546.056641   6636.816895     6936.538574   \n",
       "4      4707.916992     4723.200195   5158.883301     5204.700195   \n",
       "...            ...             ...           ...             ...   \n",
       "2887   8843.205078     8830.000000   8878.408203     8800.000000   \n",
       "2888    970.803955      958.630005   1075.812622     1087.250000   \n",
       "2889   5688.923340     5577.244629   6481.854004     6268.951660   \n",
       "2890   6552.569336     6588.406250   6458.676758     6459.303711   \n",
       "2891   7672.152344     7986.000000   8470.106445     8291.000000   \n",
       "\n",
       "      prediction_3  ground_truth_3  prediction_4  ground_truth_4  \\\n",
       "0      1801.667236     1854.199951   1800.565430     1863.800049   \n",
       "1     10841.255859    10190.000000  10977.914062    10200.000000   \n",
       "2      1471.403198     1439.545044   1466.879883     1441.876953   \n",
       "3      6688.670410     6724.092285   6591.450684     6635.496582   \n",
       "4      5172.888672     5262.500000   5273.964844     5392.399902   \n",
       "...            ...             ...           ...             ...   \n",
       "2887   8889.751953     8780.000000   8757.480469     8710.000000   \n",
       "2888   1078.613159     1063.640015   1061.907227     1064.489990   \n",
       "2889   5857.679688     6390.646484   5947.586914     6173.075195   \n",
       "2890   6349.005859     6185.271484   6411.419922     6149.212402   \n",
       "2891   8648.580078     8141.000000   8478.899414     7544.000000   \n",
       "\n",
       "      prediction_5  ground_truth_5  ...  prediction_20  ground_truth_20  \\\n",
       "0      1823.670654     1853.800049  ...    1770.563965      1833.199951   \n",
       "1     10914.329102    10210.000000  ...   10184.203125      9940.000000   \n",
       "2      1501.551147     1440.109009  ...    1473.575684      1441.203003   \n",
       "3      6903.552734     6887.514160  ...    6298.084961      6130.813477   \n",
       "4      5322.962891     5476.600098  ...    4826.148438      5034.799805   \n",
       "...            ...             ...  ...            ...              ...   \n",
       "2887   8800.332031     8790.000000  ...    8858.031250      8810.000000   \n",
       "2888   1059.749146     1038.270020  ...    1060.049194      1048.619995   \n",
       "2889   5833.675781     5789.339844  ...    5318.253418      5299.357910   \n",
       "2890   6464.003418     6352.244629  ...    6563.725586      6698.416016   \n",
       "2891   8598.875000     8331.000000  ...    8276.465820      7949.000000   \n",
       "\n",
       "      prediction_21  ground_truth_21  prediction_22  ground_truth_22  \\\n",
       "0       1770.020996      1851.099976    1792.988770      1854.900024   \n",
       "1      10418.611328      9960.000000   10535.654297      9950.000000   \n",
       "2       1490.659790      1435.150024    1477.021240      1420.425049   \n",
       "3       6609.665039      6259.539551    6578.859863      6557.993164   \n",
       "4       4918.047852      5064.700195    5080.344727      5092.600098   \n",
       "...             ...              ...            ...              ...   \n",
       "2887    8791.740234      8690.000000    8805.286133      8710.000000   \n",
       "2888    1073.467407      1077.099976    1043.175537      1073.660034   \n",
       "2889    4812.864258      4838.526367    4834.447266      4506.627441   \n",
       "2890    6566.213867      6632.667480    6541.937988      6713.057617   \n",
       "2891    8034.422363      8303.000000    8197.935547      8777.000000   \n",
       "\n",
       "      prediction_23  ground_truth_23  prediction_24  ground_truth_24  \n",
       "0       1779.965698      1854.199951    1797.135132      1870.400024  \n",
       "1      10472.678711      9970.000000   10483.978516     10050.000000  \n",
       "2       1508.834961      1441.050049    1493.636108      1425.350952  \n",
       "3       6899.544434      6951.220703    6670.315430      7044.781250  \n",
       "4       5072.863281      5120.899902    5091.442383      5156.399902  \n",
       "...             ...              ...            ...              ...  \n",
       "2887    8900.219727      8810.000000    8906.587891      8810.000000  \n",
       "2888    1044.657593      1079.089966    1060.717041      1105.900024  \n",
       "2889    6158.281250      6024.740234    6011.474609      5739.643066  \n",
       "2890    6542.205566      6679.114746    6482.615723      6442.051758  \n",
       "2891    8288.975586      8626.000000    8227.101562      9026.000000  \n",
       "\n",
       "[2892 rows x 48 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "145ccf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7182a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
