{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad4991e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import PatchTSTForPrediction\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5fbee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"coin\"\n",
    "\n",
    "output_dir = \"saved_models\"\n",
    "log_dir = os.path.join('logstf', data)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "loss_name = \"mse\"\n",
    "\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7961096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## target domain\n",
    "target_X = pd.read_csv(f\"../data/{data}/train_input_7.csv\").iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "np.random.seed(2)\n",
    "random_indices1 = np.random.choice(pd.read_csv(\"../data/M4_train.csv\").iloc[:, (1):].index,\n",
    "                                   size=target_X.shape[0] * 20, replace=True)\n",
    "\n",
    "X_data = pd.read_csv(\"../data/M4_train.csv\").iloc[:, 1 + (24 * 0):].loc[random_indices1].values.astype(np.float32)\n",
    "y_data = pd.read_csv(\"../data/M4_test.csv\").iloc[:, 1:].loc[random_indices1].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91eb7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bootstrap\n",
    "np.random.seed(42)\n",
    "select = np.random.choice(len(X_data), size=len(X_data), replace=True)\n",
    "X_bootstrap = X_data[select]\n",
    "y_bootstrap = y_data[select]\n",
    "\n",
    "val_split_index = int(len(X_bootstrap) * 0.8)\n",
    "\n",
    "def to_tensor_and_reshape(array):\n",
    "    result = torch.tensor(array)\n",
    "    result = result.reshape(-1, result.shape[1], 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "X_train, X_valid = to_tensor_and_reshape(X_bootstrap[:val_split_index]), to_tensor_and_reshape(X_bootstrap[val_split_index:])\n",
    "y_train, y_valid = to_tensor_and_reshape(y_bootstrap[:val_split_index]), to_tensor_and_reshape(y_bootstrap[val_split_index:])\n",
    "\n",
    "## setting dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 256, shuffle = True, num_workers = 16)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 256, num_workers = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "924c3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = PatchTSTForPrediction.from_pretrained(os.path.join(output_dir, \"PatchTSTBackbone\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cffd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 1940915.179348\t\t Val Loss: 1165182.000000\n",
      "Epoch 2/200 | Train Loss: 1032320.980978\t\t Val Loss: 810952.750000\n",
      "Epoch 3/200 | Train Loss: 872731.099185\t\t Val Loss: 733173.375000\n",
      "Epoch 4/200 | Train Loss: 783761.773098\t\t Val Loss: 657705.625000\n",
      "Epoch 5/200 | Train Loss: 739468.938859\t\t Val Loss: 600945.312500\n",
      "Epoch 6/200 | Train Loss: 676805.575408\t\t Val Loss: 630928.625000\n",
      "Epoch 7/200 | Train Loss: 639079.065897\t\t Val Loss: 545971.750000\n",
      "Epoch 8/200 | Train Loss: 542701.792120\t\t Val Loss: 600936.937500\n",
      "Epoch 9/200 | Train Loss: 550070.450408\t\t Val Loss: 551084.437500\n",
      "Epoch 10/200 | Train Loss: 508315.013587\t\t Val Loss: 554443.625000\n",
      "Epoch 11/200 | Train Loss: 484492.355978\t\t Val Loss: 518444.500000\n",
      "Epoch 12/200 | Train Loss: 485839.133152\t\t Val Loss: 510741.781250\n",
      "Epoch 13/200 | Train Loss: 449760.745245\t\t Val Loss: 508716.781250\n",
      "Epoch 14/200 | Train Loss: 477128.488111\t\t Val Loss: 510219.093750\n",
      "Epoch 15/200 | Train Loss: 402925.199389\t\t Val Loss: 490926.437500\n",
      "Epoch 16/200 | Train Loss: 428218.136549\t\t Val Loss: 441584.031250\n",
      "Epoch 17/200 | Train Loss: 378932.762228\t\t Val Loss: 454199.312500\n",
      "Epoch 18/200 | Train Loss: 368870.007133\t\t Val Loss: 487196.562500\n",
      "Epoch 19/200 | Train Loss: 365178.137568\t\t Val Loss: 469152.656250\n",
      "Epoch 20/200 | Train Loss: 366976.344769\t\t Val Loss: 444184.187500\n",
      "Epoch 21/200 | Train Loss: 330502.340014\t\t Val Loss: 420587.750000\n",
      "Epoch 22/200 | Train Loss: 320808.080163\t\t Val Loss: 441993.500000\n",
      "Epoch 23/200 | Train Loss: 318603.103940\t\t Val Loss: 447113.218750\n",
      "Epoch 24/200 | Train Loss: 303907.926291\t\t Val Loss: 435537.218750\n",
      "Epoch 25/200 | Train Loss: 290974.094429\t\t Val Loss: 425216.000000\n",
      "Epoch 26/200 | Train Loss: 291173.215353\t\t Val Loss: 428285.875000\n",
      "Epoch 27/200 | Train Loss: 293953.760870\t\t Val Loss: 411345.281250\n",
      "Epoch 28/200 | Train Loss: 281628.845448\t\t Val Loss: 407537.250000\n",
      "Epoch 29/200 | Train Loss: 263972.536345\t\t Val Loss: 402879.250000\n",
      "Epoch 30/200 | Train Loss: 248878.486753\t\t Val Loss: 420478.531250\n",
      "Epoch 31/200 | Train Loss: 243329.321671\t\t Val Loss: 389385.500000\n",
      "Epoch 32/200 | Train Loss: 245588.665082\t\t Val Loss: 399106.718750\n",
      "Epoch 33/200 | Train Loss: 237651.240149\t\t Val Loss: 369449.593750\n",
      "Epoch 34/200 | Train Loss: 231827.362772\t\t Val Loss: 413291.718750\n",
      "Epoch 35/200 | Train Loss: 224326.683424\t\t Val Loss: 443024.406250\n",
      "Epoch 36/200 | Train Loss: 240179.782609\t\t Val Loss: 381665.062500\n",
      "Epoch 37/200 | Train Loss: 216888.668818\t\t Val Loss: 372050.750000\n",
      "Epoch 38/200 | Train Loss: 216680.414062\t\t Val Loss: 357150.437500\n",
      "Epoch 39/200 | Train Loss: 198473.341202\t\t Val Loss: 345150.187500\n",
      "Epoch 40/200 | Train Loss: 213304.130435\t\t Val Loss: 385212.937500\n",
      "Epoch 41/200 | Train Loss: 190414.506284\t\t Val Loss: 340421.312500\n",
      "Epoch 42/200 | Train Loss: 207114.722486\t\t Val Loss: 353959.468750\n",
      "Epoch 43/200 | Train Loss: 186632.642663\t\t Val Loss: 359157.593750\n",
      "Epoch 44/200 | Train Loss: 186746.943444\t\t Val Loss: 341126.687500\n",
      "Epoch 45/200 | Train Loss: 181244.925951\t\t Val Loss: 363722.312500\n",
      "Epoch 46/200 | Train Loss: 184404.928838\t\t Val Loss: 338042.500000\n",
      "Epoch 47/200 | Train Loss: 187635.612942\t\t Val Loss: 403331.593750\n",
      "Epoch 48/200 | Train Loss: 172173.763077\t\t Val Loss: 347047.906250\n",
      "Epoch 49/200 | Train Loss: 165140.747962\t\t Val Loss: 334330.031250\n",
      "Epoch 50/200 | Train Loss: 162291.723845\t\t Val Loss: 375482.093750\n",
      "Epoch 51/200 | Train Loss: 167665.385530\t\t Val Loss: 343992.906250\n",
      "Epoch 52/200 | Train Loss: 162351.861923\t\t Val Loss: 338542.937500\n",
      "Epoch 53/200 | Train Loss: 162629.882812\t\t Val Loss: 339280.625000\n",
      "Epoch 54/200 | Train Loss: 160773.503567\t\t Val Loss: 360936.906250\n",
      "Epoch 55/200 | Train Loss: 152584.643342\t\t Val Loss: 342555.906250\n",
      "Epoch 56/200 | Train Loss: 149095.581012\t\t Val Loss: 384148.906250\n",
      "Epoch 57/200 | Train Loss: 143210.850713\t\t Val Loss: 321791.375000\n",
      "Epoch 58/200 | Train Loss: 146760.392493\t\t Val Loss: 345041.625000\n",
      "Epoch 59/200 | Train Loss: 137105.341033\t\t Val Loss: 324459.000000\n",
      "Epoch 60/200 | Train Loss: 138842.867867\t\t Val Loss: 353903.031250\n",
      "Epoch 61/200 | Train Loss: 135904.972996\t\t Val Loss: 321604.093750\n",
      "Epoch 62/200 | Train Loss: 131919.082201\t\t Val Loss: 346461.375000\n",
      "Epoch 63/200 | Train Loss: 132263.931046\t\t Val Loss: 323495.531250\n",
      "Epoch 64/200 | Train Loss: 128891.163893\t\t Val Loss: 339695.937500\n",
      "Epoch 65/200 | Train Loss: 126480.769022\t\t Val Loss: 306975.375000\n",
      "Epoch 66/200 | Train Loss: 125743.336787\t\t Val Loss: 327192.375000\n",
      "Epoch 67/200 | Train Loss: 122142.064368\t\t Val Loss: 340977.468750\n",
      "Epoch 68/200 | Train Loss: 121072.334069\t\t Val Loss: 319387.468750\n",
      "Epoch 69/200 | Train Loss: 121380.452106\t\t Val Loss: 339685.437500\n",
      "Epoch 70/200 | Train Loss: 121856.040082\t\t Val Loss: 310966.343750\n",
      "Epoch 71/200 | Train Loss: 113948.083220\t\t Val Loss: 323976.812500\n",
      "Epoch 72/200 | Train Loss: 114885.310462\t\t Val Loss: 325656.093750\n",
      "Epoch 73/200 | Train Loss: 110538.674253\t\t Val Loss: 342386.781250\n",
      "Epoch 74/200 | Train Loss: 110718.469260\t\t Val Loss: 326770.125000\n",
      "Epoch 75/200 | Train Loss: 108095.488791\t\t Val Loss: 338349.687500\n",
      "Epoch 76/200 | Train Loss: 109255.003227\t\t Val Loss: 319718.375000\n",
      "Epoch 77/200 | Train Loss: 104072.193954\t\t Val Loss: 320799.718750\n",
      "Epoch 78/200 | Train Loss: 102343.767323\t\t Val Loss: 314273.593750\n",
      "Epoch 79/200 | Train Loss: 100134.436396\t\t Val Loss: 307408.593750\n",
      "Epoch 80/200 | Train Loss: 99004.594769\t\t Val Loss: 307300.531250\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(backbone_model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_train_epochs)\n",
    "log_data = []\n",
    "\n",
    "if loss_name == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "elif loss_name == \"mae\":\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "# elif loss_name\n",
    "\n",
    "## early stopping\n",
    "PATIENCE = 15\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "for epoc in range(num_train_epochs):\n",
    "    backbone_model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = backbone_model(X).prediction_outputs\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(backbone_model.parameters(), max_norm = 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    backbone_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yys = []\n",
    "        yyhats = []\n",
    "\n",
    "        for XX, yy in test_dataloader:\n",
    "            XX = XX.to(device)\n",
    "            yys.append(yy.to(device))\n",
    "            yyhats.append(backbone_model(XX).prediction_outputs)\n",
    "\n",
    "        yyhat = torch.concat(yyhats)\n",
    "        yy = torch.concat(yys)\n",
    "\n",
    "        val_loss = loss_fn(yyhat, yy)\n",
    "\n",
    "    print(f\"Epoch {epoc+1}/{num_train_epochs} | Train Loss: {avg_train_loss:.6f}\\t\\t Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    log_data.append({\"epoch\": epoc, \"loss\": avg_train_loss, \"eval_loss\": val_loss.item()})\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(backbone_model.state_dict(), os.path.join(output_dir, f\"model_{loss_name}_{1}.pth\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        break\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d282dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save log\n",
    "pd.DataFrame(log_data).to_csv(os.path.join(log_dir, f\"pretrain_{loss_name}_model{1}.csv\"))\n",
    "\n",
    "## load best model\n",
    "backbone_model.load_state_dict(torch.load(os.path.join(output_dir, f\"model_{loss_name}_{1}.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbc946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f2f51ff2840>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8adff370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 24, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yys[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b7a679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b045298",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhats = []\n",
    "yys = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for XX, yy in test_dataloader:\n",
    "        XX = XX.to(device)\n",
    "        yys.append(yy.to(device))\n",
    "        yyhats.append(backbone_model(XX).prediction_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c8fce1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhat, yy = torch.concat(yyhats).squeeze(), torch.concat(yys).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "327c8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyhat = pd.DataFrame(yyhat.to(\"cpu\"))\n",
    "yyhat.columns = [f\"{i}A\" for i in range(yyhat.shape[1])]\n",
    "yy = pd.DataFrame(yy.to(\"cpu\"))\n",
    "yy.columns = [f\"{i}B\" for i in range(yyhat.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca073ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1ded59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result = pd.concat([yyhat, yy], axis = 1).sort_index(axis = 1)\n",
    "val_result.columns = [f\"prediction_{(i+1)//2}\" if i%2 == 1 else f\"ground_truth_{(i+1)//2}\" for i in range(1, val_result.shape[1]+1)]\n",
    "val_result.to_csv(os.path.join(log_dir, f\"prediction_val_results_{loss_name}_model{1}.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "42b8e373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>ground_truth_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>ground_truth_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>ground_truth_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>ground_truth_4</th>\n",
       "      <th>prediction_5</th>\n",
       "      <th>ground_truth_5</th>\n",
       "      <th>...</th>\n",
       "      <th>prediction_20</th>\n",
       "      <th>ground_truth_20</th>\n",
       "      <th>prediction_21</th>\n",
       "      <th>ground_truth_21</th>\n",
       "      <th>prediction_22</th>\n",
       "      <th>ground_truth_22</th>\n",
       "      <th>prediction_23</th>\n",
       "      <th>ground_truth_23</th>\n",
       "      <th>prediction_24</th>\n",
       "      <th>ground_truth_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1707.848877</td>\n",
       "      <td>1745.800049</td>\n",
       "      <td>1734.792236</td>\n",
       "      <td>1869.199951</td>\n",
       "      <td>1742.787720</td>\n",
       "      <td>1854.199951</td>\n",
       "      <td>1742.532227</td>\n",
       "      <td>1863.800049</td>\n",
       "      <td>1741.122925</td>\n",
       "      <td>1853.800049</td>\n",
       "      <td>...</td>\n",
       "      <td>1782.030518</td>\n",
       "      <td>1833.199951</td>\n",
       "      <td>1752.822144</td>\n",
       "      <td>1851.099976</td>\n",
       "      <td>1737.945923</td>\n",
       "      <td>1854.900024</td>\n",
       "      <td>1730.955444</td>\n",
       "      <td>1854.199951</td>\n",
       "      <td>1735.780029</td>\n",
       "      <td>1870.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9779.167969</td>\n",
       "      <td>9980.000000</td>\n",
       "      <td>10503.027344</td>\n",
       "      <td>10070.000000</td>\n",
       "      <td>10413.624023</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10503.994141</td>\n",
       "      <td>10200.000000</td>\n",
       "      <td>10352.125000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10159.966797</td>\n",
       "      <td>9940.000000</td>\n",
       "      <td>10064.224609</td>\n",
       "      <td>9960.000000</td>\n",
       "      <td>9982.077148</td>\n",
       "      <td>9950.000000</td>\n",
       "      <td>9934.011719</td>\n",
       "      <td>9970.000000</td>\n",
       "      <td>10219.075195</td>\n",
       "      <td>10050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1464.434570</td>\n",
       "      <td>1445.834961</td>\n",
       "      <td>1465.694580</td>\n",
       "      <td>1433.120972</td>\n",
       "      <td>1462.740845</td>\n",
       "      <td>1439.545044</td>\n",
       "      <td>1478.380981</td>\n",
       "      <td>1441.876953</td>\n",
       "      <td>1485.951416</td>\n",
       "      <td>1440.109009</td>\n",
       "      <td>...</td>\n",
       "      <td>1470.554688</td>\n",
       "      <td>1441.203003</td>\n",
       "      <td>1465.894409</td>\n",
       "      <td>1435.150024</td>\n",
       "      <td>1465.591187</td>\n",
       "      <td>1420.425049</td>\n",
       "      <td>1463.738525</td>\n",
       "      <td>1441.050049</td>\n",
       "      <td>1471.424438</td>\n",
       "      <td>1425.350952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6840.120605</td>\n",
       "      <td>6546.056641</td>\n",
       "      <td>6810.293945</td>\n",
       "      <td>6936.538574</td>\n",
       "      <td>6597.047852</td>\n",
       "      <td>6724.092285</td>\n",
       "      <td>6590.900391</td>\n",
       "      <td>6635.496582</td>\n",
       "      <td>6789.518555</td>\n",
       "      <td>6887.514160</td>\n",
       "      <td>...</td>\n",
       "      <td>6373.643555</td>\n",
       "      <td>6130.813477</td>\n",
       "      <td>6514.244629</td>\n",
       "      <td>6259.539551</td>\n",
       "      <td>6702.911133</td>\n",
       "      <td>6557.993164</td>\n",
       "      <td>6620.116699</td>\n",
       "      <td>6951.220703</td>\n",
       "      <td>6860.129883</td>\n",
       "      <td>7044.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4431.148438</td>\n",
       "      <td>4723.200195</td>\n",
       "      <td>4997.003906</td>\n",
       "      <td>5204.700195</td>\n",
       "      <td>5002.770996</td>\n",
       "      <td>5262.500000</td>\n",
       "      <td>5179.929688</td>\n",
       "      <td>5392.399902</td>\n",
       "      <td>5096.360352</td>\n",
       "      <td>5476.600098</td>\n",
       "      <td>...</td>\n",
       "      <td>4705.217773</td>\n",
       "      <td>5034.799805</td>\n",
       "      <td>4628.752930</td>\n",
       "      <td>5064.700195</td>\n",
       "      <td>4815.059570</td>\n",
       "      <td>5092.600098</td>\n",
       "      <td>4739.848633</td>\n",
       "      <td>5120.899902</td>\n",
       "      <td>4808.609375</td>\n",
       "      <td>5156.399902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>8804.451172</td>\n",
       "      <td>8830.000000</td>\n",
       "      <td>8885.068359</td>\n",
       "      <td>8800.000000</td>\n",
       "      <td>8857.950195</td>\n",
       "      <td>8780.000000</td>\n",
       "      <td>8766.936523</td>\n",
       "      <td>8710.000000</td>\n",
       "      <td>8753.163086</td>\n",
       "      <td>8790.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8936.829102</td>\n",
       "      <td>8810.000000</td>\n",
       "      <td>8830.230469</td>\n",
       "      <td>8690.000000</td>\n",
       "      <td>8805.250000</td>\n",
       "      <td>8710.000000</td>\n",
       "      <td>8899.399414</td>\n",
       "      <td>8810.000000</td>\n",
       "      <td>8882.827148</td>\n",
       "      <td>8810.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>948.323975</td>\n",
       "      <td>958.630005</td>\n",
       "      <td>1034.923950</td>\n",
       "      <td>1087.250000</td>\n",
       "      <td>1048.290894</td>\n",
       "      <td>1063.640015</td>\n",
       "      <td>1046.935547</td>\n",
       "      <td>1064.489990</td>\n",
       "      <td>1038.391357</td>\n",
       "      <td>1038.270020</td>\n",
       "      <td>...</td>\n",
       "      <td>1018.620239</td>\n",
       "      <td>1048.619995</td>\n",
       "      <td>1033.620850</td>\n",
       "      <td>1077.099976</td>\n",
       "      <td>1021.959717</td>\n",
       "      <td>1073.660034</td>\n",
       "      <td>1027.536987</td>\n",
       "      <td>1079.089966</td>\n",
       "      <td>1030.504395</td>\n",
       "      <td>1105.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>5970.575684</td>\n",
       "      <td>5577.244629</td>\n",
       "      <td>6280.669922</td>\n",
       "      <td>6268.951660</td>\n",
       "      <td>5848.233887</td>\n",
       "      <td>6390.646484</td>\n",
       "      <td>5752.697754</td>\n",
       "      <td>6173.075195</td>\n",
       "      <td>6050.154785</td>\n",
       "      <td>5789.339844</td>\n",
       "      <td>...</td>\n",
       "      <td>5600.000977</td>\n",
       "      <td>5299.357910</td>\n",
       "      <td>4989.262695</td>\n",
       "      <td>4838.526367</td>\n",
       "      <td>5203.392090</td>\n",
       "      <td>4506.627441</td>\n",
       "      <td>6041.491211</td>\n",
       "      <td>6024.740234</td>\n",
       "      <td>5888.279785</td>\n",
       "      <td>5739.643066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>6513.104004</td>\n",
       "      <td>6588.406250</td>\n",
       "      <td>6442.052734</td>\n",
       "      <td>6459.303711</td>\n",
       "      <td>6390.719727</td>\n",
       "      <td>6185.271484</td>\n",
       "      <td>6389.219238</td>\n",
       "      <td>6149.212402</td>\n",
       "      <td>6503.473145</td>\n",
       "      <td>6352.244629</td>\n",
       "      <td>...</td>\n",
       "      <td>6600.451660</td>\n",
       "      <td>6698.416016</td>\n",
       "      <td>6508.057617</td>\n",
       "      <td>6632.667480</td>\n",
       "      <td>6494.035645</td>\n",
       "      <td>6713.057617</td>\n",
       "      <td>6566.168457</td>\n",
       "      <td>6679.114746</td>\n",
       "      <td>6510.375977</td>\n",
       "      <td>6442.051758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>7873.526367</td>\n",
       "      <td>7986.000000</td>\n",
       "      <td>7984.726074</td>\n",
       "      <td>8291.000000</td>\n",
       "      <td>8335.994141</td>\n",
       "      <td>8141.000000</td>\n",
       "      <td>8082.491211</td>\n",
       "      <td>7544.000000</td>\n",
       "      <td>8144.758301</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7879.981445</td>\n",
       "      <td>7949.000000</td>\n",
       "      <td>7785.458008</td>\n",
       "      <td>8303.000000</td>\n",
       "      <td>8000.172852</td>\n",
       "      <td>8777.000000</td>\n",
       "      <td>8023.607422</td>\n",
       "      <td>8626.000000</td>\n",
       "      <td>7921.097168</td>\n",
       "      <td>9026.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2892 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prediction_1  ground_truth_1  prediction_2  ground_truth_2  \\\n",
       "0      1707.848877     1745.800049   1734.792236     1869.199951   \n",
       "1      9779.167969     9980.000000  10503.027344    10070.000000   \n",
       "2      1464.434570     1445.834961   1465.694580     1433.120972   \n",
       "3      6840.120605     6546.056641   6810.293945     6936.538574   \n",
       "4      4431.148438     4723.200195   4997.003906     5204.700195   \n",
       "...            ...             ...           ...             ...   \n",
       "2887   8804.451172     8830.000000   8885.068359     8800.000000   \n",
       "2888    948.323975      958.630005   1034.923950     1087.250000   \n",
       "2889   5970.575684     5577.244629   6280.669922     6268.951660   \n",
       "2890   6513.104004     6588.406250   6442.052734     6459.303711   \n",
       "2891   7873.526367     7986.000000   7984.726074     8291.000000   \n",
       "\n",
       "      prediction_3  ground_truth_3  prediction_4  ground_truth_4  \\\n",
       "0      1742.787720     1854.199951   1742.532227     1863.800049   \n",
       "1     10413.624023    10190.000000  10503.994141    10200.000000   \n",
       "2      1462.740845     1439.545044   1478.380981     1441.876953   \n",
       "3      6597.047852     6724.092285   6590.900391     6635.496582   \n",
       "4      5002.770996     5262.500000   5179.929688     5392.399902   \n",
       "...            ...             ...           ...             ...   \n",
       "2887   8857.950195     8780.000000   8766.936523     8710.000000   \n",
       "2888   1048.290894     1063.640015   1046.935547     1064.489990   \n",
       "2889   5848.233887     6390.646484   5752.697754     6173.075195   \n",
       "2890   6390.719727     6185.271484   6389.219238     6149.212402   \n",
       "2891   8335.994141     8141.000000   8082.491211     7544.000000   \n",
       "\n",
       "      prediction_5  ground_truth_5  ...  prediction_20  ground_truth_20  \\\n",
       "0      1741.122925     1853.800049  ...    1782.030518      1833.199951   \n",
       "1     10352.125000    10210.000000  ...   10159.966797      9940.000000   \n",
       "2      1485.951416     1440.109009  ...    1470.554688      1441.203003   \n",
       "3      6789.518555     6887.514160  ...    6373.643555      6130.813477   \n",
       "4      5096.360352     5476.600098  ...    4705.217773      5034.799805   \n",
       "...            ...             ...  ...            ...              ...   \n",
       "2887   8753.163086     8790.000000  ...    8936.829102      8810.000000   \n",
       "2888   1038.391357     1038.270020  ...    1018.620239      1048.619995   \n",
       "2889   6050.154785     5789.339844  ...    5600.000977      5299.357910   \n",
       "2890   6503.473145     6352.244629  ...    6600.451660      6698.416016   \n",
       "2891   8144.758301     8331.000000  ...    7879.981445      7949.000000   \n",
       "\n",
       "      prediction_21  ground_truth_21  prediction_22  ground_truth_22  \\\n",
       "0       1752.822144      1851.099976    1737.945923      1854.900024   \n",
       "1      10064.224609      9960.000000    9982.077148      9950.000000   \n",
       "2       1465.894409      1435.150024    1465.591187      1420.425049   \n",
       "3       6514.244629      6259.539551    6702.911133      6557.993164   \n",
       "4       4628.752930      5064.700195    4815.059570      5092.600098   \n",
       "...             ...              ...            ...              ...   \n",
       "2887    8830.230469      8690.000000    8805.250000      8710.000000   \n",
       "2888    1033.620850      1077.099976    1021.959717      1073.660034   \n",
       "2889    4989.262695      4838.526367    5203.392090      4506.627441   \n",
       "2890    6508.057617      6632.667480    6494.035645      6713.057617   \n",
       "2891    7785.458008      8303.000000    8000.172852      8777.000000   \n",
       "\n",
       "      prediction_23  ground_truth_23  prediction_24  ground_truth_24  \n",
       "0       1730.955444      1854.199951    1735.780029      1870.400024  \n",
       "1       9934.011719      9970.000000   10219.075195     10050.000000  \n",
       "2       1463.738525      1441.050049    1471.424438      1425.350952  \n",
       "3       6620.116699      6951.220703    6860.129883      7044.781250  \n",
       "4       4739.848633      5120.899902    4808.609375      5156.399902  \n",
       "...             ...              ...            ...              ...  \n",
       "2887    8899.399414      8810.000000    8882.827148      8810.000000  \n",
       "2888    1027.536987      1079.089966    1030.504395      1105.900024  \n",
       "2889    6041.491211      6024.740234    5888.279785      5739.643066  \n",
       "2890    6566.168457      6679.114746    6510.375977      6442.051758  \n",
       "2891    8023.607422      8626.000000    7921.097168      9026.000000  \n",
       "\n",
       "[2892 rows x 48 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fded4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "mseLoss = torch.nn.MSELoss()\n",
    "maeLoss = torch.nn.L1Loss()\n",
    "\n",
    "def smape(yy, yyhat):\n",
    "    numerator = 100*abs(yy - yyhat)\n",
    "    denominator = (abs(yy) + abs(yyhat))/2\n",
    "    smape = torch.mean(numerator / denominator)\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7ce51d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE: 554.2002563476562\n",
      "test MAE: 233.8564453125\n",
      "test SMAPE: 6.526806831359863\n"
     ]
    }
   ],
   "source": [
    "print(f\"test RMSE: {torch.sqrt(mseLoss(yyhat, yy))}\")\n",
    "print(f\"test MAE: {maeLoss(yyhat, yy)}\")\n",
    "print(f\"test SMAPE: {smape(yy, yyhat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "145ccf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7182a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
